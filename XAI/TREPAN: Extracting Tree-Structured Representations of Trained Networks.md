# TREPAN: Extracting Tree-Structured Representations of Trained Networks

**著者**: Mark W. Craven, Jude W. Shavlik  
**公開**: NeurIPS 1995  
**リンク**: [Proceedings PDF](https://proceedings.neurips.cc/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf)

本論文は、学習済みニューラルネットワークから**決定木**を抽出し、人間可読な概念記述を得る**TREPAN**を提案します。学習済みネットを**オラクル**としてクエリしつつ、データ分布に沿ったサンプリングで木を誘導する点が特徴で、**高い忠実度（fidelity）**と**理解しやすさ**の両立を示します。

---

### 1. 新規性：この論文の貢献
- **オラクル付き学習**: 目標関数を「ネットの入出力関係」とみなし、学習過程でネットにクエリすることで、必要な領域で情報を効率的に収集。
- **分布尊重のクエリ生成**: 特徴の周辺分布（離散: 頻度、連続: カーネル密度推定）からサンプル生成。部分的制約を満たすサンプルも生成可能。
- **m-of-n split** と**best-first 展開**: 可読性と忠実度を両立する分割表現・木成長戦略でスケール性を確保。

---

### 2. 手法の核心：数式・アルゴリズム

#### 設定
- 学習済みネット \(f: \mathcal{X}\to\mathcal{Y}\)。
- 目的: 決定木 \(T\) により \(f\) を近似し、可読な規則集合を得る。
- オラクル \(\mathcal{O}(x)=f(x)\) へクエリ可能。

#### オラクル（Oracle）とは
**オラクル（Oracle）**とは、TREPANにおいて**学習済みのニューラルネットワーク**のことを指します。

**なぜ「オラクル」と呼ぶか**:
- **定義**: オラクル \(\mathcal{O}(x) = f(x)\) は、任意の入力 \(x\) に対して、学習済みネットワーク \(f\) の予測を返す「完璧な回答者」として機能する
- **役割**: 決定木を構築する過程で、必要なサンプルに対して「この入力に対するネットワークの予測は何か」を教えてくれる
- **特徴**: 
  - 学習済みネットワークは既に訓練されているため、任意の入力に対して予測を返すことができる
  - 決定木の学習中に、データセットにない新しいサンプルに対しても予測を提供できる
  - これにより、決定木はネットワークの挙動をより正確に近似できる

**オラクルの使い方**:
1. **サンプル生成**: 決定木の各ノードで、制約を満たすサンプルを生成
2. **オラクルへのクエリ**: 生成したサンプルをオラクル（学習済みネットワーク）に入力し、予測を取得
3. **決定木の構築**: オラクルの予測を「正解ラベル」として使い、決定木を学習

**オラクルの利点**:
- **データ効率**: 実際のデータセットにないサンプルでも、ネットワークの挙動を調べられる
- **忠実度向上**: ネットワークの入出力関係を直接学習できるため、決定木の忠実度が高い
- **柔軟性**: 必要な領域で必要なだけサンプルを生成し、オラクルにクエリできる

#### オラクルの準備方法

**重要なポイント**: オラクルはTREPANで「作る」ものではなく、**既に学習済みのニューラルネットワーク**をオラクルとして「使う」ものです。

**オラクルの準備プロセス**:

1. **通常の機械学習でネットワークを学習**:
   - データセット \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N\) を使用
   - ニューラルネットワーク \(f\) を通常の方法（バックプロパゲーションなど）で学習
   - 学習済みネットワーク \(f: \mathcal{X} \to \mathcal{Y}\) を得る

2. **学習済みネットワークをオラクルとして定義**:
   - オラクル \(\mathcal{O}(x) = f(x)\) として定義
   - 任意の入力 \(x\) に対して、ネットワークの予測を返す関数として機能

3. **TREPANでオラクルを使用**:
   - TREPANは、この学習済みネットワーク（オラクル）を「完璧な回答者」として扱う
   - 決定木の学習中に、必要なサンプルを生成し、オラクルにクエリして予測を取得

**具体例**:
```
ステップ1: データセットでニューラルネットワークを学習
  データセット → [学習] → 学習済みネットワーク f

ステップ2: 学習済みネットワークをオラクルとして定義
  オラクル O(x) = f(x)

ステップ3: TREPANでオラクルを使用して決定木を構築
  オラクル O → [TREPAN] → 決定木 T
```

**オラクルが既に存在する理由**:
- TREPANの目的は「学習済みネットワークを理解可能な決定木に変換する」こと
- そのため、まずネットワークを学習する必要がある
- 学習済みネットワークがオラクルとして機能する

**まとめ**: オラクルは、TREPANの**入力**として与えられる既存の学習済みネットワークです。TREPANはオラクルを「作る」のではなく、オラクルを「使って」決定木を構築します。

#### ノード選択（best-first）
忠実度向上余地が大きいノードから展開：
\[
\text{score}(n) = \operatorname{reach}(n)\,\cdot\,\big(1-\operatorname{fidelity}(n)\big),
\]
- \(\operatorname{reach}(n)\): 木を通したときにノード \(n\) に到達する確率の推定。
- \(\operatorname{fidelity}(n)\): ノード配下での木とネットの一致度（推定）。

#### 分割候補の評価
- 候補 split（閾値、カテゴリ分割、m-of-n など）を生成。
- ノードの**制約集合** \(S_n\) を満たすサンプルを、学習分布の推定から生成し、\(\mathcal{O}\) でラベル付け。
- 分割後の純度（ネット出力に対する）や情報利得を基準に最良 split を選択。
- さらに **m-of-n split**（n 個のリテラルのうち m 個が真）を seed から探索し、簡潔かつ高忠実度な分割を採用。

#### 子ノード生成と葉判定
- 子ごとに制約集合を拡張 \(S_c = S_n \cup \{\text{split outcome}\}\)。
- オラクルへのクエリで単一クラス（ネット予測がほぼ一定）と判定できれば葉ノード化、さもなくばキューに追加。

---

### 3. サンプリング（オラクルクエリ）の詳細
- 部分的制約（例: \(x_j\in[a,b]\), \(x_k = v\)）を満たす擬似サンプルを生成。
- 離散特徴: 頻度推定。連続特徴: カーネル密度推定（Silverman, 1986）。
- これによりデータ分布に沿ったクエリを行い、分布外アーティファクトを抑制。

---

### 4. 評価の観点と結果（要旨）
- **忠実度**: テスト集合で木の予測とネット予測の一致度が高い。
- **精度**: いくつかの実問題で、純粋な決定木学習（C4.5 等）より精度が高い場合を示す。
- **可読性**: 内部ノード数やシンボル数（規則の複雑さ）が抑えられ、理解可能なサイズの木を抽出。

---

### 5. 実装ノート
- **木サイズ制約**やビーム幅で複雑さを制御。
- **m-of-n** は表現力と簡潔性のバランスに有効だが、探索の計算量に注意。
- 特徴分布推定は前処理の品質に依存。高次元では次元削減や特徴選択を併用すると安定。

---

### 6. 「キモ」と重要性
- **キモ**: 「ネットをオラクルとするクエリ学習」+「分布尊重のサンプリング」+「best-first + m-of-n split」で、忠実で可読な決定木を抽出。
- **重要性**: アーキテクチャ非依存でスケーラブルなルール抽出法として、以後のモデル蒸留・サロゲート説明の先駆けとなった。
