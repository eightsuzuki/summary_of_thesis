# DecompX: Explaining Transformers' Decisions by Propagating Token Decomposition

**著者**: Modarressi et al.  
**会議**: ACL 2023 (Long Paper)  
**論文**: DecompX: Explaining Transformers' Decisions by Propagating Token Decomposition  
**ACL Anthology**: https://aclanthology.org/2023.acl-long.149/

---

## 一言で

**隠れ状態を各入力トークンの寄与分にベクトル分解したまま伝播させ、最終ロジットで集約する**。寄与の形式は**クラス数 $C$ のベクトル**（各トークンが各ラベルに与えた影響）である。

---

## 位置づけ

統合勾配法がモデルを「ブラックボックス」として扱うポストホックな手法であるのに対し、**モデルの内部構造（特に Transformer の注意機構や FFN 層）を利用して**ベクトル値の寄与を算出する手法。IG のように経路積分は使わず、**アーキテクチャに即した分解と伝播**で寄与を定義する。

---

## 核心的なアイデア

### トークン表現のベクトル分解と非混合伝播

各層の隠れ状態ベクトルを、元の**入力トークンの寄与分**に数学的に「分解」し、その分解状態を層を追うごとに伝播させる。

第 $l$ 層の第 $i$ トークンの表現 $x^l_i$ は、全入力トークン $1 \dots N$ からの寄与ベクトルの和として表現される。

$$x^l_i = \sum_{k=1}^{N} x^l_{i \Leftarrow k}$$

ここで $x^l_{i \Leftarrow k}$ は、第 $k$ 入力トークンに起因する多次元ベクトル。従来のアテンション・ロールアウトなどがアテンション重みという**スカラー**を積算していくのに対し、DecompX は各トークンの**ベクトル**をそのまま保持し、**非混合（Non-mixing）**状態で後続の層へ伝播させる。

### 分類ヘッドを介したロジットベクトルへの変換

最終層の埋め込み表現（通常は [CLS] トークン）の分解ベクトルを、そのまま**分類ヘッド（線形層など）**に通過させる。これにより、各入力トークンが「各予測ラベル」に対してどのような影響を与えたかを示す、**分解されたロジットベクトル**（次元＝クラス数 $C$）が得られる。

- ある単語が「ポジティブ」の確率を押し上げると同時に「ネガティブ」の確率をどれだけ押し下げたか  
- 無関係なクラスに対してどのような影響を与えたか  

を、スカラー化による情報欠損なしに直接定量化できる。Modarressi らは、このベクトルベースのアプローチが、スカラー化を伴う統合勾配法よりも**モデルの忠実性（Faithfulness）**において優れていることを実験的に示している。

---

## 比較メモ（ベクトル値寄与サーベイ内での整理）

| 項目 | 内容 |
|------|------|
| 手法名 | DecompX |
| 寄与ベクトルの構成 | 隠れ状態を各入力トークンの寄与分にベクトル分解したまま伝播させ、最終ロジットで集約 |
| 解釈の次元 | クラス数 $C$ |

---

## 関連

- [統合勾配法におけるベクトル値特徴量寄与_サーベイ](./統合勾配法におけるベクトル値特徴量寄与_サーベイ.md) — セクション 3 で DecompX を概説
