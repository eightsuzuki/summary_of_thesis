# LIME: "Why Should I Trust You?" Explaining the Predictions of Any Classifier

**著者**: Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin  
**公開**: arXiv 2016（v3: 2016-08-09）  
**DOI**: 10.48550/arXiv.1602.04938  
**リンク**: [arXiv:1602.04938](https://arxiv.org/abs/1602.04938)

本論文は、任意のブラックボックス分類器 \(f\) に対し、特定入力 \(x\) の近傍で**解釈可能かつ忠実**な局所的近似モデル \(g\)（例: スパース線形モデル）を学習することで、予測の説明を与える **LIME (Local Interpretable Model-agnostic Explanations)** を提案します。さらに、代表的な個別予測の説明を冗長なく提示する **SP-LIME** を導入します。

---

### 1. 新規性：この論文の貢献と既存研究との差分

- **モデル非依存性**: 任意の分類器に適用可能（勾配や内部構造に依存しない）。
- **局所的忠実性**: 入力 \(x\) の近傍での忠実度（局所近似誤差）を明示的に最適化。
- **解釈可能性制約**: モデルクラス \(\mathcal{G}\) と複雑度ペナルティ \(\Omega\) により、人間可読性（例: スパース性）を担保。
- **SP-LIME**: 代表インスタンス選択を劣モジュラ最適化として定式化。

---

### 2. 理論/手法の核心：数式できっちり定式化

#### 記法と前提
- ブラックボックス分類器 \(f: \mathbb{R}^d \to \mathbb{R}\)（クラススコアまたは確率を返す）。
- 入力 \(x \in \mathbb{R}^d\)。説明に用いる**解釈可能表現**を \(x' \in \{0,1\}^{d'}\) とする（例: テキストの単語有無、画像のスーパーピクセルON/OFF）。
- 元表現と解釈可能表現を結ぶ写像 \(\phi: \{0,1\}^{d'} \to \mathbb{R}^d\)（オンになっている単語やスーパーピクセルのみ残して他を無効化する等）。
- 近傍重み（ローカリティ） \(\pi_x(z) = \exp\bigl(- D(x, z)^2 / \sigma^2\bigr)\)。距離 \(D\) はタスク依存（テキスト: ハミング距離 on \(x'\)、画像: スーパーピクセルの一致度など）。

#### 2.1 説明モデルの定義
説明は、入力 \(x\) の近傍でブラックボックス \(f\) をよく近似し、かつ解釈可能な \(g\) を求める最適化問題：
\[
\xi(x) \;=\; \underset{g \in \mathcal{G}}{\arg\min}\; \mathcal{L}\bigl(f, g, \pi_x\bigr)\; +\; \Omega(g).
\]
ここで、\(\mathcal{G}\) は解釈可能モデル族（例: スパース線形モデル、浅い決定木）、\(\Omega\) は複雑度（例: 非ゼロ係数数）へのペナルティ。

局所忠実度（重み付き損失）は、\(N\) 個の近傍サンプル \(\{z_i\}_{i=1}^N\) に対し、
\[
\mathcal{L}\bigl(f, g, \pi_x\bigr) \;=\; \sum_{i=1}^{N} \pi_x(z_i)\,\bigl( f(z_i) - g(z_i') \bigr)^2,\quad z_i' \in \{0,1\}^{d'}.
\]

#### 2.2 スパース線形説明（K-Lasso 近似）
\(\mathcal{G}\) を \(g(z') = w^{\top} z'\) とする線形モデルに制限し、\(K\) 個以下の非ゼロ係数（\(\lVert w \rVert_0 \le K\)）を許容：
\[
\min_{w}\; \sum_{i=1}^{N} \pi_x(z_i)\,\bigl( f(z_i) - w^{\top} z_i' \bigr)^2 \quad \text{s.t.}\quad \lVert w \rVert_0 \le K.
\]
実装では K-Lasso（L1で候補を絞りK個にトリミング）や前進選択で近似。説明は上位 \(K\) 特徴の重み \(w\) で与えられる。

#### 2.3 サンプリング戦略
- テキスト: \(x'\) の各次元（単語）を独立に0/1サンプルし、\(z'\) を生成、\(z=\phi(z')\) を模型に入力して \(f(z)\) を得る。
- 画像: スーパーピクセル分割後、いくつかをOFFにした \(z'\) をサンプルし、OFF領域を平均色等で塗り潰した \(z=\phi(z')\) を評価。\(\pi_x\) は \(z'\) と \(x'\) のハミング距離などで定義。

---

### 3. SP-LIME（代表的説明の提示）
多数のインスタンスに対する説明から、冗長性を避けつつ代表的な \(B\) 個を選ぶ問題を、劣モジュラ最大化として定式化：
- 各説明から得られる特徴重要度ベクトルを行列 \(W \in \mathbb{R}^{m \times d'}\) に格納（\(m\): 候補インスタンス数）。
- 重要度カバレッジを測る集合関数 \(F(A)\)（例: 特徴ごとのカバレッジ関数の総和）。
- 目的: \(\max_{A \subseteq [m],\, |A| \le B} F(A)\)。

劣モジュラ性により貪欲法で \((1-1/e)\) 近似保証を得る。結果として、非冗長で多様性の高い説明セットを提示できる。

---

### 4. 「キモ」と重要性：本論文の核と影響
- **キモ**: 「局所近似の忠実度（\(\mathcal{L}\)）＋解釈可能性制約（\(\Omega\)）」という汎用目的関数で、ブラックボックスの個別予測を説明する原理を確立。画像・テキストを通じてサンプリングと距離設計で柔軟に適用可能。
- **重要性/影響**: モデル非依存の標準的手法としてXAIの実務・研究に広く普及。SP-LIMEにより、意思決定者に「代表的な」説明を提示するUX設計にも影響。

---

### 5. まとめ（要点）
- 説明最適化：\(\xi(x) = \arg\min_{g \in \mathcal{G}} \sum_i \pi_x(z_i)(f(z_i)-g(z_i'))^2 + \Omega(g)\)。
- 線形説明：\(g(z') = w^{\top} z'\)、\(\lVert w \rVert_0 \le K\)。K-Lasso等で近似。
- 近傍重み：\(\pi_x(z)=\exp(-D(x,z)^2/\sigma^2)\)。
- 画像はスーパーピクセル、テキストは単語有無で解釈可能表現を構成。
- SP-LIMEで代表的かつ非冗長な説明集合を選択。

参考: [arXiv:1602.04938](https://arxiv.org/abs/1602.04938)
