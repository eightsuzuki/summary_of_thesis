# Attention is Not Entirely Explanation

**著者**: Sarthak Jain (Northeastern University), Byron C. Wallace (Northeastern University)  
**arXiv**: [arXiv:1902.10186](https://arxiv.org/abs/1902.10186)  
**発表年**: 2019年

---

### 概要

この論文は、自然言語処理（NLP）モデルにおける**注意機構（attention mechanisms）の解釈可能性**に疑問を投げかけた重要な研究です。TransformerやBERTなどのモデルで広く使われている注意重み（attention weights）が、モデルの予測に対する「説明」としてどれだけ信頼できるかを、複数のNLPタスクにわたる広範な実験を通じて検証しました。その結果、**注意重みは勾配ベースの特徴重要度指標と相関がなく、異なる注意分布が同一の予測をもたらす**ことが示され、標準的な注意モジュールは有意義な説明を提供しないと結論付けられています。

---

### 論文の核心：注意機構の解釈可能性への疑問

#### 1. 研究の背景と動機

注意機構は、Transformerアーキテクチャの登場以降、NLPモデルにおいて広く採用されています。多くの研究や実務において、**注意重みを可視化することで、モデルがどの単語やトークンに注目しているかを「説明」として解釈**することが一般的に行われてきました。

しかし、本論文の著者らは、このような解釈の妥当性に疑問を呈しました。注意重みが本当にモデルの意思決定プロセスを説明しているのか、それとも単なる副産物に過ぎないのかを、厳密に検証する必要があると主張しました。

#### 2. 検証方法：3つの主要な実験

論文では、注意機構の説明能力を評価するために、以下の3つの主要な実験が行われました。

##### 実験1：注意重みと勾配ベース指標の相関分析

**目的**: 注意重みが、勾配ベースの特徴重要度指標（gradient-based feature importance）と一致するかを検証する。

**方法**: 
- 複数のNLPタスク（感情分析、自然言語推論、質問応答など）において、注意重みと勾配ベースの重要度スコアを計算
- 両者の相関を測定

**結果**: 
- **注意重みと勾配ベース指標の間に有意な相関が見られない**ことが判明
- これは、注意重みがモデルの実際の意思決定プロセスを反映していない可能性を示唆

##### 実験2：注意分布の置き換え実験

**目的**: 注意重みをランダムに置き換えても、モデルの予測が変わらないかを検証する。

**方法**:
- 学習済みモデルの注意重みを、ランダムな分布や一様分布に置き換える
- 置き換え後のモデルの予測精度を測定

**結果**:
- **注意重みをランダムに置き換えても、モデルの予測精度がほとんど低下しない**ことが判明
- これは、注意重みがモデルの予測に本質的な役割を果たしていない可能性を示唆

##### 実験3：異なる注意分布による同一予測の生成

**目的**: 同じ予測を生成するが、異なる注意分布を持つモデルが存在するかを検証する。

**方法**:
- 同じ入力に対して同じ予測を生成するが、異なる注意重みを持つモデルを構築
- 注意分布の多様性を測定

**結果**:
- **同じ予測を生成するが、全く異なる注意分布を持つモデルが存在する**ことが判明
- これは、注意分布が予測を一意に決定するものではないことを示唆

---

### 主要な発見と結論

#### 1. 注意重みの限界

論文の実験結果から、以下の重要な限界が明らかになりました：

- **注意重みは勾配ベース指標と一致しない**: モデルの実際の意思決定プロセスを反映していない可能性
- **注意重みは予測に本質的ではない**: ランダムに置き換えても予測精度が維持される
- **注意分布は一意ではない**: 同じ予測を生成するが異なる注意分布を持つモデルが存在する

#### 2. 注意機構の解釈に関する警告

これらの発見から、著者らは以下の重要な警告を発しています：

- **注意重みを盲目的に「説明」として解釈することは危険である**
- 注意機構はモデルの予測に貢献するが、その重み自体が必ずしも説明として機能するわけではない
- 注意重みの解釈には、より慎重な検証が必要である

---

### 論文の意義と影響

#### 1. 解釈可能性研究への影響

この論文は、NLPにおける解釈可能性研究に大きな影響を与えました：

- **注意機構の解釈可能性に対する批判的視点の提供**: それまで当たり前とされていた注意重みの解釈に疑問を投げかけた
- **より厳密な評価方法の必要性の指摘**: 注意重みの解釈には、より厳密な検証が必要であることを示した

#### 2. 後続研究への影響

この論文は、以下のような後続研究を生み出しました：

- **「Attention is not not Explanation」**: Sarah WiegreffeとYuval Pinterによる反論論文。注意機構の解釈可能性に関する評価方法を再検討し、注意重みが説明として有用である可能性を示唆
- **注意機構の解釈可能性に関するより深い研究**: 注意機構の解釈可能性をより厳密に評価する方法の開発

---

### 技術的な詳細

#### 1. 実験設定

論文では、以下のNLPタスクで実験が行われました：

- **感情分析（Sentiment Analysis）**: IMDb映画レビューの感情分類
- **自然言語推論（Natural Language Inference）**: SNLIデータセット
- **質問応答（Question Answering）**: SQuADデータセット

#### 2. 評価指標

注意機構の説明能力を評価するために、以下の指標が使用されました：

- **注意重みと勾配ベース指標の相関**: Spearman相関係数、Pearson相関係数
- **注意分布の多様性**: エントロピー、KLダイバージェンス
- **予測精度**: タスク固有の評価指標（精度、F1スコアなど）

---

### 実践的な示唆

#### 1. 注意重みの解釈に関する推奨事項

論文の結果から、以下の推奨事項が導き出されます：

- **注意重みを単独で解釈しない**: 注意重みだけでなく、他の解釈手法（勾配ベース手法など）と組み合わせて使用する
- **注意重みの解釈を検証する**: 注意重みの解釈が実際にモデルの動作を反映しているかを検証する
- **注意機構の限界を理解する**: 注意機構は有用なツールであるが、その解釈には限界があることを認識する

#### 2. 代替的な解釈手法

注意重みに頼らない解釈手法として、以下のような手法が推奨されます：

- **勾配ベース手法**: Integrated Gradients、Gradient × Inputなど
- **摂動ベース手法**: LIME、SHAPなど
- **注意機構の改良**: より解釈可能な注意機構の設計

---

### まとめ

「Attention is Not Entirely Explanation」は、NLPにおける注意機構の解釈可能性に重要な疑問を投げかけた画期的な研究です。注意重みが必ずしもモデルの説明として機能しないことを示したこの論文は、解釈可能性研究に批判的視点をもたらし、より厳密な評価方法の必要性を指摘しました。この論文の影響は大きく、後続研究においても注意機構の解釈可能性に関する議論が続いています。

注意機構は依然として強力なツールですが、その解釈には慎重さが必要であり、他の解釈手法と組み合わせて使用することが推奨されます。
