# How to Explain Individual Classification Decisions

**著者**: David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Klaus-Robert Müller  
**公開**: arXiv 2009, JMLR 2010  
**arXiv**: [arXiv:0912.1128](https://arxiv.org/abs/0912.1128)  
**JMLR**: [JMLR Volume 11](https://jmlr.org/beta/papers/v11/baehrens10a.html)

---

### 概要

この論文は、**任意の分類器の個別予測を説明するための勾配ベースの手法**を提案した初期の重要な研究です。ブラックボックス分類器が特定のラベルを予測した理由を、**説明ベクトル（explanation vectors）**として定式化し、クラス確率の勾配を用いて特徴の重要度を計算する方法を提案しました。当時、個別予測を説明できる唯一の手法は決定木でしたが、本論文は任意の分類手法に適用可能な一般フレームワークを提供しました。

---

### 1. 新規性：この論文の最も大きな貢献と、既存研究と比べて何が新しいのか

**主な貢献**:

1. **勾配ベースの説明ベクトル**: クラス確率の勾配を説明ベクトルとして定義し、特徴の重要度を定量化する手法を初めて提案しました。

2. **任意の分類器への適用**: 決定木以外の任意の分類手法（SVM、ニューラルネットワークなど）に対して、個別予測の説明を提供する一般フレームワークを提案しました。

3. **局所的な説明**: グローバルな特徴重要度ではなく、**個別インスタンスに対する局所的な説明**を提供する手法を確立しました。

**既存研究との差分**:

- 従来は決定木のみが個別予測の説明を提供できましたが、本論文は任意の分類器に適用可能な手法を提案しました。
- グローバルな特徴重要度ではなく、個別インスタンスに対する局所的な説明に焦点を当てました。

---

### 2. 理論/手法の核心：提案されている理論や手法の要点

#### 2.1 説明ベクトルの定義

**基本的なアイデア**: 特徴の影響度は、その特徴値を少し変化させたときに出力がどれだけ変化するかで測定できます。

**説明ベクトル（Explanation Vector）**:
クラス確率の勾配を説明ベクトルとして定義します：

\[
\text{explanation vector} = \nabla_x P(Y \neq c | X = x)
\]

ここで：
- $P(Y \neq c | X = x)$: クラス $c$ 以外のクラスが選ばれる条件付き確率
- $\nabla_x$: 入力 $x$ に関する勾配

**直感的な説明**:
- 勾配が大きい特徴 = その特徴を少し変えると予測が大きく変わる = 重要
- 勾配が小さい特徴 = その特徴を変えても予測があまり変わらない = 重要でない

#### 2.2 確率出力がない分類器への対応

確率を直接出力しない分類器（例: SVM）に対しては、以下の方法で説明ベクトルを計算します：

1. **確率推定**: 分類器の出力から確率を推定する（例: Platt scaling）
2. **局所的な線形近似**: 入力の近傍で分類器を線形近似し、その勾配を計算

---

### 3. 実験結果と評価

#### 3.1 適用例

- **SVM**: 確率推定を用いて説明ベクトルを計算
- **ニューラルネットワーク**: 直接確率を出力するため、勾配を直接計算
- **その他の分類器**: 局所的な線形近似を用いて説明ベクトルを計算

#### 3.2 影響

この論文は非常に影響力があり、1,100以上の引用があります。勾配ベースの説明可能性手法の基礎を築き、後続研究（Integrated Gradients、SHAPなど）に大きな影響を与えました。

---

### 4. 意義と今後の展開

**意義**:

- **勾配ベース説明可能性の基礎**: 勾配を用いた説明可能性手法の初期の重要な研究
- **個別予測の説明**: グローバルな特徴重要度ではなく、個別インスタンスに対する局所的な説明の重要性を示した
- **一般フレームワーク**: 任意の分類器に適用可能な説明可能性フレームワークを提供

**今後の展開**:

- Integrated Gradients (Sundararajan et al., 2017): ベースラインからの経路積分による拡張
- SHAP (Lundberg & Lee, 2017): Shapley値に基づく統一フレームワーク
- その他の勾配ベース手法: Gradient × Input、Guided Backpropagationなど

---

### 参考文献

- Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen, K., & Müller, K.-R. (2010). How to Explain Individual Classification Decisions. *Journal of Machine Learning Research*, 11, 1803-1831.
- arXiv: [arXiv:0912.1128](https://arxiv.org/abs/0912.1128)
