# Axiomatic Attribution for Deep Networks

**著者**: Mukund Sundararajan (Google Inc.), Ankur Taly (Google Inc.), Qiqi Yan (Google Inc.)
**会議**: Proceedings of the 34th International Conference on Machine Learning (ICML), 2017 [cite: 1]

この論文は、深層学習モデルの予測結果をどの入力特徴量に起因させるかという「アトリビューション問題」に対して、公理的なアプローチを導入し、それを満たす新しい手法「Integrated Gradients (IG)」を提案しています。

---

### 1. 新規性：この論文の最も大きな貢献と、既存研究と比べて何が新しいのか

この論文の最大の貢献は、アトリビューション手法が満たすべき**2つの基本的な公理（Axioms）**を定義し、それに基づいて理論的に堅牢な新手法「Integrated Gradients」を提案した点です。これにより、これまで経験的な評価が主で曖昧さのあったアトリビューション手法の評価に、明確な基準をもたらしました。

既存研究の多くは、この論文が提唱する公理の少なくとも一方を満たしていませんでした [cite: 35]。

* **公理1: Sensitivity (感度)**
    * **定義**: ベースライン（比較基準となる入力）とある入力が1つの特徴量のみで異なり、かつモデルの予測値も異なる場合、その特徴量のアトリビューションは**ゼロであってはならない** [cite: 42]。
    * **既存研究の問題点**: 単純な勾配法（Gradients）やGuided Backpropagation、Deconvolutional Networksといった手法は、モデルの出力が入力近傍で飽和（flat）していると勾配がゼロになり、予測に影響を与えたはずの特徴量にアトリビューションが割り当てられない問題がありました [cite: 41, 44, 50]。これは`Sensitivity`の公理に違反します。

* **公理2: Implementation Invariance (実装不変性)**
    * **定義**: 2つのネットワークが数学的に全く同じ関数（任意の入力に対して同じ出力を返す）を表現している場合、それらの実装方法が異なっていても、アトリビューションは**同一でなければならない** [cite: 57, 58]。アトリビューションはモデルの機能に帰属されるべきで、内部の実装の詳細に依存すべきではないという考え方です [cite: 59, 60]。
    * **既存研究の問題点**: DeepLIFTやLayer-wise Relevance Propagation (LRP) といった手法は、`Sensitivity`の問題を解決しようとしましたが、通常の勾配の代わりに「離散的な勾配」を用いるため、勾配計算における連鎖律が成り立ちません [cite: 53, 67, 68]。これにより、同じ機能を持つ異なる実装のネットワークに対して異なるアトリビューションを生成してしまい、`Implementation Invariance`の公理に違反します [cite: 68]。

Integrated Gradientsは、勾配法の`Implementation Invariance`と、LRPなどが目指した`Sensitivity`の両方を、エレガントな方法で同時に満たす初の現実的な手法として提案されました [cite: 73]。

### 2. 理論/手法の核心：提案されている理論や手法の要点

この論文の核心は、ベースライン入力から実際の入力までの経路に沿って勾配を積分するというアイデアに基づいた **Integrated Gradients (IG)** です。

#### アトリビューションの定義
まず、アトリビューション問題は以下のように形式化されます。
* $F: \mathbb{R}^n \rightarrow [0, 1]$: 深層ネットワークを表す関数。
* $x = (x_1, \dots, x_n) \in \mathbb{R}^n$: アトリビューションを計算したい入力。
* $x' \in \mathbb{R}^n$: ベースラインとなる入力（例：画像認識における黒い画像 [cite: 32]）。
* $A_F(x, x') = (a_1, \dots, a_n) \in \mathbb{R}^n$: 予測に対するアトリビューションベクトル。$a_i$は特徴量$x_i$の貢献度を表します [cite: 9]。

#### Integrated Gradients (IG) の定義式
IGは、ベースライン$x'$から入力$x$までの直線経路上の勾配を積分（累積）することで計算されます [cite: 77]。特徴量$i$に対するアトリビューションは以下の式で定義されます。

$$\text{IntegratedGrads}_i(x) ::= (x_i - x'_i) \times \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} d\alpha$$
[cite: 77]

各記号の定義は以下の通りです。
* $\text{IntegratedGrads}_i(x)$: 入力$x$の$i$番目の特徴量に対するアトリビューション。
* $x_i$: 入力$x$の$i$番目の特徴量の値。
* $x'_i$: ベースライン入力$x'$の$i$番目の特徴量の値。
* $\alpha$: ベースライン$x'$と入力$x$を結ぶ直線経路上の点を表す係数で、0から1まで変化します。$\alpha=0$のとき点は$x'$に、$\alpha=1$のとき点は$x$に一致します。
* $x' + \alpha(x - x')$: ベースラインから入力への直線経路上の点。
* $\frac{\partial F(\cdot)}{\partial x_i}$: ネットワーク関数$F$の$i$番目の入力特徴量に関する偏導関数（勾配）。

この式は、ベースラインからの特徴量の変化量 $(x_i - x'_i)$ に、経路上の平均的な勾配を掛けていると解釈できます。

#### IGが満たす重要な性質

* **Completeness (完全性)**: IGの重要な性質として**Completeness**があります。これは、全ての特徴量のアトリビューションの総和が、入力$x$におけるネットワークの出力とベースライン$x'$における出力の差と正確に一致するというものです [cite: 78]。この性質は、DeepLIFTやLRPでも望ましいとされていました [cite: 78]。

    $$
    \sum_{i=1}^{n} \text{IntegratedGrads}_i(x) = F(x) - F(x')
    $$
    [cite: 80]

    この性質は、微積分学の基本定理（経路積分の文脈）から直接導かれ [cite: 79]、アトリビューションがモデルの出力変化を「漏れなくダブりなく」説明していることを保証します。また、`Completeness`は`Sensitivity(a)`を包含します。なぜなら、1つの特徴量だけが変化した場合、その特徴量のアトリビューションは出力の変化そのものに等しくなり、出力が変化していればアトリビューションは非ゼロになるからです [cite: 81, 82]。

* **Path MethodsとIGの唯一性**: IGは、ベースラインと入力を結ぶ「経路」に沿って勾配を積分する**Path Methods**と呼ばれる手法群の一つです [cite: 99]。特にIGは、最もシンプルな**直線経路** ($P_2$ in Figure 1) を採用しています [cite: 94, 109]。このPath Methodsというクラスは、`Implementation Invariance`, `Sensitivity(b)`, `Linearity`, `Completeness`という4つの公理を同時に満たす唯一の手法群であることが示されています [cite: 112, 119]。さらに、その中でもIGは**Symmetry-Preserving**（対称な変数は対称なアトリビューションを得る）という自然な性質を満たす**唯一のPath Method**であることが証明されています [cite: 134]。

### 3. 「キモ」と重要性：この論文の核となるアイデアと分野への影響

* **論文の「キモ」 (核となるアイデア)**
    1.  **公理による基礎付け**: 良いアトリビューションとは何かという問題を、直感や事例ではなく、「誰もが納得するべき性質（公理）」から出発して定義した点です。これにより、手法の評価に客観的で強固な理論的基盤を与えました [cite: 20, 21]。これは、経験的な評価手法が抱える「モデルの誤り」と「アトリビューション手法の誤り」を区別できないという根本的な問題を回避する画期的なアプローチでした [cite: 19, 95]。
    2.  **積分による勾配の統合**: 局所的な情報である「勾配」が持つ感度の問題を、ベースラインからの経路に沿って「積分」するという数学的に自然な操作で解決した点です。一点の勾配がゼロでも、経路全体での変化を捉えることで、特徴量の真の貢献度をより正確に評価できます [cite: 77]。

* **分野への重要性と影響**
    1.  **信頼できる説明性の提供**: IGは明確な公理に裏打ちされているため、その出力は他のアドホックな手法よりも信頼性が高いと見なされています。これにより、モデルのデバッグ [cite: 7, 26]、学習済みモデルからのルール抽出 [cite: 17, 195]、あるいは医師への診断根拠の提示 [cite: 14, 15] といった実応用において、AIの説明に対する信頼を大きく向上させました。
    2.  **XAI分野の標準手法へ**: その理論的な堅牢さに加え、実装が非常にシンプルであること（標準的な勾配計算を数回呼び出すだけ [cite: 6, 165]）から、IGは発表後またたく間に普及し、今日ではXAI（説明可能なAI）分野における最も基本的で重要なベースライン手法の一つとして広く認知・利用されています。
    3.  **経済学の知見の導入**: この論文は、経済学の費用分担問題におけるAumann-Shapley値などの考え方に着想を得ており [cite: 121]、異分野の確立された理論を深層学習に応用することでブレークスルーを生み出した成功例としても重要です.
