# On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation
**著者と所属機関**:
**Sebastian Bach**:
    ( Machine Learning Group, Fraunhofer Heinrich Hertz Institute, Berlin, Germany
     Machine Learning Group, Technische Universität Berlin, Berlin, Germany)
**Alexander Binder**:
    ( Machine Learning Group, Technische Universität Berlin, Berlin, Germany
     ISTD Pillar, Singapore University of Technology and Design (SUTD), Singapore)
**Grégoire Montavon**:
    ( Machine Learning Group, Technische Universität Berlin, Berlin, Germany)
**Frederick Klauschen**:
    ( Institute of Pathology, Charité University Hospital, Berlin, Germany)
**Klaus-Robert Müller**:
    ( Machine Learning Group, Technische Universität Berlin, Berlin, Germany
     Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea)
**Wojciech Samek**:
    ( Machine Learning Group, Fraunhofer Heinrich Hertz Institute, Berlin, Germany)

**発表**: PLOS ONE, 2015
**DOI**: [10.1371/journal.pone.0130140](https://doi.org/10.1371/journal.pone.0130140)

---

### 1. 新規性：この論文の貢献と既存研究との違い

この論文が発表された2015年当時、ディープラーニングモデルは「ブラックボックス」であり、その予測根拠を理解することが大きな課題でした。既存の説明手法には以下のようなものがありましたが、それぞれに課題がありました。

* **感度分析 (Sensitivity Analysis)**: モデルの出力を入力で微分する（つまり勾配を計算する）ことで、どの入力ピクセルが予測に影響を与えたかを見る手法。しかし、この方法は勾配が飽和する（ReLUなど）場合や、非線形性が強い場合にうまく機能せず、ノイズが多く解釈しにくいヒートマップを生成することがありました。
* **摂動ベースの手法 (Perturbation-based Methods)**: 入力画像の一部を隠したり変化させたりして、出力がどれだけ変わるかを計測する手法。直感的で分かりやすい一方、どの部分をどう変化させるかの組み合わせが爆発的に増えるため、計算コストが非常に高いという欠点がありました。

この論文が提案した**Layer-Wise Relevance Propagation (LRP)**の最大の新規性は、**「関連性保存の法則 (Relevance Conservation Principle)」**を導入した点にあります。これは、**「モデルの最終的な出力スコア（関連性）は、ネットワークの各層で保存されながら入力層まで逆伝播され、その総和は変化しない」**という考え方です。

これにより、LRPは以下の点で既存研究を大きく前進させました。

* **理論的妥当性**: 関連性が保存されるため、モデルの出力全体が入力の各ピクセルに「漏れなく・重複なく」分配されます。これにより、説明の完全性 (Completeness) が保証されます。
* **高品質な説明**: 勾配ベースの手法よりもノイズが少なく、人間にとって解釈しやすい、より鮮明なヒートマップを生成できることが示されました。
* **計算効率**: 摂動ベースの手法のように何度も推論を繰り返す必要がなく、一度の逆伝播計算でヒートマップを生成できるため、計算効率に優れています。

---

### 2. 理論/手法の核心：LRPの数式による詳細解説

LRPの目標は、ある入力データ $x$ に対する分類器の出力 $f(x)$ を、個々の入力特徴（ピクセル）の貢献度 $R_i$ の和として分解することです。

$$f(x) \approx \sum_i R_i$$

ここで、$R_i$ は入力特徴 $i$ の**関連性スコア (Relevance Score)** を表します。LRPは、この関連性を出力層から入力層に向かって、層ごとに（Layer-Wise）逆伝播（Propagate）させて計算します。

#### 基本的な伝播ルール

ある層のニューロン $k$ が持つ関連性 $R_k$ を、そのニューロンへの入力となっている一つ下の層のニューロン群 $j$ へ分配することを考えます。

ニューロン $j$ から $k$ への接続の強さを $z_{jk}$ とすると、ニューロン $j$ が受け取る関連性 $R_j$ は、上の層の全てのニューロン $k$ から分配された関連性の合計になります。

$$R_j = \sum_k \frac{z_{jk}}{\sum_{j'} z_{j'k}} R_k$$

各記号の定義は以下の通りです。
* $R_j$: 下の層にあるニューロン $j$ の関連性スコア。
* $R_k$: 上の層にあるニューロン $k$ の関連性スコア。
* $z_{jk}$: ニューロン $j$ がニューロン $k$ の活性化にどれだけ貢献したかを示す値。標準的なニューラルネットワークでは、下の層のニューロン $j$ の活性化出力を $a_j$、結合重みを $w_{jk}$ として、$z_{jk} = a_j w_{jk}$ と計算されます。
* $\sum_{j'} z_{j'k}$: ニューロン $k$ への全入力の合計。分母は正規化項として機能します。

この式の意味は、**「ニューロンkの関連性 $R_k$ を、その活性化に貢献した度合い $z_{jk}$ に比例して、下の層のニューロン $j$ に分配する」**ということです。この分配ルールを各層で適用することで、関連性保存則が局所的に満たされ、結果としてネットワーク全体での保存が保証されます。

#### 実用的な伝播ルール

上記の基本ルールは直感的ですが、分母が0になる不安定性などの問題があります。そのため、論文ではより安定した以下のルールを導入しています。

1.  **$\epsilon$-ルール (epsilon-rule)**:
    分母に微小な安定化項 $\epsilon$ を加えます。これにより、貢献がほぼ0のニューロンに大きな関連性が分配されることを防ぎます。

    $$R_j = \sum_k \frac{z_{jk}}{\sum_{j'} z_{j'k} + \epsilon \cdot \text{sign}(\sum_{j'} z_{j'k})} R_k$$

    * $\epsilon$: $10^{-9}$ のような非常に小さい正の値。
    * $\text{sign}(\cdot)$: 引数の符号（+1, -1, 0）を返す関数。

2.  **$\alpha, \beta$-ルール (alpha-beta-rule)**:
    予測に対して**促進的 (excitatory) な貢献**と**抑制的 (inhibitory) な貢献**を分けて扱うルールです。これにより、何が予測の「根拠」となり、何が「反証」となったかを区別して分析できます。

    $$R_j = \sum_k \left( \alpha \frac{(z_{jk})^+}{\sum_{j'} (z_{j'k})^+} - \beta \frac{(z_{jk})^-}{\sum_{j'} (z_{j'k})^-} \right) R_k$$

    * $(z_{jk})^+$: $z_{jk}$ のうち正の貢献のみを取り出したもの ($max(0, z_{jk})$)。予測を促進する要因。
    * $(z_{jk})^-$: $z_{jk}$ のうち負の貢献のみを取り出したもの ($min(0, z_{jk})$)。予測を抑制する要因。
    * $\alpha, \beta$: 正の貢献と負の貢献の重み付けを制御するハイパーパラメータ。関連性保存則を満たすために $\alpha - \beta = 1$ という制約が課されます。例えば、$\alpha=1, \beta=0$ に設定すると、促進的な貢献のみを考慮して逆伝播させることができます。

これらのルールを層の種類（全結合層、畳み込み層）やネットワークの深さに応じて使い分けることで、より質の高い説明を得ることができます。

---

### 3. 「キモ」と重要性：論文の核となるアイデアと分野への影響

#### 論文の「キモ」💡
この論文の核となるアイデア（「キモ」）は、**「ニューラルネットワークの予測プロセスを、出力から入力へと遡る『関連性の流れ』として捉え、その流れが各層で保存されるように厳密に設計したこと」**です。

これは物理学における**保存則**（エネルギー保存則など）から着想を得ています。「予測スコア」という一種のエネルギーが、どこかで勝手に生まれたり消えたりすることなく、システム（ネットワーク）の入力源（ピクセル）まで完全に遡ることができる、という考え方が非常に強力です。これにより、LRPは単なるヒューリスティックな手法ではなく、理論的な裏付けを持つ説明手法としての地位を確立しました。

#### 分野への重要性と影響 🏛️

* **説明可能AI (XAI) の基盤技術**: LRPは、その後の多くのXAI研究の基礎となりました。Deep Taylor Decompositionとの理論的な関係性が示されたり、LRPを改良した様々な手法が提案されたりするなど、XAI分野における重要なマイルストーンと見なされています。
* **モデルの信頼性とデバッグ**: LRPによって生成されるヒートマップは、モデルが画像のどの部分に注目して判断を下しているかを可視化します。これにより、研究者や開発者は「モデルが意図通りに学習しているか」を検証できます。例えば、犬を分類するモデルが、犬自身ではなく背景の芝生に反応していることが分かれば、データセットのバイアスを特定し、モデルを修正する手がかりになります。
* **科学・医療分野への応用**: 判断根拠の透明性が不可欠な医療画像診断などの分野で、AIの診断結果を医師が信頼し、補助的に利用するための重要なツールとなっています。LRPは、AIの予測が画像のどの病変部に基づいているかを示すことで、医師の意思決定を支援します。