# Improving performance of deep learning models with axiomatic attribution priors and expected gradients

**著者**: Gabriel Erion (University of Washington), Joseph D. Janizek (University of Washington), Scott M. Lundberg (Microsoft Research), Su-In Lee (University of Washington)  
**発表**: Nature Machine Intelligence, 2021  
**arXiv**: [arXiv:1906.10670](https://arxiv.org/abs/1906.10670)

---

### 概要

この論文は、**Expected Gradients (EG) の提案論文**です。単一のベースライン（例：ゼロベクトル）を選ぶことの恣意性（Arbitrariness）を排除するため、**訓練データセット全体からサンプリングした参照点との差分の期待値を取る手法を確立**しました。本論文は、ベースライン選択の問題を解決し、説明可能性とモデル性能の両方を改善する重要な貢献をしています。

---

### 論文の核心：Expected Gradientsの提案

#### 1. 研究の背景と動機

##### ベースライン選択の恣意性

Integrated Gradientsなどのパス属性法では、ベースラインの選択が恣意的です：

- **単一ベースラインの限界**: 単一のベースライン（例：ゼロベクトル）を選ぶことの恣意性
- **説明の不安定性**: ベースライン選択により、説明が不安定になる
- **アーティファクト**: 特定のベースラインがアーティファクトを生む可能性

##### Expected Gradientsの必要性

単一のベースラインに依存せず、**複数のベースラインからの期待値を取る**アプローチが必要です：

- **分布ベースライン**: 訓練データセット全体からサンプリングした参照点を使用
- **期待値の計算**: 複数のベースラインからの期待値を計算
- **恣意性の排除**: ベースライン選択の恣意性を排除

#### 2. Expected Gradientsの提案

##### 基本アイデア

Expected Gradientsは、以下のアイデアに基づいています：

- **分布からのサンプリング**: 訓練データセットの分布からベースラインをサンプリング
- **期待値の計算**: 複数のベースラインからの期待値を計算
- **安定した説明**: より安定した説明を生成

##### 数式的定義

Expected Gradientsは、以下のように定義されます：

$$\phi_i^{EG}(f, x, D) = \mathbb{E}_{x' \sim D} \left[ (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial f(x' + \alpha (x - x'))}{\partial x_i} d\alpha \right]$$

ここで：
- $f$: モデルの出力関数
- $x$: 入力
- $x'$: ベースライン（分布 $D$ からサンプリング）
- $D$: 訓練データセットの分布

##### Integrated Gradientsとの関係

Expected Gradientsは、Integrated Gradientsの拡張です：

- **Integrated Gradients**: 単一のベースライン $x'$ を使用
- **Expected Gradients**: 分布 $D$ からサンプリングした複数のベースラインを使用
- **期待値**: 複数のベースラインからの期待値を計算

---

### 技術的な詳細

#### 1. Expected Gradientsの実装

##### サンプリング戦略

ベースラインのサンプリング戦略は、以下のように実装されます：

- **訓練データからのサンプリング**: 訓練データセットからランダムにサンプリング
- **分布の近似**: 訓練データセットの分布を近似
- **バッチ処理**: 複数のベースラインを同時にサンプリング

##### 期待値の計算

期待値の計算は、以下のように実装されます：

- **モンテカルロ法**: モンテカルロ法で期待値を近似
- **サンプル数**: 十分な数のサンプルを取得
- **収束の保証**: サンプル数を増やすことで収束を保証

##### 計算効率

計算効率を向上させるため、以下の工夫が行われています：

- **バッチ処理**: 複数のベースラインを同時に処理
- **勾配の再利用**: 可能な限り勾配を再利用
- **近似手法**: 必要に応じて近似手法を使用

#### 2. Axiomatic Attribution Priors

##### 公理的な制約

論文では、**公理的な制約（Axiomatic Attribution Priors）**をモデルの訓練に組み込むことを提案しています：

- **完全性**: アトリビューションの合計が予測の変化に等しい
- **対称性**: 対称な入力に対して対称なアトリビューション
- **感度**: 予測に影響を与える特徴に非ゼロのアトリビューション

##### 正則化項

公理的な制約を正則化項として組み込みます：

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \mathcal{L}_{\text{attribution}}$$

ここで：
- $\mathcal{L}_{\text{task}}$: タスクの損失関数
- $\mathcal{L}_{\text{attribution}}$: アトリビューションの損失関数
- $\lambda$: 正則化パラメータ

##### モデル性能の改善

公理的な制約を組み込むことで、モデル性能が改善されます：

- **一般化性能**: 一般化性能が向上
- **説明可能性**: 説明可能性が向上
- **ロバスト性**: ロバスト性が向上

---

### 実験と評価

#### 1. 実験設定

##### データセット

- **画像分類**: ImageNet、CIFAR-10など
- **医療データ**: 医療画像データなど

##### ベースライン手法

論文では、以下のベースライン手法と比較しています：

- **Integrated Gradients**: 単一のベースライン（ゼロベクトルなど）を使用
- **GradientShap**: SHAP値に基づく説明
- **Expected Gradients**: 分布ベースラインを使用

#### 2. 主要な結果

##### Expected Gradientsの優位性

- **説明の安定性**: Expected Gradientsがより安定した説明を生成
- **アーティファクトの減少**: アーティファクトが減少
- **恣意性の排除**: ベースライン選択の恣意性が排除

##### モデル性能の改善

- **一般化性能**: 公理的な制約を組み込むことで、一般化性能が向上
- **説明可能性**: 説明可能性が向上
- **ロバスト性**: ロバスト性が向上

#### 3. 実践的な示唆

##### ベースライン選択の推奨事項

- **分布ベースライン**: 可能であれば、分布ベースラインを使用する
- **複数サンプル**: 十分な数のサンプルを取得する
- **計算効率**: 計算効率を考慮した実装を行う

##### モデル訓練への応用

- **公理的な制約**: 公理的な制約をモデル訓練に組み込む
- **正則化**: 適切な正則化パラメータを選択する
- **バランス**: タスク性能と説明可能性のバランスを取る

---

### 論文の意義と影響

#### 1. Expected Gradientsの提案

この論文は、**Expected Gradientsを提案**し、ベースライン選択の問題を解決しました：

- **恣意性の排除**: ベースライン選択の恣意性を排除
- **安定した説明**: より安定した説明を生成
- **実装の提供**: 実装を提供し、再現性を確保

#### 2. モデル性能の改善

公理的な制約をモデル訓練に組み込むことで、**モデル性能が改善**されました：

- **一般化性能**: 一般化性能が向上
- **説明可能性**: 説明可能性が向上
- **ロバスト性**: ロバスト性が向上

#### 3. 説明可能性研究への影響

この論文は、説明可能性研究に以下のような影響を与えました：

- **ベースライン選択の見直し**: 単一ベースラインの見直しを促した
- **分布ベースライン**: 分布ベースラインの重要性を認識させた
- **モデル訓練への統合**: 説明可能性をモデル訓練に統合する新しいアプローチを提供

---

### 技術的な革新点

#### 1. Expected Gradientsの提案

この論文の最大の革新点は、**Expected Gradientsを提案**したことです：

- **分布ベースライン**: 訓練データセットの分布からベースラインをサンプリング
- **期待値の計算**: 複数のベースラインからの期待値を計算
- **恣意性の排除**: ベースライン選択の恣意性を排除

#### 2. Axiomatic Attribution Priors

公理的な制約をモデル訓練に組み込むことを提案しました：

- **公理的な制約**: 完全性、対称性、感度などの公理を制約として組み込み
- **正則化項**: 公理的な制約を正則化項として実装
- **モデル性能の改善**: モデル性能が改善されることを示した

#### 3. 実装の提供

実装を提供し、再現性を確保しました：

- **アルゴリズムの詳細**: Expected Gradientsのアルゴリズムの詳細を記述
- **実装の公開**: 実装を公開し、再現性を確保
- **実験の詳細**: 実験の詳細を記述

---

### 限界と今後の課題

#### 1. 計算コスト

- **サンプリングのコスト**: 分布からサンプリングするコスト
- **期待値の計算**: 期待値を計算するコスト
- **効率化**: より効率的な実装の開発

#### 2. 分布の推定

- **分布の近似**: 訓練データセットの分布を適切に近似する必要がある
- **高次元データ**: 高次元データでの分布推定の困難さ
- **近似手法**: 分布推定の近似手法の開発

#### 3. モデル訓練への統合

- **正則化パラメータ**: 適切な正則化パラメータの選択
- **バランス**: タスク性能と説明可能性のバランス
- **スケーラビリティ**: 大規模なモデルでの適用

---

### 実践的な示唆

#### 1. ベースライン選択の推奨事項

論文の結果から、以下の推奨事項が導き出されます：

- **分布ベースライン**: 可能であれば、分布ベースラインを使用する
- **複数サンプル**: 十分な数のサンプルを取得する
- **計算効率**: 計算効率を考慮した実装を行う

#### 2. モデル訓練への応用

- **公理的な制約**: 公理的な制約をモデル訓練に組み込む
- **正則化**: 適切な正則化パラメータを選択する
- **バランス**: タスク性能と説明可能性のバランスを取る

#### 3. 実装時の注意点

- **サンプリング戦略**: 適切なサンプリング戦略を選択する
- **期待値の計算**: 十分な数のサンプルを取得する
- **収束の保証**: 期待値の計算が収束することを保証する

---

### まとめ

「Improving performance of deep learning models with axiomatic attribution priors and expected gradients」は、Expected Gradients (EG) の提案論文です。単一のベースライン（例：ゼロベクトル）を選ぶことの恣意性を排除するため、訓練データセット全体からサンプリングした参照点との差分の期待値を取る手法を確立しました。本論文は、ベースライン選択の問題を解決し、説明可能性とモデル性能の両方を改善する重要な貢献をしています。

この論文の影響は大きく、多くの研究でExpected Gradientsが使用されるようになり、分布ベースラインの重要性が認識されるようになりました。今後の研究では、計算効率の向上、分布推定の改善、モデル訓練への統合の最適化などが重要な課題となるでしょう。
