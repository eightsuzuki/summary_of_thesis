# Captum: A unified and generic model interpretability library for PyTorch

**著者**: Narine Kokhlikyan (Facebook AI Research), Vivek Miglani (Facebook AI Research), Miguel Martin (Facebook AI Research), Edward Wang (Facebook AI Research), Alsdair Alsdair (Facebook AI Research), Orion Reblitz-Richardson (Facebook AI Research), et al.  
**arXiv**: [arXiv:2009.07896](https://arxiv.org/abs/2009.07896)  
**発表年**: 2020年

---

### 概要

この論文は、**Facebook AI (Meta) が開発した「Captum」という統一的なモデル解釈可能性ライブラリ**を紹介した研究です。Captumは、PyTorchモデルに対する様々な解釈可能性手法を統一的に提供するライブラリであり、**NLPタスクのチュートリアルや実装例で「<PAD>トークンをベースラインとして使用する」方法を提供したことで、研究・実務においてこの手法が広く定着**しました。なお、論文自体で<PAD>トークンをベースラインとして「提案」したわけではなく、ライブラリの実装やチュートリアルで使用例として提供されたものです。本論文は、解釈可能性手法の実装を標準化し、研究と実務の橋渡しをした重要な貢献をしています。

---

### 論文の核心：統一的な解釈可能性ライブラリ

#### 1. 研究の背景と動機

##### 解釈可能性手法の多様性

深層学習モデルの解釈可能性を高めるため、様々な手法が提案されていますが、その実装は統一されていません：

- **勾配ベース手法**: Integrated Gradients、Gradient × Input、DeepLIFTなど
- **摂動ベース手法**: LIME、SHAP、Occlusionなど
- **注意ベース手法**: Attention weights、Attention rolloutなど
- **内部表現の分析**: Neuron conductance、Activationなど

##### 実装の困難さ

解釈可能性手法の実装には、以下の困難があります：

- **実装の複雑さ**: 各手法の実装が複雑で、統一的なインターフェースがない
- **フレームワーク依存性**: 異なるフレームワークで異なる実装が必要
- **ドキュメントの不足**: 実装方法に関するドキュメントが不足している

#### 2. Captumの設計思想

##### 統一的なインターフェース

Captumは、様々な解釈可能性手法を統一的なインターフェースで提供します：

- **Attribution**: 入力特徴量の重要度を計算する手法
- **Integrated Gradients**: ベースラインから入力までの経路に沿って勾配を積分
- **GradientShap**: SHAP値に基づく説明
- **DeepLIFT**: 参照点からの差分を伝播

##### 柔軟性と拡張性

Captumは、以下の点で柔軟性と拡張性を提供します：

- **カスタムベースライン**: ユーザーが独自のベースラインを定義可能
- **レイヤー単位の分析**: 特定のレイヤーでの分析が可能
- **バッチ処理**: 複数のサンプルを同時に処理可能

---

### 技術的な詳細

#### 1. Captumのアーキテクチャ

##### 主要コンポーネント

Captumは、以下の主要コンポーネントで構成されています：

- **Attribution**: 入力特徴量の重要度を計算する基本クラス
- **IntegratedGradients**: Integrated Gradientsの実装
- **GradientShap**: Gradient SHAPの実装
- **DeepLIFT**: DeepLIFTの実装
- **Occlusion**: Occlusionベースの説明

##### ベースラインの扱い

Captumでは、ベースラインを柔軟に扱えます：

- **デフォルトベースライン**: ゼロベクトルや<PAD>トークンなど
- **カスタムベースライン**: ユーザーが独自のベースラインを定義可能
- **分布ベースライン**: 複数のベースラインからの期待値

#### 2. NLPタスクでの実装例とチュートリアル

##### <PAD>トークンをベースラインとして使用する実装例

CaptumのNLPタスクのチュートリアルや実装例では、**<PAD>トークンをベースラインとして使用する**方法が提供されています：

- **TokenReferenceBaseクラス**: `TokenReferenceBase(reference_token_idx=PAD_IND)`を使用して、PADトークンを参照トークンとして設定
- **TextTokenInputクラス**: baseline引数として、padding tokenのトークンIDを指定可能（デフォルトは0、通常unknown tokenに対応）
- **チュートリアル例**: IMDBレビューのチュートリアルなどで、`PAD_IND = TEXT.vocab.stoi[TEXT.pad_token]`を使用した例が示されている

```python
# 例: Integrated Gradients with <PAD> baseline (チュートリアルの実装例)
from captum.attr import IntegratedGradients
from captum.attr._utils.common import TokenReferenceBase

ig = IntegratedGradients(model)
PAD_IND = tokenizer.pad_token_id
reference = TokenReferenceBase(reference_token_idx=PAD_IND)
baseline = reference.generate_reference(input_ids.shape[1])
attributions = ig.attribute(input_ids, baselines=baseline)
```

**注意**: 論文自体では<PAD>トークンをベースラインとして「提案」したわけではなく、ライブラリの実装やチュートリアルで使用例として提供されたものです。他のベースライン（ゼロベクトル、unknown token、訓練データの分布など）も選択肢として存在します。

##### 実装例の影響

- **研究への影響**: 多くの研究で、Captumの実装例が参照され、<PAD>トークンをベースラインとして使用する手法が広く使われるようになった
- **実務への影響**: 実務でも、Captumのチュートリアルが参照され、この手法が採用されるようになった
- **ベースライン選択の標準化**: <PAD>トークンをベースラインとして使用することが事実上の標準となった（ただし、これが最適とは限らない）

#### 3. 実装の特徴

##### PyTorchとの統合

Captumは、PyTorchと完全に統合されています：

- **自動微分**: PyTorchの自動微分機能を活用
- **テンソル操作**: PyTorchのテンソル操作を直接使用
- **モデル互換性**: 任意のPyTorchモデルに適用可能

##### 計算効率

Captumは、計算効率を考慮した実装を提供します：

- **バッチ処理**: 複数のサンプルを同時に処理
- **勾配の再利用**: 可能な限り勾配を再利用
- **メモリ効率**: メモリ効率を考慮した実装

---

### 実験と評価

#### 1. 使用例

##### 画像分類タスク

- **モデル**: ResNet、VGGなど
- **手法**: Integrated Gradients、GradientShapなど
- **ベースライン**: 黒画像、平均画像など

##### NLPタスク

- **モデル**: BERT、GPTなど
- **手法**: Integrated Gradients、DeepLIFTなど
- **ベースライン**: <PAD>トークン、ゼロ埋め込みなど

#### 2. 実装の検証

##### 正確性の検証

- **理論との一致**: 実装が理論と一致することを検証
- **既存実装との比較**: 既存の実装と結果を比較
- **エッジケースの処理**: エッジケースを適切に処理

##### パフォーマンスの評価

- **計算時間**: 計算時間を測定
- **メモリ使用量**: メモリ使用量を測定
- **スケーラビリティ**: 大規模なモデルでの性能を評価

---

### 論文の意義と影響

#### 1. 実装の標準化

この論文は、解釈可能性手法の実装を標準化しました：

- **統一的なインターフェース**: 様々な手法を統一的なインターフェースで提供
- **再現性の向上**: 実装を標準化することで、再現性が向上
- **研究の加速**: 標準的な実装により、研究が加速

#### 2. ベースライン選択の実装例の提供

Captumの実装例やチュートリアルにより、**<PAD>トークンをベースラインとして使用する手法が広く定着**しました：

- **実装例の提供**: チュートリアルで<PAD>トークンをベースラインとして使用する実装例を提供
- **事実上の標準**: <PAD>トークンをベースラインとして使用することが事実上の標準となった（ただし、論文で「提案」したわけではなく、実装例として提供されたもの）
- **実践への影響**: 多くの研究や実務で、この手法が使用されるようになった
- **議論のきっかけ**: この実装例が、ベースライン選択に関する議論のきっかけとなった（特に、<PAD>トークンが最適かどうかという議論）

#### 3. 研究と実務の橋渡し

Captumは、研究と実務の橋渡しをしました：

- **実装の提供**: 研究で提案された手法を実装として提供
- **ドキュメント**: 詳細なドキュメントとチュートリアルを提供
- **コミュニティ**: 活発なコミュニティを形成

---

### 技術的な革新点

#### 1. 統一的なインターフェース

Captumの最大の革新点は、**様々な解釈可能性手法を統一的なインターフェースで提供**したことです：

- **一貫性**: 異なる手法を一貫した方法で使用可能
- **学習コストの削減**: 新しい手法を学ぶコストが削減
- **比較の容易さ**: 異なる手法を容易に比較可能

#### 2. 柔軟なベースライン選択

Captumは、柔軟なベースライン選択を可能にします：

- **デフォルトベースライン**: 一般的なベースラインをデフォルトで提供
- **カスタムベースライン**: ユーザーが独自のベースラインを定義可能
- **分布ベースライン**: 複数のベースラインからの期待値

#### 3. PyTorchとの完全統合

Captumは、PyTorchと完全に統合されています：

- **自動微分**: PyTorchの自動微分機能を活用
- **テンソル操作**: PyTorchのテンソル操作を直接使用
- **モデル互換性**: 任意のPyTorchモデルに適用可能

---

### 限界と今後の課題

#### 1. ベースライン選択の標準化

- **デフォルトの見直し**: <PAD>トークンをデフォルトとして使用することの見直しが必要
- **ガイドライン**: ベースライン選択に関するガイドラインの提供
- **自動選択**: 適切なベースラインを自動的に選択する機能

#### 2. フレームワーク依存性

- **PyTorch専用**: 現在はPyTorch専用で、他のフレームワークには対応していない
- **TensorFlow対応**: TensorFlowへの対応が求められる
- **JAX対応**: JAXへの対応も検討される

#### 3. ドキュメントとチュートリアル

- **多様なタスク**: より多様なタスクでのチュートリアルの提供
- **ベストプラクティス**: ベストプラクティスの明確化
- **トラブルシューティング**: トラブルシューティングガイドの充実

---

### 実践的な示唆

#### 1. ベースライン選択の推奨事項

Captumを使用する際は、以下の点に注意が必要です：

- **デフォルトの理解**: デフォルトのベースライン（<PAD>トークンなど）が適切かどうかを検討
- **カスタムベースライン**: 必要に応じて、カスタムベースラインを定義
- **複数ベースラインの比較**: 可能であれば、複数のベースラインを比較

#### 2. 実装時の注意点

- **ドキュメントの確認**: 使用する手法のドキュメントを確認
- **ベースラインの記録**: 使用したベースラインを明確に記録
- **結果の検証**: 説明結果が期待通りかどうかを検証

#### 3. 研究への応用

- **再現性**: Captumを使用することで、再現性が向上
- **比較**: 異なる手法を容易に比較可能
- **標準化**: 標準的な実装により、研究が加速

---

### まとめ

「Captum: A unified and generic model interpretability library for PyTorch」は、Facebook AI (Meta) が開発した統一的なモデル解釈可能性ライブラリを紹介した重要な研究です。Captumは、様々な解釈可能性手法を統一的なインターフェースで提供し、特にNLPタスクのチュートリアルや実装例で「<PAD>トークンをベースラインとして使用する」方法を提供したことで、研究・実務においてこの手法が広く定着しました。なお、論文自体で<PAD>トークンをベースラインとして「提案」したわけではなく、ライブラリの実装やチュートリアルで使用例として提供されたものです。この論文は、解釈可能性手法の実装を標準化し、研究と実務の橋渡しをした重要な貢献をしています。

Captumの影響は大きく、多くの研究や実務で使用されるようになりました。今後の研究では、ベースライン選択の標準化、他のフレームワークへの対応、ドキュメントとチュートリアルの充実などが重要な課題となるでしょう。
