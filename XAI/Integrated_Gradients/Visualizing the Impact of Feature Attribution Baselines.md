# Visualizing the Impact of Feature Attribution Baselines

**著者**: Pascal Sturmfels (University of Washington), Scott Lundberg (Microsoft Research), Su-In Lee (University of Washington)  
**発表**: Distill Journal, 2020年1月10日  
**URL**: [distill.pub/2020/attribution-baselines](https://distill.pub/2020/attribution-baselines)

---

### 概要

この論文は、**Integrated Gradientsなどのパス属性法（path attribution methods）におけるベースライン（baseline）選択が生成される説明に与える影響を視覚的・定性的に検証**した重要な研究です。画像とテキストの両方で、**ゼロベクトル（Zero Baseline）がアーティファクト（誤った説明）を生むことを視覚的に証明**し、特にNLPにおいてゼロ埋め込みが**OOD（学習分布外、Out-of-Distribution）**であることを指摘しました。この論文は、ベースライン選択の重要性を明確に示し、多くの実践でデフォルトとして使われるゼロベクトルや黒画像を用いた定数ベースラインの安易な使用を再考させる重要な貢献をしています。

---

### 論文の核心：ベースライン選択の重要性

#### 1. 研究の背景と動機

##### ベースラインとは

パス属性法（例：Integrated Gradients）では、入力 $x$ をどこから始めるかを示す基準点 $x'$（ベースライン）が必要です。ベースラインは「欠如（missingness）」を表現するものとして扱われ、入力特徴量が「存在しない」状態を表します。

##### ベースライン選択の問題

ベースライン選択は、説明の出力・意味合いを大きく左右し、誤解やアーティファクト（誤った説明）の原因となる可能性があります。特に：

- **定数ベースライン（ゼロベクトル、黒画像など）**: ベースラインと同じ色・概念の特徴が「無効化」され、重要度がゼロになってしまう
- **OOD問題**: ベースラインがモデルの学習分布外（OOD）である場合、説明の信頼性が歪められる
- **視覚的な誤解**: ベースライン選択により、実際には重要でない特徴が強調されたり、重要な特徴が無視されたりする

#### 2. 主要な発見

##### 画像分類タスクでの発見

- **黒画像ベースラインの問題**: 黒画像をベースラインにすると、黒い特徴（例：黒い毛皮、黒い背景）が重要度ゼロになり、実際には重要な特徴を見落とす
- **白画像ベースラインの問題**: 白画像をベースラインにすると、白い特徴が無視される
- **定数ベースラインの限界**: 定数ベースラインは、baseline色と同じ特徴を強調できず、重要な部分を見落とすことがある

##### NLPタスクでの発見

- **ゼロ埋め込みのOOD問題**: NLPにおいて、ゼロ埋め込み（zero embedding）はモデルの学習分布外（OOD）である
- **意味のないベースライン**: ゼロ埋め込みは、モデルが学習時に見たことのない入力であり、意味のある比較基準として機能しない
- **アーティファクトの生成**: ゼロ埋め込みをベースラインに使用すると、誤った説明（アーティファクト）が生成される

---

### 技術的な詳細

#### 1. Integrated Gradientsの定義

Integrated Gradientsは、ベースライン $x'$ から入力 $x$ までの経路に沿って勾配を積分する手法です：

$$\phi_i^{IG}(f, x, x') = (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial f(x' + \alpha (x - x'))}{\partial x_i} d\alpha$$

ここで：
- $f$: モデルの出力関数
- $x_i$: 入力特徴量 $i$ の値
- $x'_i$: ベースライン特徴量 $i$ の値
- $\alpha$: 経路上のパラメータ（0から1まで）

**重要な点**: baselineとの差分 $(x_i - x'_i)$ の項が、baselineそのものの値を消してしまう要因となります。つまり、$x'_i = 0$（ゼロベースライン）の場合、$x_i = 0$ の特徴量は重要度がゼロになってしまいます。

#### 2. Expected Gradientsの導入

本論文では、**Expected Gradients**という拡張手法も紹介しています。これは、複数のベースラインを分布 $D$ からサンプリングし、経路上の勾配を積分・期待値化する手法です：

$$\phi_i^{EG}(f, x, D) = \mathbb{E}_{x' \sim D} \left[ (x_i - x'_i) \times \int_{\alpha=0}^1 \frac{\partial f(x' + \alpha (x - x'))}{\partial x_i} d\alpha \right]$$

この手法により、単一のベースラインに依存せず、より安定した説明を生成できます。

#### 3. 完全性性質（Completeness Axiom）

Integrated Gradientsは、以下の完全性性質を満たします：

$$\sum_i \phi_i^{IG}(f, x, x') = f(x) - f(x')$$

これは、ベースラインをどのように選んでも理論上保たれる性質です。しかし、この性質だけでは、ベースライン選択が説明の質に与える影響を保証できません。

---

### 実験と評価

#### 1. 実験設定

##### モデルとデータセット

- **画像分類**: Inception V4（ImageNet）を使用
- **評価方法**: 
  - サリエンシーマップの視覚的評価
  - トップK特徴のアブレーション（重要度上位の画素を削除またはぼかす）
  - 重心アブレーション（saliencyの重心を中心に領域を削除）

##### ベースラインの種類

論文では、以下のベースラインを比較しました：

1. **定数色（黒、白など）**: すべての画素が同じ値（0または1）
2. **ぼかし画像（blurred baseline）**: 入力画像をぼかしたもの
3. **一様ノイズ（uniform baseline）**: 一様分布からサンプリングされたノイズ
4. **ガウシアンノイズ（gaussian baseline）**: ガウシアン分布からサンプリングされたノイズ
5. **訓練データ分布からのサンプリング（training data baseline）**: 訓練データからランダムにサンプリングされた画像

#### 2. 主要な結果

##### 画像分類での発見

- **定数ベースラインの限界**: 黒画像や白画像をベースラインにすると、同じ色の特徴が無視される
- **訓練データベースラインの優位性**: 訓練データからの分布を用いたbaselineや一様／ガウスノイズ系、ぼかし画像がより直感に沿ったサリエンシーを生みやすい
- **視覚的な評価の限界**: 定性的な視覚評価は有用だが、評価者の先入観に左右されやすく、「人が正しいと思う説明」がモデルが本当に使っている特徴と一致するとは限らない

##### NLPでの発見

- **ゼロ埋め込みのOOD問題**: NLPにおいて、ゼロ埋め込みはモデルの学習分布外であり、意味のある比較基準として機能しない
- **アーティファクトの生成**: ゼロ埋め込みをベースラインに使用すると、誤った説明が生成される
- **代替ベースライン**: 訓練データからサンプリングされた埋め込みや、平均埋め込みなどがより適切

#### 3. アブレーション評価の限界

論文では、アブレーション評価（重要度の高い特徴を削除して予測の変化を観察）の限界も指摘しています：

- **OOD問題**: アブレーションにより、入力が訓練分布外（OOD）になる可能性がある
- **説明の信頼性の歪み**: OODの影響により、説明の信頼性が歪められる可能性がある
- **評価設計の重要性**: アブレーション評価を行う際は、入力がモデルの訓練分布から離れすぎないよう注意が必要

---

### 論文の意義と影響

#### 1. アーティファクトの可視化

この論文の最大の貢献は、**ベースラインが原因となる誤った説明（アーティファクト）を視覚的に示した**ことです：

- **画像での可視化**: 黒画像や白画像をベースラインにした場合のアーティファクトを明確に示した
- **テキストでの可視化**: ゼロ埋め込みをベースラインにした場合のOOD問題を指摘
- **他領域への示唆**: 画像だけでなく、NLPなど他領域への示唆も強い

#### 2. ベースライン選択の重要性の強調

この論文は、**ベースライン選択の重要性を明確に示し**、多くの実践でデフォルトとして使われるゼロベクトルや黒画像を用いた定数ベースラインの安易な使用を再考させました：

- **デフォルトの見直し**: ゼロベクトルをデフォルトとして使用することの危険性を指摘
- **適切なベースラインの選択**: 訓練データ分布に基づくベースラインの重要性を強調
- **実践への影響**: 多くの研究や実務で、ベースライン選択がより慎重に検討されるようになった

#### 3. 解釈可能性研究への影響

この論文は、解釈可能性研究に以下のような影響を与えました：

- **評価方法の見直し**: アブレーション評価の限界を指摘し、より適切な評価方法の必要性を示した
- **視覚的評価の限界**: 視覚的評価の主観性と限界を明確にした
- **理論と実践の橋渡し**: 理論的な性質（完全性など）と実践的な説明の質の違いを明確にした

---

### 技術的な革新点

#### 1. 視覚的な証明

この論文の最大の革新点は、**ベースライン選択の影響を視覚的に証明した**ことです：

- **直感的な理解**: 数式だけでなく、視覚的な例を通じて問題を理解しやすくした
- **実証的な検証**: 理論的な議論だけでなく、実際のモデルでの検証を行った
- **再現性**: Distill Journalという形式により、インタラクティブな可視化を提供

#### 2. OOD問題の指摘

NLPにおいて、**ゼロ埋め込みがOODであることを明確に指摘**したことは重要な貢献です：

- **分布の不一致**: ゼロ埋め込みがモデルの学習分布外であることを指摘
- **意味のない比較**: OODなベースラインが意味のある比較基準として機能しないことを示した
- **代替手法の提案**: より適切なベースライン選択方法を提案

#### 3. 多様なベースラインの比較

様々なベースラインを体系的に比較し、それぞれの特性を明らかにしました：

- **定数ベースライン**: 黒、白など
- **ノイズベースライン**: 一様ノイズ、ガウシアンノイズ
- **データベースライン**: 訓練データからのサンプリング
- **ぼかしベースライン**: 入力画像をぼかしたもの

---

### 限界と今後の課題

#### 1. 評価方法の限界

- **視覚的評価の主観性**: 定性的な視覚評価は主観的で、評価者の先入観に左右される
- **アブレーション評価のOOD問題**: アブレーションにより入力がOODになる可能性
- **定量的評価の必要性**: より客観的な定量的評価方法の開発が必要

#### 2. ベースライン選択のガイドライン

- **タスク依存性**: 最適なベースラインはタスクやモデルに依存する可能性
- **汎用的なガイドライン**: 様々なタスクに適用可能なベースライン選択のガイドラインが必要
- **自動選択**: 適切なベースラインを自動的に選択する手法の開発

#### 3. 理論的な理解

- **ベースラインと説明の関係**: ベースライン選択が説明の質に与える影響の理論的理解
- **最適なベースライン**: 理論的に最適なベースラインの定義と選択方法
- **評価基準**: 説明の質を評価する客観的な基準の開発

---

### 実践的な示唆

#### 1. ベースライン選択の推奨事項

論文の結果から、以下の推奨事項が導き出されます：

- **訓練データ分布の使用**: 可能であれば、訓練データの分布を用いたベースラインを使用する
- **複数ベースラインの平均**: Expected Gradientsのように、複数のベースラインを平均する手法が好ましい
- **定数ベースラインの回避**: ゼロベクトルや黒画像などの定数ベースラインは、アーティファクトを生む可能性があるため、慎重に使用する

#### 2. 評価方法の改善

- **OODの考慮**: アブレーション評価を行う際は、入力がモデルの訓練分布から離れすぎないよう注意する
- **複数の評価方法**: 視覚的評価だけでなく、複数の評価方法を組み合わせる
- **定量的評価**: 可能な限り定量的な評価方法を使用する

#### 3. 実装時の注意点

- **デフォルトの見直し**: ゼロベクトルをデフォルトとして使用しない
- **ドキュメント化**: 使用したベースラインを明確にドキュメント化する
- **検証**: ベースライン選択が説明に与える影響を検証する

---

### まとめ

「Visualizing the Impact of Feature Attribution Baselines」は、Integrated Gradientsなどのパス属性法におけるベースライン選択の重要性を視覚的に証明した画期的な研究です。画像とテキストの両方で、ゼロベクトル（Zero Baseline）がアーティファクト（誤った説明）を生むことを示し、特にNLPにおいてゼロ埋め込みがOOD（学習分布外）であることを明確に指摘しました。この論文は、ベースライン選択の重要性を明確に示し、多くの実践でデフォルトとして使われるゼロベクトルや黒画像を用いた定数ベースラインの安易な使用を再考させ、解釈可能性研究に大きな影響を与えました。

この論文の影響は大きく、多くの研究や実務で、ベースライン選択がより慎重に検討されるようになりました。今後の研究では、より適切なベースライン選択方法の開発、評価方法の改善、理論的な理解の深化などが重要な課題となるでしょう。
