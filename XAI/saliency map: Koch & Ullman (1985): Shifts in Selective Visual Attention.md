# Shifts in Selective Visual Attention: towards the underlying neural circuitry (Koch & Ullman, 1985)

**著者**: Christof Koch, Shimon Ullman  
**公開**: 1985  
**リンク**: [Koch & Ullman (1985) PDF](https://cseweb.ucsd.edu/classes/fa09/cse258a/papers/koch-ullman-1985.pdf)

本論文は、ヒト/霊長類視覚の**選択的注意**の計算理論を示し、初期視覚の複数の特徴マップから「顕著性（conspicuity）」に基づいて**単一点を選択**する神経回路（Winner-Take-All: WTA と抑制戻り）を提案します。選択された位置の情報が**非トポグラフィックな中心表現**に写像され、複数特徴の融合や高次処理が行われる、という枠組みを与えます。

---

### 1. 新規性：この論文の貢献
- **多特徴・並列処理 → 単一点選択**の二段階仮説を神経回路で具現化（初期表現と中心表現の分離）。
- **顕著性に基づくWTA選択**と**抑制戻り（Inhibition-of-Return; IOR）**により、シーン内の最顕著→次顕著…へと自動走査する仕組みを提案。
- **近接・類似ルール**などの選択バイアスを、局所的相互作用で実装可能であることを議論。
- 大量の心理物理・生理学的知見（ポズナー課題、SC/PP/ V4の注意効果など）と整合する計算枠組み。

---

### 2. 理論/手法の核心：数式で定式化

#### 2.1 初期表現（特徴マップ）
色・方位・運動方向・両眼視差などの特徴ごとにトポグラフィックなマップ \(F_f(p)\)（位置 \(p\)）を計算。特徴ごとの正規化/バンドパス等を経て、**顕著性（conspicuity）**を統合：
\[
S(p) \,=\, \sum_{f} \alpha_f\, \mathcal{N}\big(F_f(p)\big),\quad \alpha_f\ge 0.
\]
ここで \(\mathcal{N}(\cdot)\) はスケール/特徴間の正規化（後のItti & Koch の顕著性マップに連なる考え）。

#### 2.2 WTA（Winner-Take-All）による位置選択
顕著性マップ \(S\) から1点を選ぶ。単純な規則は
\[
 p^* \,=\, \arg\max_{p} S(p).
\]
神経回路としては、側方抑制付きユニット \(x_p\) の力学系で実装：
\[
\tau\, \frac{\mathrm{d}x_p}{\mathrm{d}t} \,=\, -x_p + g\!\left(S(p) - \gamma \sum_{q\neq p} w_{pq} x_q \right),\quad x_p\ge 0,
\]
- \(g(\cdot)\): 単調非線形（例: ReLU/シグモイド）
- \(w_{pq}\): 近傍に強い抑制（側方抑制）。
十分時間が経つと唯一のユニットが活性化し（\(x_{p^*}\)が最大）、WTA選択が達成される。

#### 2.3 抑制戻り（Inhibition-of-Return; IOR）
選択後、同じ位置に注意が固定し続けないよう、選択点を抑制：
\[
 S(p) \leftarrow S(p) - \beta\, K\big(\lVert p - p^*\rVert\big),\quad \beta>0,
\]
\(K\) は局所抑制核。これにより次の時刻には2番目に顕著な位置が選ばれ、シーンを自動走査する。

#### 2.4 中心表現への選択的写像（ゲーティング）
選択位置の情報のみを非トポグラフィック中心表現 \(c\) に写像：
\[
 c \,=\, G\big(\{F_f(p^*)\}_f\big),\quad \text{例）} G:\text{特徴統合/連結}
\]
形式的には、選択マスク \(\delta(p-p^*)\) を用いたサンプリング \(F_f(p)\,\delta(p-p^*)\) の読出しと同値。複数特徴が**一貫した単一点**として統合される。

#### 2.5 近接・類似ルール（バイアス）
近接/類似刺激を優先する場合、側方結合を調整：
\[
 w_{pq} \,=\, w_{\text{space}}\big(\lVert p-q\rVert\big) \;+\; \lambda\, w_{\text{feature}}\big(\lVert \phi(p)-\phi(q)\rVert\big),
\]
\(\phi(\cdot)\) は特徴ベクトル。\(\lambda>0\) で類似性に基づく相互作用を強め、群化や連結性（輪郭追跡等）を実現可能。

---

### 3. 生理・心理実験との接続（抜粋）
- ポズナー型事前手掛かり課題：WTA/IORにより事前注意の利益を説明（[Posner 1980] 等）。
- 上丘/頭頂連合野/V4 の選択的増強：選択位置の応答増強は中心表現への写像・ゲーティングと整合。
- 「ポップアウト」と直列探索：単一特徴は高い \(S(p)\) で並列検出、結合探索はWTA+IORで逐次走査。

---

### 4. 実装ノート（現代的観点）
- 数値安定化：\(g\) の飽和や \(\gamma\) の過大で多点発火/発散を避ける。\(w_{pq}\) は中心強抑制・周辺弱抑制のドーナツ型が有効。
- スケール融合：\(F_f\) を多解像度で構築し、\(\mathcal{N}\) でスケール間正規化すると頑健。
- 視覚タスク連携：注意位置 \(p^*\) を用いて上位処理（認識・カウント・追跡）に順次入力。

---

### 5. 「キモ」と重要性
- **キモ**: 多特徴の並列初期処理 → 顕著性に基づくWTA選択 → IORで次点へ移動、という**単純で一般的な注意回路**を提示。単一点の選択写像により、複数特徴の融合と計算資源の節約を両立。
- **重要性**: 後続の顕著性マップ研究（Itti & Koch 1998 など）や深層学習時代の視覚注意/サリエンシ手法の理論基盤として引用され続ける古典的枠組み。

---

### 6. まとめ（要点）
- 顕著性統合：\(S(p)=\sum_f \alpha_f\,\mathcal{N}(F_f(p))\)。
- WTA力学：側方抑制ネットで \(p^* = \arg\max_p S(p)\)。
- IOR：\(S\leftarrow S-\beta K(\lVert p-p^*\rVert)\) で自動走査。
- 選択写像：\(\{F_f(p^*)\}\) を中心表現へゲートして多特徴融合。

参考: [Koch & Ullman (1985)](https://cseweb.ucsd.edu/classes/fa09/cse258a/papers/koch-ullman-1985.pdf)
