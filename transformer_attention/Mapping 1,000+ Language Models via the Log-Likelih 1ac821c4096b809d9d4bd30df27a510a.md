# Mapping 1,000+ Language Models via the Log-Likelihood Vector

**「2 言語モデルをテキスト確率分布空間にマッピングする」** の節の全文翻訳です（省略せずに全文を訳しています citeturn1file2）。

---

# 2 言語モデルをテキスト確率分布空間にマッピングする

本節では、我々の提案手法を提示する。セクション2.2および2.3では、テキスト生成確率から導出されるモデルの特徴ベクトルを紹介する。セクション2.4では、これらの特徴を用いて構築された座標系における二乗ユークリッド距離が、モデル間のカルバック・ライブラー（KL）ダイバージェンスに近似することを示す。さらに、セクション2.5では、得られたモデル座標の解釈について述べる。なお、あるトークン系列を生成するための条件付き確率の系列を用いてモデル座標を定義する手法の拡張は、付録Eに示している。

---

## 2.1 自己回帰型言語モデル

ここで、$X$ をすべての可能なテキストの集合、VV をトークンの語彙とする。テキスト $x \in X$ はトークンの列として表され、すなわち
$$
x = (y_1, \dots, y_n), \quad y_t \in V.
$$
テキストの最大長を $⁡n_{\max}$ とすれば、
$$
X = \bigcup_{n=0}^{n_{\max}} V^n.
$$
また、$K$ 個の言語モデル $\{p_i\}_{i=1}^K$ を考える。ここで、$y_0$ はシーケンス開始（BOS）トークンを表し、各言語モデル $p_i$ は直前のトークン列
$$
y_{t-1} = (y_0, \dots, y_{t-1})
$$
を条件として次のトークン yty_t を予測する。すなわち、pip_i による条件付き確率は
$$
y_t \sim p_i(y_t \mid y_{t-1}), \quad t = 1, \dots, n.
$$
このとき、モデル $p_i$ の下でのテキスト $x$ の確率（$x \sim p_i$ と書く）は
$$
p_i(x) = \prod_{t=1}^{n} p_i(y_t \mid y_{t-1})
$$
と与えられる。

さらに、理論的な議論のために、基底分布を表す言語モデル $p_0$ も導入する。ここでは、各テキストが互いに独立に $p_0$ から抽出された $N$ 個のテキストからなるデータセット（コーパス）
$$
D = (x_1, x_2, \dots, x_N) \in X^N
$$
を仮定する。

---

## 2.2 対数尤度ベクトル

モデル $p_i$ に対して、テキスト $x$ を生成する確率を $p_i(x)$ と表す。統計モデル選択の慣例に従い、$p_i(x)$ を「テキスト $x$ が与えられたときのモデル $p_i$ の尤度」と呼ぶ。これに対し、対数尤度は次のように定義される：

$$\ell_i(x)=\sum_{t=1}^{n} \log p_i(y_t \mid y_{t-1}).$$

実際の言語モデルの実装では、$$-\ell_i(x)$$がテキスト$$x$$のクロスエントロピー損失に対応し、$$\exp(-\ell_i(x)/n)$$はパープレキシティとして知られている。

我々の手法は非常にシンプルである。データセット $D$が$N$個のテキストからなることから、各モデル $p_i$ の特徴ベクトルとして対数尤度ベクトル

$$\ell_i=\bigl( \ell_i(x_1), \dots, \ell_i(x_N) \bigr)^\top \in \mathbb{R}^N$$

を用いる。まず、これら $K$個のモデルの対数尤度ベクトル $\ell_i$を縦に積み上げて、対数尤度行列

$$L = \begin{pmatrix} \ell_1 \\ \vdots \\ \ell_K \end{pmatrix} \in \mathbb{R}^{K \times N}$$

を構築する。

---

## 2.3 ダブルセンタリング

モデル解析の前処理として、我々は行列 $L$に対してダブルセンタリング（Borg and Groenen, 2005）という手法を適用する。まず、各行（各モデルごと）のセンタリングを行う。各行の平均、すなわち平均対数尤度は次のように定義される：

$$\bar{\ell}_i = \frac{1}{N} \sum_{s=1}^{N} \ell_i(x_s).$$

この値を各成分から引くことで、中心化された対数尤度ベクトル $\xi_i = (\xi_{i1}, \dots, \xi_{iN})^\top \in \mathbb{R}^N$を定義する。ここで

$$\xi_{is} := \ell_i(x_s) - \bar{\ell}_i, \quad s = 1, \dots, N.$$

次に、この中心化されたベクトル群$$(\xi_1, \dots, \xi_K)^\top$$に対して、列ごと（各テキストごと）のセンタリングを行う。全モデルの平均ベクトルは

$$\bar{\xi} = \frac{1}{K} \sum_{i=1}^{K} \xi_i$$

と定義され、この平均ベクトルを各$$\xi_i$$から引くことで、ダブルセンタリングされた対数尤度ベクトル

$$q_i = \xi_i - \bar{\xi}$$

を得る。詳細は付録BおよびCを参照せよ。

---

## 2.4 カルバック・ライブラー（KL）ダイバージェンス

カルバック・ライブラー（KL）ダイバージェンスは、確率分布空間において2つのモデル $p_i$ と $p_j$ の差異を測るために頻繁に用いられる指標である。これは以下のように定義される：

$$KL(p_i, p_j) = \sum_{x \in X} p_i(x) \log \frac{p_i(x)}{p_j(x)} = \mathbb{E}_{x \sim p_i} \bigl( \ell_i(x) - \ell_j(x) \bigr). \quad (1)$$

ここで、データセット $D$ は未知の基底モデル $p_0$ から生成され、かつモデル $p_i$ と $p_j$ は $p_0$ を十分に近似していると仮定する。この仮定の下、KLダイバージェンスは次のように近似できる：

$$2\,KL(p_i, p_j) \approx \operatorname{Var}_{x \sim p_0} \bigl( \ell_i(x) - \ell_j(x) \bigr). \quad (2)$$

(1) の定義が $\ell_i(x) - \ell_j(x)$ の期待値を含むのに対し、(2) の近似は分散の形をとる。この結果はやや驚くべきであるが、非常に示唆に富む。特に、KLダイバージェンス自体は非対称であるが、(2) の近似は対称となる。我々は、データセット $D$ に基づいて以下のように (2) を推定する：

$$2\,KL(p_i, p_j) \approx \frac{1}{N} \| \xi_i - \xi_j \|^2. \quad (3)$$

なお、$\| \xi_i - \xi_j \|^2 = \| q_i - q_j \|^2$であるため、(3)が成立する。さらに、もしモデル $p_i$ の座標を $q_i/\sqrt{N}$ とみなすならば、2点間の二乗ユークリッド距離は $2\,KL(p_i, p_j)$ を近似することになる。

---

## 2.5 モデル座標

我々は主に、モデル $p_i$ の特徴ベクトルとして $q_i$ を用い、これをモデル座標と呼ぶ。(3) で示したように、$q$ 座標系における二乗ユークリッド距離は、言語モデル間のKLダイバージェンスを近似する。これは、$q_i$ が確率分布空間内における $p_i$ の位置を表していることを意味する。なお、$\xi_i$ は原点からのオフセットのみが $q_i$ と異なるため、$\xi_i$ もまたモデル座標として機能し、$\| q_i - q_j \|^2 = \| \xi_i - \xi_j \|^2$ となる。しかし、より解釈しやすい成分を持つ $q_i$ を採用するため、本論文では以降 $q_i$ を用いる。

可視化の目的では、モデルマップの座標として主に $\ell_i$ を使用する。これは、$\ell_i$ が直感的に「高さ」方向に $\sqrt{N}\,\bar{\ell}_i$ を、また「水平方向」には $q_i$ をエンコードしていると解釈できるからである。付録D.6に示すように、

$$\| \ell_i - \ell_j \|^2 = \| q_i - q_j \|^2 + N(\bar{\ell}_i - \bar{\ell}_j)^2, \quad (4)$$

すなわち、$\ell$ 座標系における二乗ユークリッド距離は $2\,KL(p_i, p_j)$ と $N(\bar{\ell}_i - \bar{\ell}_j)^2$ の和に分解できる。

---

以上が、セクション2「言語モデルをテキスト確率分布空間にマッピングする」の全文の日本語訳となります。各数式および説明は原文の内容を省略せずに忠実に訳しています citeturn1file2.

以下は、セクション2.4で示されている計算の詳細な説明です。

---

### 1. KLダイバージェンスの定義

まず、言語モデル $p_i$ と $p_j$ について、KLダイバージェンスは次のように定義されます：

$$KL(p_i, p_j) = \sum_{x \in X} p_i(x) \log \frac{p_i(x)}{p_j(x)} = \mathbb{E}_{x \sim p_i} \bigl( \ell_i(x) - \ell_j(x) \bigr). \quad (1)$$

ここで、$\ell_i(x)=\sum_{t=1}^{n} \log p_i(y_t \mid y_{t-1})$は、テキスト$x$に対するモデル$p_i$の対数尤度です。

※ここで、分散$\operatorname{Var}(X)$は$X$の平均からの偏差の二乗の期待値として定義されます。
$$
\xi_i = \ell_i - \bar{\ell}_i\mathbf{1}_N,
$$
すなわち
$$\xi_{is} = \ell_i(x_s) - \bar{\ell}_i$$

と定義します。同様に、モデル$p_j$に対しても$\xi_j$を定義します。

$$\left\| \frac{q_i}{\sqrt{N}} - \frac{q_j}{\sqrt{N}} \right\|^2 = \frac{1}{N}\|q_i - q_j\|^2$$

となり、これがちょうど$2\,KL(p_i, p_j)$に近似されることになります。つまり、スケール済みの座標間の距離が、KLダイバージェンスを表す指標として機能するという解釈が可能です。

$$KL(p_i, p_j) \approx \frac{1}{2}\,\mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr]$$

すなわち

$$2\,KL(p_i, p_j) \approx \mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr] \approx \operatorname{Var}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr).$$

この変換過程では、モデル間の差が小さい（＝$p_i$や$p_j$が$p_0$に非常に近い）という仮定のもと、期待値がほぼゼロであるために二乗項（すなわち分散）が主要な寄与となるという考え方を用いています。