# What Does BERT Look at? An Analysis of BERT's Attention

**著者**: Kevin Clark (Stanford University), Urvashi Khandelwal (Stanford University), Omer Levy (Facebook AI Research), Christopher D. Manning (Stanford University)  
**arXiv**: [arXiv:1906.04341](https://arxiv.org/abs/1906.04341) 
---

## 第1章: Introduction

この章では、研究の背景と目的が述べられています。

- **背景:**
    
    大規模な自己教師あり言語モデル（例：BERT）が多くの下流タスクで非常に高い性能を示すことは広く知られています。しかし、これらのモデルがどのようにして言語の構造（文法、依存関係、照応など）を内部で獲得しているのか、その具体的なメカニズムは完全には理解されていません。従来は、言語モデルの出力（例：言語モデルのサプライズ）や内部のベクトル表現に注目した研究が多く行われていました。
    
- **目的:**
    
    本研究では、BERTの内部で使われている**アテンション機構**に焦点を当て、144個（12層×12ヘッド）のアテンションヘッドの動作パターンを詳細に解析します。各ヘッドがどのようなパターンで単語間の関連性（例：直前・直後、特殊トークン、広範囲への注意分布など）を学習しているのかを調べ、さらに特定のヘッドが文法的な依存関係や照応といった言語現象をどの程度捉えているかを検証します。
    

---

## 第2章: Background: Transformers and BERT

この章では、TransformerアーキテクチャとBERTの基本的な仕組みについて解説しています。

- **Transformerの自己注意機構:**
    
    各入力トークン $h_i$ は、以下のような線形変換によりクエリ $q_i$、キー $k_i$、バリュー $v_i$ に変換されます。
        
    $$
    q_i = W^Q h_i,\quad k_i = W^K h_i,\quad v_i = W^V h_i
    $$
    
    ここで $W^Q$、$W^K$、$W^V$ は学習可能な重み行列です。これらの行列は、モデルの学習過程で勾配降下法により最適化されます。
    
    次に、トークン $i$ が他の各トークン $j$ にどの程度注意を向けるかは、クエリとキーの内積をソフトマックスで正規化することで決まります。
    
    $$
    \alpha_{ij} = \frac{\exp\left(q_i^\top k_j\right)}{\sum_{l=1}^{n} \exp\left(q_i^\top k_l\right)}
    $$
        
    最終的に、各トークンの出力表現 $o_i$ はバリューの重み付き和として得られます。
        
    $$
    o_i = \sum_{j=1}^{n} \alpha_{ij} v_j
    $$
        
- **BERTの特徴:**
    
    BERTは、Transformerをベースにしており、入力前に[CLS]（文全体の表現を集約）や[SEP]（文の区切りを示す）といった特殊トークンを追加します。さらに、事前学習としてMasked Language Modeling（MLM）やNext Sentence Prediction（NSP）のタスクを用いて膨大なデータから学習を行っています。これらの前処理やタスク設定が、後述するアテンションの解析においても重要な役割を果たします。
    

---

## 第3章: Surface-Level Patterns in Attention

この章では、BERTの各アテンションヘッドの「表層的な」パターン、すなわちどのようにして注意が分配されるかという観点からの解析が行われています。

### 3.1 相対位置に基づく注意

- **観察:**
多くのヘッドは、現在のトークン自体にはあまり注意を向けず、直前または直後のトークンに強い注意を与える傾向が見られます。
- **数式の背景:**
これは、各ヘッドが $\alpha_{ij}$ という注意重みを計算する際に、内積 $q_i^\top k_j$ が近接位置のトークンで大きくなるように学習しているためと考えられます。実際、特定の層（例：層2、4、7、8）では前のトークンに50%以上の注意が集中するヘッドが観察されています。

### 3.2 特殊トークン（[CLS], [SEP]）への注意

- **観察:**
ある層のヘッド、特に中～深層では、[SEP]や[CLS]などの特殊トークンに対して、通常の単語以上の注意が向けられることが確認されました。
- **解釈:**
    - たとえば、[SEP]トークンは文中で数回しか現れないため、通常なら均等に注意が分散すると $\frac{1}{64}$ 程度ですが、実際には50%以上の割合で注意が向けられることもあります。
    - 勾配解析（後述）により、[SEP]への注意を変化させても損失 $L$ に与える影響が小さいことが示され、[SEP]は「no-op」として機能している可能性が示唆されます。

### 3.3 注意分布のエントロピー

- **観察:**
各ヘッドの注意分布のエントロピーを計算することで、あるヘッドが特定の単語に集中しているか、あるいは文全体に広く注意を分散しているかを評価しています。
- **エントロピーの意味:**
エントロピーが低い＝特定の単語に強く集中、エントロピーが高い＝広範囲に均等に注意を向けていると解釈されます。特に、低層のヘッドでは「bag-of-vectors」的な広い表現を生成する傾向が見られます。

---

## 第4章: Probing Individual Attention Heads

この章では、各アテンションヘッドがどの程度言語学的な情報（例：依存関係や文法構造）を捉えているかを調べるためのプロービング実験が紹介されています。

### 4.1 方法

- **トークンから単語への変換:**
    
    BERTはbyte-pair tokenizationを用いるため、単語が複数のサブトークンに分割されることがあります。そこで、複数のサブトークンに対する注意重みを合計または平均することで、単語単位の注意分布を再構築します。
    
- **最も注意が向けられた単語の予測:**
    
    各ヘッドについて、ある単語 $j$ に対して最も高い注意重み $\alpha_{ij}$ を持つ単語 $i$ を、その単語の「ヘッド」として予測します。
    
    $$
    \hat{i} = \operatorname{argmax}_i \alpha_{ij}
    $$

### 4.2 依存構文解析への応用

- **評価:**
Wall Street JournalのPenn Treebankのデータを用いて、各ヘッドが依存構造（例えば、決定詞が名詞に、目的語が動詞に）をどの程度正しく予測できるかを評価しています。
- **結果:**
単一のヘッドでは全体的な依存構造を十分に捉えられないものの、特定の依存関係（例：det, dobj, prepなど）に対しては75〜95%と高い精度を示すヘッドが見つかりました。

### 4.3 コアリファレンス解析への応用

- **実験:**
CoNLL-2012のデータセットを用い、各ヘッドがコアリファレンス（照応）において、ある言及がどの先行言及と最も強く結びついているかを予測する性能を評価しました。
- **結果:**
特定のヘッドが、ルールベースの手法に近い、あるいはそれを上回る性能を発揮していることが示されました。

---

## 第5章: Probing Attention Head Combinations

この章では、個々のヘッドだけでなく、複数のアテンションヘッドを組み合わせて全体としての文法的知識をどれだけ捉えているかを評価するためのプロービング分類器が提案されています。

### 5.1 Attention-Only Probe

- **モデル:**
    
    各ヘッドの注意重み $\alpha_{ij}^{(k)}$（ヘッド $k$ の $i$ から $j$ への注意）を線形に組み合わせ、ある単語 $j$ のヘッドとして単語 $i$ が選ばれる確率 $p(i|j)$ を以下のように定義します。
        
    $$
    p(i|j) \propto \exp\left(\sum_{k=1}^{n} w_k\, \alpha_{ij}^{(k)} + u_k\, \alpha_{ji}^{(k)}\right)
    $$
        
    ここで、$w_k$ と $u_k$ は学習可能な重みパラメータであり、両方向（候補ヘッドから依存語、依存語から候補ヘッド）の情報を組み合わせています。
    

### 5.2 Attention-and-Words Probe

- **拡張:**
    
    上記のAttention-Only Probeに加え、各単語の意味情報として事前学習済みのGloVe埋め込み $v$ を組み込みます。単語 $i$ と $j$ の埋め込みを連結（$v_i \oplus v_j$）し、その情報に基づいて各ヘッドの重み付けを調整します。
    
    $$
    p(i|j) \propto \exp\left(\sum_{k=1}^{n} \left[ W_{k}\cdot (v_i \oplus v_j)\, \alpha_{ij}^{(k)} + U_{k}\cdot (v_i \oplus v_j)\, \alpha_{ji}^{(k)} \right]\right)
    $$
    
    $W_{k}$ と $U_{k}$ は各ヘッドごとの重み行列です。このモデルにより、単語間の意味的類似性とアテンションパターンの両面から依存関係を予測することが可能になります。
    
- **評価:**
    
    Penn Treebank上で評価した結果、Attention-and-Words ProbeはUAS（Unlabeled Attachment Score）で77に達し、BERTのアテンションマップが十分な文法情報を保持していることが示されました。
    

---

## 第6章: Clustering Attention Heads

この章では、各アテンションヘッド間の類似性を定量的に評価し、クラスタリングや可視化を行っています。

### 6.1 ヘッド間の距離計算: JS Divergence

- **定義:**
    
    2つのヘッド $H_i$ と $H_j$ の間の距離は、各トークンに対する注意分布の違いをJensen-Shannon Divergence（JS Divergence）を用いて評価し、全トークンについて合計したものと定義されます。
    
    $$
    D_{JS}(H_i, H_j) = \sum_{\text{token} \in \text{data}} JS\left(H_i(\text{token}) \parallel H_j(\text{token})\right)
    $$
    
    ここで、JS Divergenceは2つの確率分布 $P$ と $Q$ に対して、
    
    $$
    JS(P \parallel Q) = \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M)
    $$
    
    （$M = \frac{1}{2}(P + Q)$）で計算され、対称性や有界性などの利点があります。
    
- **解釈:**
    - ヘッド間の距離が小さい＝類似した注意分布を持つ（同じような機能を果たしている）。
    - 距離が大きい＝異なる注意パターン、すなわち異なる役割や情報に基づく動作をしていると解釈されます。

### 6.2 多次元尺度構成法（MDS）による可視化

- **手法:**
上記で得られた全ヘッド間のJS距離行列を基に、多次元尺度構成法（MDS）を適用して2次元空間にヘッドを埋め込みます。
- **結果:**
可視化の結果、同一層内のヘッドは互いに近い位置にプロットされ、似た挙動（例：特定の特殊トークンへの集中、相対位置情報の重視など）のクラスタが形成されることが示されました。
これは、アテンションドロップアウトなどの正則化の影響や、学習の副産物としての冗長性が示唆されます。

---

## 第7章: Related Work

この章では、本研究の位置づけや他の関連研究との比較が行われています。

- **言語モデル解析:**
これまで、言語モデルの出力や内部のベクトル表現（例：プロービングタスク、部分品詞タグ付け、依存解析など）に焦点を当てた研究が多数存在します。
- **アテンション解析:**
アテンション可視化ツールを用いた定性的解析（Vig 2019など）や、特定のタスクへの影響に関する解析が挙げられます。
- **本研究の新規性:**
本論文は、アテンションマップ自体に含まれる言語情報に直接焦点を当て、個々のヘッドの振る舞いやそれらの組み合わせが文法情報や照応情報をどの程度捉えているかを、定量的な評価およびクラスタリングによって示しています。

---

## 第8章: Conclusion

最後の章では、本研究の総括と今後の展望が示されています。

- **主要な発見:**
    - BERTのアテンションヘッドは、単なる内部計算の一部に留まらず、文法的依存関係や照応といった高度な言語情報を自発的に学習している。
    - 特定のヘッドは、特定の依存関係（例：決定詞、目的語、前置詞の目的語など）を非常に高い精度で予測しており、また全体の文法情報は複数のヘッドに分散して保持されている。
    - アテンションマップから得られる情報は、従来の内部ベクトル表現の解析と補完的な情報を提供する。
- **意義:**
    
    この解析手法は、BERTをはじめとする大規模言語モデルがどのように言語の階層構造を獲得しているのか、そしてそれが下流タスクの高性能にどう寄与しているのかを理解する上で非常に有用です。
    
    また、今後のモデル改良や解釈可能性の向上のための基礎研究としても大きな示唆を与えます。
    

---

以上が、論文の各章ごとに詳細な内容と背景、数式を交えた解説となります。各章で扱われる実験手法や評価指標、クラスタリング手法などは、BERTが自己教師あり学習を通じて内部でどのように言語の構造を学んでいるのかを深く理解するための重要な知見を提供しています。