# Word2Vec：単語の効率的な分散表現推定

**著者**:
* Tomas Mikolov (Google Inc.)
* Kai Chen (Google Inc.)
* Greg Corrado (Google Inc.)
* Jeffrey Dean (Google Inc.)

**出版**: 
* "Efficient Estimation of Word Representations in Vector Space" (ICLR 2013)
* "Distributed Representations of Words and Phrases and their Compositionality" (NIPS 2013)

**URL**: 
* [Efficient Estimation (arXiv)](https://arxiv.org/abs/1301.3781)
* [Distributed Representations (NIPS)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

---

### 概要

Word2Vecは、大規模なテキストコーパスから単語の分散表現（distributed representation）を効率的に学習するための画期的な手法です。この手法は、単語を高次元の疎なone-hotベクトルではなく、意味的な情報を保持する低次元の密なベクトルとして表現します。Word2Vecには主に2つのモデルアーキテクチャがあります：**CBOW（Continuous Bag-of-Words）**と**Skip-gram**です。これらのモデルは、浅いニューラルネットワークを用いて、単語の周辺文脈から単語を予測する（CBOW）、または単語から周辺文脈を予測する（Skip-gram）ことで、単語埋め込みを学習します。

Word2Vecの特筆すべき点は、学習された単語ベクトルが意味的な類似性や類推関係を捉えることができる点です。例えば、`vector("King") - vector("Man") + vector("Woman") ≈ vector("Queen")` のような有名な関係性が成り立ちます。

### 新規性

Word2Vecの主な貢献は以下の点にあります：

* **計算効率の大幅な改善**: 従来のニューラル言語モデル（NNLM）と比較して、Word2Vecは計算量を劇的に削減しました。これにより、数十億単語規模の大規模コーパスでの学習が現実的になりました。
* **シンプルなアーキテクチャ**: 隠れ層を1つだけ持つ浅いニューラルネットワークを使用することで、モデルの複雑さを抑えながらも高品質な単語埋め込みを学習できることを示しました。
* **意味的・統語的関係の保存**: 学習された単語ベクトル空間において、ベクトル演算によって単語間の意味的・統語的関係を捉えることができることを実証しました。
* **2つの学習方式の提案**: CBOWとSkip-gramという2つの異なる学習パラダイムを提案し、それぞれの特性と適用場面を明らかにしました。

### 理論/手法の核心

#### 1. CBOW（Continuous Bag-of-Words）

CBOWモデルは、周辺の文脈単語から中心の単語を予測するモデルです。

* **入力**: ターゲット単語の前後 $C$ 個の文脈単語（ウィンドウサイズ $C$）
* **出力**: 中心のターゲット単語

**モデル構造**:
1. 入力層：文脈単語のone-hotベクトル
2. 投影層：各文脈単語を埋め込みベクトルに変換し、それらを平均化
3. 出力層：ソフトマックス関数を用いてターゲット単語の確率分布を計算

**目的関数**:
$$
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \log p(w_t \mid w_{t-C}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+C})
$$

ここで、$T$ はコーパス内の単語総数、$w_t$ は位置 $t$ の単語、$C$ は文脈ウィンドウサイズです。

#### 2. Skip-gram

Skip-gramモデルは、CBOWとは逆に、中心の単語から周辺の文脈単語を予測するモデルです。

* **入力**: 中心のターゲット単語
* **出力**: 周辺の文脈単語

**目的関数**:
$$
\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \sum_{-C \leq j \leq C, j \neq 0} \log p(w_{t+j} \mid w_t)
$$

Skip-gramは、特に小規模なデータセットや稀な単語に対してCBOWよりも良い性能を示すことが知られています。

#### 3. 効率化手法

Word2Vecの計算効率を実現するために、以下の手法が導入されています：

**階層的ソフトマックス（Hierarchical Softmax）**:
* 語彙全体に対するソフトマックスの代わりに、二分木構造（ハフマン木）を使用
* 計算量を $O(V)$ から $O(\log V)$ に削減（$V$ は語彙サイズ）

**ネガティブサンプリング（Negative Sampling）**:
* 全語彙に対する確率計算の代わりに、少数の「ネガティブ」サンプル（ランダムに選ばれた単語）のみを使用
* 目的関数を以下のように簡略化：

$$
\log \sigma(v'_{w_O}{}^T v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-v'_{w_i}{}^T v_{w_I})]
$$

ここで、$\sigma(x) = 1/(1 + e^{-x})$ はシグモイド関数、$k$ はネガティブサンプル数（通常5〜20）、$P_n(w)$ はノイズ分布です。

**サブサンプリング（Subsampling）**:
* 頻出語（"the", "a" など）を確率的にスキップすることで、学習の高速化と品質向上を実現
* 各単語 $w_i$ を確率 $P(w_i) = 1 - \sqrt{t/f(w_i)}$ で破棄（$f(w_i)$ は単語の頻度、$t$ は閾値）

#### 4. フレーズの学習

単語だけでなく、"New York" や "machine learning" のようなフレーズも単一のトークンとして扱うことで、より豊かな表現を学習できます。フレーズは、共起頻度に基づくスコアリング関数を用いて自動的に検出されます：

$$
\text{score}(w_i, w_j) = \frac{\text{count}(w_i w_j) - \delta}{\text{count}(w_i) \times \text{count}(w_j)}
$$

### 「キモ」と重要性

Word2Vecの「キモ」は、**分布仮説（distributional hypothesis）**、すなわち「似た文脈に現れる単語は似た意味を持つ」という言語学的原理を、効率的なニューラルネットワークアーキテクチャで実現した点にあります。

**重要性**:

1. **NLP研究のパラダイムシフト**: Word2Vecは、それまで主流だった疎な表現（one-hot、TF-IDF など）から、密な分散表現への移行を加速させました。これにより、意味的な類似性を自然に計算できるようになりました。

2. **ベクトル演算による意味操作**: 単語ベクトルに対する線形演算が、意味的な関係性を捉えることができるという発見は、言語の代数的な性質を示唆しており、極めて興味深い結果です。

3. **転移学習の先駆け**: 大規模コーパスで事前学習された単語埋め込みを、様々な下流タスク（分類、系列ラベリング、機械翻訳など）に転用する「事前学習→ファインチューニング」の流れを確立しました。

4. **計算効率とスケーラビリティ**: 数十億単語規模のコーパスで学習可能な効率性により、実用的なアプリケーションでの利用が広まりました。

5. **後続研究への影響**: Word2VecはGloVe、FastText、そして最終的にはBERTやGPTのようなTransformerベースモデルへと続く、埋め込み学習研究の基礎を築きました。

Word2Vecは、そのシンプルさと効率性、そして驚くべき性能により、自然言語処理における最も影響力のある手法の1つとなり、現代のNLP技術の基盤を形成しました。

