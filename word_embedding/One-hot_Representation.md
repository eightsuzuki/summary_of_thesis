# One-hot表現：疎なベクトルによる単語の符号化

**概要**:
One-hot表現は、単語を表現する最も基本的で古典的な手法です。各単語を、語彙サイズと同じ次元を持つ疎な（sparse）ベクトルとして表現します。このベクトルでは、対応する単語の位置のみが1で、他のすべての位置は0になります。

**定義**:
語彙 $V = \{w_1, w_2, \ldots, w_{|V|}\}$ が与えられたとき、単語 $w_i$ のone-hot表現 $\mathbf{e}_i$ は以下のように定義されます：

$$
\mathbf{e}_i = [0, 0, \ldots, 0, 1, 0, \ldots, 0]^T
$$

ここで、$i$ 番目の位置のみが1で、他の位置はすべて0です。

**例**:
語彙が $V = \{\text{cat}, \text{dog}, \text{car}, \text{house}\}$ の場合：
- "cat" → $[1, 0, 0, 0]^T$
- "dog" → $[0, 1, 0, 0]^T$
- "car" → $[0, 0, 1, 0]^T$
- "house" → $[0, 0, 0, 1]^T$

**特徴**:
1. **次元の呪い**: 語彙サイズが大きくなると、ベクトルの次元も比例して大きくなります
2. **疎性**: 各ベクトルは1つの非ゼロ要素のみを持ちます
3. **直交性**: 異なる単語のベクトルは互いに直交します（内積が0）
4. **意味的類似性の欠如**: すべての単語ペアが等しく「異なる」と扱われます

**問題点**:
- **意味的類似性を捉えない**: "cat"と"dog"の類似度と"cat"と"car"の類似度が同じ（0）になってしまいます
- **次元の爆発**: 語彙が10万語なら10万次元のベクトルが必要
- **計算効率**: 疎なベクトルでも、大規模な語彙では計算コストが高くなります

**用途**:
- 機械学習モデルの入力層での初期表現
- カテゴリカル変数の符号化
- 現在では、より高次な埋め込み手法の前処理段階で使用

**現代的な位置づけ**:
One-hot表現は現在では直接的な単語表現としては使われませんが、多くの埋め込み手法（Word2Vec、GloVeなど）の入力として、または埋め込み層の初期化に使用されています。
