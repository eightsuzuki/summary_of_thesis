# 単語埋め込み（Word Embedding）の歴史

単語埋め込みは、自然言語処理において単語を密なベクトル表現に変換する技術であり、過去数十年にわたって大きな進化を遂げてきました。

## 前史：One-hot から分散表現へ（〜2012）

### 初期の表現方法
- **One-hot表現**：各単語を語彙サイズの疎なベクトルで表現。意味的類似性を捉えられない。
- **TF-IDF**：文書内での単語の重要度を反映するが、依然として疎な表現。
- **LSA（Latent Semantic Analysis, 1990）**：特異値分解により文書-単語行列を低次元化。初期の分散表現の試み。
- **HAL（Hyperspace Analogue to Language, 1996）**：単語の共起情報から分散表現を構築。

### ニューラル言語モデルの登場
- **Bengio et al. (2003)**：ニューラルネットワークを用いた言語モデル（NNLM）が提案され、単語を低次元ベクトルとして学習する枠組みが確立。しかし、計算コストが高く、大規模データへの適用は困難だった。

## 第1世代：効率的な静的埋め込み（2013〜2016）

### Word2Vec（2013）
- **Mikolov et al.** が提案した革命的手法
- **CBOW**（Continuous Bag-of-Words）と**Skip-gram**の2つのアーキテクチャ
- 階層的ソフトマックスとネガティブサンプリングによる高速化
- 数十億単語規模のコーパスで学習可能
- ベクトル演算による意味的類推（"king" - "man" + "woman" ≈ "queen"）の発見
- **影響**：NLP研究のパラダイムシフト。静的埋め込みの標準手法となる。

### GloVe（2014）
- **Pennington, Socher, Manning** によるスタンフォード大学の研究
- グローバルな共起統計を直接モデル化
- カウントベースと予測ベースの利点を統合
- 共起確率の比が意味を捉えるという理論的洞察
- **影響**：Word2Vecと並ぶ主要な埋め込み手法として広く採用。

### FastText（2016）
- **Bojanowski et al.** によるFacebook AI Researchの研究
- 単語をサブワード（文字n-gram）の集合として表現
- **未知語問題の解決**：学習データにない単語にも対応可能
- 形態論的に豊富な言語（トルコ語、フィンランド語など）で特に有効
- **影響**：157言語の事前学習モデルが公開され、多言語NLPを促進。BPE、SentencePieceなどのサブワードトークナイゼーションの先駆け。

## 第1.5世代：確率的・幾何学的埋め込み（2015〜2020）

### ガウス埋め込み / Word2GM（2015）
- **Vilnis, McCallum** による確率的表現の提案
- 単語をガウス分布 $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ として表現
- **不確実性のモデリング**：分散により意味の広がりや曖昧さを表現
- **非対称な類似度**：KLダイバージェンスにより階層的関係を捉える
- **影響**：確率的知識表現、Poincaré埋め込み、Box埋め込みなどへの発展。

### Poincaré Embeddings（2017）
- **Nickel, Kiela** による双曲空間での埋め込み
- 階層構造を効率的に表現（木構造など）
- ユークリッド空間よりも少ない次元で階層をモデル化
- **影響**：双曲幾何学のNLPへの応用を開拓。

### Word2Box（2020）
- **Dasgupta et al.** によるボックス（超矩形）表現
- 単語を空間内の領域 $[\mathbf{m}, \mathbf{M}]$ として表現
- **包含関係の明示的モデリング**：is-a 関係を幾何学的包含として表現
- ボックスの体積が概念の一般性を反映
- **影響**：知識グラフ埋め込み、論理的推論への応用。

## 第2世代：文脈依存埋め込み（2018〜）

### ELMo（2018）
- **Peters et al.** によるAllen AI Instituteの研究
- **文脈化された表現**：同じ単語でも文脈によって異なるベクトル
- 双方向LSTM言語モデルの内部状態を利用
- **多義性の解決**："bank"（銀行／土手）を文脈で区別
- 6つの異なるNLPタスクで大幅な性能向上
- **影響**：静的埋め込みから動的埋め込みへのパラダイムシフト。BERTへの道を開く。

### BERT（2018）
- **Devlin et al.** によるGoogleの研究
- Transformerベースの双方向事前学習
- Masked Language Modeling（MLM）とNext Sentence Prediction（NSP）
- **Fine-tuningパラダイム**：事前学習済みモデル全体をタスクに適応
- **影響**：現代NLPの基盤技術。BERT以降、GPT、RoBERTa、T5など多数の派生モデル。

### GPT（2018〜）
- **Radford et al.** によるOpenAIの研究
- 自己回帰型（左から右）の言語モデル
- GPT-2（2019）、GPT-3（2020）、GPT-4（2023）と進化
- **Few-shot / Zero-shot学習**：大規模モデルによるプロンプトベース学習
- **影響**：大規模言語モデル（LLM）時代の幕開け。

## 表現形式の進化

### 1. 点表現（Point Embeddings）
各単語を $\mathbb{R}^d$ 空間の単一点として表現
- Word2Vec, GloVe, FastText

### 2. 確率分布表現（Probabilistic Embeddings）
各単語を確率分布として表現し、不確実性をモデル化
- ガウス埋め込み（Word2GM）

### 3. 幾何学的領域表現（Geometric Region Embeddings）
各単語を空間内の領域として表現
- Word2Box（ボックス）
- Poincaré Embeddings（双曲空間の球）

### 4. 文脈依存表現（Contextualized Embeddings）
文脈に応じて動的に変化する表現
- ELMo, BERT, GPT

## 主要なマイルストーン

| 年 | 手法 | 主な貢献 | インパクト |
|---|------|----------|-----------|
| 2003 | NNLM | ニューラル言語モデルの基礎 | 分散表現学習の枠組み |
| 2013 | Word2Vec | 効率的な静的埋め込み | NLP研究のパラダイムシフト |
| 2014 | GloVe | グローバル統計の活用 | 理論的基盤の提供 |
| 2015 | Word2GM | 確率的表現 | 不確実性・階層性のモデリング |
| 2016 | FastText | サブワード情報 | 未知語問題の解決 |
| 2017 | Poincaré | 双曲空間埋め込み | 階層構造の効率的表現 |
| 2018 | ELMo | 文脈依存表現 | 多義性の解決 |
| 2018 | BERT | Transformer事前学習 | 現代NLPの基盤 |
| 2020 | Word2Box | ボックス表現 | 包含関係の明示的モデリング |
| 2020 | GPT-3 | 超大規模LLM | Few-shot学習の実現 |

## 現在のトレンド（2023〜）

### 大規模言語モデル（LLM）の時代
- **GPT-4, Claude, Gemini**：数千億〜数兆パラメータのモデル
- **In-context Learning**：プロンプトによる柔軟なタスク適応
- **Instruction Tuning**：指示に従う能力の強化（InstructGPT, ChatGPT）
- **RLHF（Reinforcement Learning from Human Feedback）**：人間のフィードバックによる最適化

### 効率化とアクセシビリティ
- **LoRA, Adapter**：効率的なファインチューニング手法
- **量子化・圧縮**：小型デバイスでの実行
- **オープンソースLLM**：LLaMA, Mistral, Phi など

### 新しい研究方向
- **マルチモーダル埋め込み**：テキスト・画像・音声の統合表現（CLIP, DALL-E）
- **検索拡張生成（RAG）**：外部知識との統合
- **解釈可能性**：埋め込み空間の理解（Sparse Autoencoders, Circuit Tracing）

## まとめ

単語埋め込みの歴史は、**表現の高度化**と**文脈の取り込み**という2つの軸で進化してきました：

1. **表現の高度化**：点 → 分布 → 領域 → 文脈依存
2. **スケールの拡大**：数百万単語 → 数十億単語 → 数兆トークン
3. **計算効率の向上**：NNLM → Word2Vec → Transformer
4. **適用範囲の拡大**：単語 → 文 → 文書 → マルチモーダル

現在は、大規模言語モデルが主流となっていますが、Word2VecやGloVeのような軽量な静的埋め込みも、リソース制約のある環境や解釈可能性が重要な場面では依然として有用です。また、確率的埋め込みや幾何学的埋め込みは、特定の構造（階層、論理など）を明示的にモデル化する必要がある場面で今後も重要な役割を果たすでしょう。

