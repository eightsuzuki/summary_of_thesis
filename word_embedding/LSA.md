# LSA：潜在意味解析による初期の分散表現

**著者**:
* Scott Deerwester (Bell Communications Research)
* Susan T. Dumais (Bell Communications Research)
* George W. Furnas (Bell Communications Research)
* Thomas K. Landauer (Bell Communications Research)
* Richard Harshman (University of Western Ontario)

**出版**: Journal of the American Society for Information Science (1990)

**タイトル**: "Indexing by Latent Semantic Analysis"

**URL**: [Indexing by Latent Semantic Analysis](https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9)

---

### 概要

LSA（Latent Semantic Analysis、潜在意味解析）は、文書-単語行列の特異値分解（SVD）を用いて、単語と文書を低次元の意味空間に埋め込む手法です。1990年に提案されたこの手法は、**分散表現（distributed representation）**の概念をNLPに導入した先駆的な研究として位置づけられます。

LSAの核心的なアイデアは、文書-単語の共起行列に潜む**潜在的意味構造**を発見することです。高次元の疎な表現から、低次元の密な表現へと変換することで、意味的に類似した単語や文書を近い位置に配置します。

### 新規性

LSAの主な貢献は以下の点です：

* **分散表現の導入**: 単語を密なベクトルとして表現する概念をNLPに導入
* **次元削減による意味抽出**: 高次元データから本質的な意味情報を抽出
* **共起統計の活用**: 文書-単語の共起パターンから意味的関係を学習
* **情報検索への応用**: 意味的類似性に基づく検索の実現

### 理論/手法の核心

#### 1. 文書-単語行列の構築

まず、文書集合 $D = \{d_1, d_2, \ldots, d_m\}$ と語彙 $V = \{w_1, w_2, \ldots, w_n\}$ から、$m \times n$ の文書-単語行列 $A$ を構築します：

$$
A_{ij} = \text{weight}(w_j, d_i)
$$

ここで、$\text{weight}(w_j, d_i)$ は文書 $d_i$ 内での単語 $w_j$ の重みです。通常、TF-IDF値が使用されます。

#### 2. 特異値分解（SVD）

文書-単語行列 $A$ を特異値分解します：

$$
A = U \Sigma V^T
$$

ここで：
* $U \in \mathbb{R}^{m \times m}$：左特異ベクトル行列（文書空間）
* $\Sigma \in \mathbb{R}^{m \times n}$：特異値の対角行列
* $V \in \mathbb{R}^{n \times n}$：右特異ベクトル行列（単語空間）

#### 3. 次元削減

意味空間の次元 $k$（通常 $k \ll \min(m,n)$）を選択し、上位 $k$ 個の特異値のみを保持します：

$$
A_k = U_k \Sigma_k V_k^T
$$

ここで：
* $U_k \in \mathbb{R}^{m \times k}$：上位 $k$ 個の左特異ベクトル
* $\Sigma_k \in \mathbb{R}^{k \times k}$：上位 $k$ 個の特異値
* $V_k \in \mathbb{R}^{n \times k}$：上位 $k$ 個の右特異ベクトル

#### 4. 埋め込みの抽出

**単語埋め込み**:
単語 $w_j$ の埋め込みは、$V_k$ の $j$ 行目から取得します：

$$
\mathbf{w}_j = V_k[j,:] \Sigma_k^{1/2}
$$

**文書埋め込み**:
文書 $d_i$ の埋め込みは、$U_k$ の $i$ 行目から取得します：

$$
\mathbf{d}_i = U_k[i,:] \Sigma_k^{1/2}
$$

#### 5. 類似度の計算

**単語間類似度**:
2つの単語 $w_i$ と $w_j$ の類似度は、埋め込みベクトルのコサイン類似度で計算します：

$$
\text{sim}(w_i, w_j) = \frac{\mathbf{w}_i \cdot \mathbf{w}_j}{||\mathbf{w}_i|| \cdot ||\mathbf{w}_j||}
$$

**文書間類似度**:
同様に、文書間の類似度も計算できます：

$$
\text{sim}(d_i, d_j) = \frac{\mathbf{d}_i \cdot \mathbf{d}_j}{||\mathbf{d}_i|| \cdot ||\mathbf{d}_j||}
```

#### 6. 数学的直感

LSAの動作原理を理解するために、以下の例を考えます：

**例**: 文書集合
- 文書1: "cat sat on mat"
- 文書2: "dog ran in park"  
- 文書3: "cat and dog are friends"

**共起パターン**:
- "cat" は文書1,3に出現
- "dog" は文書2,3に出現
- "cat" と "dog" は文書3で共起

**LSAの効果**:
- 高次元の疎な表現から、低次元の密な表現へ変換
- "cat" と "dog" の埋め込みが近くなる（動物という共通概念）
- 文書3の埋め込みが文書1,2の中間的な位置になる

### 「キモ」と重要性

LSAの「キモ」は、**高次元の疎な共起データから、低次元の密な意味表現を抽出する**という、統計的次元削減の応用にあります。

**重要性**:

1. **分散表現の先駆**:
   * LSAは、単語を密なベクトルとして表現する概念をNLPに導入しました
   * これにより、意味的類似性を計算可能になりました
   * Word2Vec、GloVeなどの現代的な埋め込み手法の基礎を築きました

2. **次元削減の威力**:
   * 高次元の疎なデータから本質的な情報を抽出
   * ノイズの除去と重要なパターンの保持
   * 計算効率の向上

3. **意味的類似性の実現**:
   * 従来のキーワードマッチングを超えた検索が可能
   * 同義語や関連語の自動発見
   * 文書の意味的分類

4. **情報検索への影響**:
   * 意味的検索の実現
   * クエリ拡張の自動化
   * 推薦システムへの応用

5. **理論的基盤**:
   * 線形代数（SVD）のNLPへの応用
   * 統計的学習理論の実践
   * 次元削減理論の発展

**限界と課題**:

1. **線形性の制約**:
   * SVDは線形変換のため、非線形な意味関係を捉えにくい
   * 文脈依存の意味変化に対応できない

2. **計算コスト**:
   * 大規模データでのSVD計算は高コスト
   * リアルタイム更新が困難

3. **解釈性**:
   * 特異ベクトルの意味解釈が困難
   * 埋め込み空間の幾何学的性質が不明確

4. **スパース性**:
   * 入力行列が疎な場合、SVDの効果が限定的

**後続研究への影響**:

1. **現代的な埋め込み手法**:
   * Word2Vec：LSAの非線形版
   * GloVe：LSAの統計的基盤を発展
   * FastText：LSAのサブワード拡張

2. **次元削減手法**:
   * PCA、ICA、t-SNE、UMAPなど
   * 非線形次元削減の発展

3. **情報検索**:
   * 意味的検索システム
   * 推薦システム
   * 質問応答システム

**実用的な価値**:

LSAは現在でも以下の場面で使用されています：
- **軽量なベースライン**：リソース制約のある環境
- **解釈可能性**：埋め込みの意味を理解したい場合
- **初期分析**：データの構造を把握する段階
- **教育目的**：分散表現の概念を理解するための教材

**現代的な位置づけ**:

LSAは、現代のNLPにおける「古典的」な手法として位置づけられますが、その影響は現在でも続いています：

- **理論的基盤**：分散表現の数学的基礎
- **実用的価値**：特定の場面での有効性
- **教育的価値**：概念理解のための教材
- **歴史的意義**：NLP研究の転換点

LSAは、単語埋め込み研究の出発点として、その後のWord2Vec、GloVe、ELMo、BERTなどの発展の礎を築いた、極めて重要な論文です。
