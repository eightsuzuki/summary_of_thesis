# FastText：サブワード情報を用いた単語ベクトルの強化

**著者**:
* Piotr Bojanowski (Facebook AI Research)
* Edouard Grave (Facebook AI Research)
* Armand Joulin (Facebook AI Research)
* Tomas Mikolov (Facebook AI Research)

**出版**: Transactions of the Association for Computational Linguistics (TACL 2017)

**URL**: [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)

**プロジェクトページ**: [https://fasttext.cc/](https://fasttext.cc/)

---

### 概要

FastTextは、Meta（旧Facebook）のAI Research labが開発した、単語埋め込みを学習するための手法です。FastTextの最大の特徴は、単語全体を単一の単位として扱うのではなく、**単語を文字n-gram（サブワード）の集合として表現する**点にあります。

従来のWord2VecやGloVeでは、各単語は独立したベクトルとして学習されるため、学習データに含まれていない未知語（out-of-vocabulary, OOV）に対応できず、また形態論的に関連する単語（例："teach", "teacher", "teaching"）間の関係を明示的にモデル化できませんでした。FastTextはこれらの問題を、サブワード情報を活用することで解決しています。

### 新規性

FastTextの主な貢献は以下の点です：

* **サブワードモデルの導入**: 単語を文字n-gramの集合として表現することで、単語内部の構造（形態論的情報）を捉えることができます。
* **未知語への対応**: 学習時に見たことのない単語でも、その構成要素（文字n-gram）が既知であれば、それらの埋め込みを組み合わせることで表現を生成できます。
* **形態論的に豊富な言語への対応**: トルコ語、フィンランド語、ハンガリー語など、語形変化や複合語が豊富な言語において、特に効果を発揮します。
* **効率性の維持**: サブワード情報を追加しながらも、Word2Vecと同程度の学習速度を維持しています。

### 理論/手法の核心

#### 1. 基本的なアイデア

FastTextの核心は、各単語を**文字n-gramのバッグ（bag of character n-grams）**として表現することです。

**例**: 単語 "where" を $n = 3$ の文字tri-gramで表現すると：
* まず、単語の境界を示すために特殊文字を付加：`<where>`
* 次に、すべての文字tri-gramを抽出：
  * `<wh`, `whe`, `her`, `ere`, `re>`
  * 加えて、単語全体も含める：`<where>`

この単語は、これら6つのn-gramの集合として表現されます。

#### 2. スコアリング関数

Skip-gramモデルを基礎として、各単語 $w$ のスコアを、その構成n-gramのベクトルの和として定義します：

$$
s(w, c) = \sum_{g \in \mathcal{G}_w} \mathbf{z}_g^T \mathbf{v}_c
$$

ここで：
* $\mathcal{G}_w \subset \{1, \ldots, G\}$ は単語 $w$ に現れるn-gramの集合
* $G$ は辞書内のn-gramの総数
* $\mathbf{z}_g$ はn-gram $g$ のベクトル表現
* $\mathbf{v}_c$ は文脈単語 $c$ のベクトル

単語全体も特別なn-gramとして扱われ、$\mathcal{G}_w$ に含まれます。

#### 3. 目的関数

Skip-gramモデルと同様に、ネガティブサンプリングを用いた目的関数を最適化します：

$$
\sum_{t=1}^{T} \left[ \sum_{c \in \mathcal{C}_t} \ell(s(w_t, c)) + \sum_{n \in \mathcal{N}_{t,c}} \ell(-s(w_t, n)) \right]
$$

ここで：
* $T$ はコーパス内の単語総数
* $w_t$ は位置 $t$ の単語
* $\mathcal{C}_t$ は単語 $w_t$ の文脈単語の集合
* $\mathcal{N}_{t,c}$ はネガティブサンプルの集合
* $\ell(x) = \log(1 + e^{-x})$ はロジスティック損失関数

#### 4. 実装の詳細

**n-gramの範囲**:
* 通常、$n$ は 3 から 6 の範囲で選択されます
* すべての長さ（3-gram, 4-gram, 5-gram, 6-gram）のn-gramを同時に使用します
* これにより、短い形態素から長い形態素まで、様々なレベルの構造を捉えることができます

**n-gram辞書のサイズ削減**:
* 可能なすべての文字n-gramを保存すると、辞書が膨大になります
* そこで、ハッシュ関数を用いてn-gramを固定サイズ（例：2,000,000）のバケットにマッピングします
* これにより、メモリ効率を保ちながら、サブワード情報を活用できます

**単語ベクトルの計算**:
* 学習後、単語 $w$ のベクトル表現は、その構成n-gramのベクトルの和として計算されます：

$$
\mathbf{v}_w = \sum_{g \in \mathcal{G}_w} \mathbf{z}_g
$$

* 未知語に対しても、その文字n-gramが学習データに含まれていれば、ベクトルを生成できます

#### 5. 従来のSkip-gramとの関係

従来のSkip-gramモデルは、FastTextの特殊ケースと見なすことができます：
* $\mathcal{G}_w = \{w\}$（単語全体のみ）とすると、FastTextは通常のSkip-gramと等価になります

### 「キモ」と重要性

FastTextの「キモ」は、**単語を原子的な単位ではなく、構成要素（サブワード）の組み合わせとして扱う**という、柔軟で実用的なアプローチにあります。

**重要性**:

1. **未知語問題の解決**: これは実用上非常に重要です。実世界のアプリケーションでは、学習データに含まれない単語（固有名詞、新語、誤記など）が頻繁に登場します。FastTextはこれらに対してもゼロベクトルではなく、意味のある表現を提供できます。

2. **形態論的情報の活用**:
   * 派生語や屈折形を持つ単語（例："quickly" と "quick", "running" と "run"）は、共通のサブワードを共有するため、自動的に類似したベクトル表現を持ちます
   * 形態論的に豊富な言語（アグルチネーション言語など）において、語彙の爆発的増加に対処できます

3. **希少語への対応**: 学習データ中に数回しか出現しない単語でも、そのサブワードは他の単語と共有されるため、より良い表現を学習できます。

4. **多言語対応**: FastTextは157言語の事前学習済みモデルを公開しており、特に形態論的に複雑な言語や資源の少ない言語で有効です。

5. **計算効率**: サブワード情報を追加しているにもかかわらず、実装の工夫（ハッシュ化、効率的なn-gram計算など）により、Word2Vecと同程度の速度で学習できます。

6. **実用性の高さ**:
   * テキスト分類タスクにも対応しており、単なる埋め込み学習ツール以上の機能を提供
   * 使いやすいコマンドラインインターフェースとライブラリにより、広く普及しました

7. **BPE・SentencePieceへの影響**: FastTextのサブワード概念は、その後のByte Pair Encoding（BPE）やSentencePieceなど、現代のTransformerモデルで標準的に使用されるトークナイゼーション手法の先駆けとなりました。

8. **クロスリンガル埋め込み**: FastTextの多言語モデルは、言語間の単語アライメントやゼロショット転移学習の研究を促進しました。

**実例での有効性**:
* ドイツ語の "Büchern"（複数形・与格）と "Buch"（単数形・主格）は、サブワードの共有により自動的に関連付けられます
* 英語の "unhappiness" は、学習データになくても "un-", "happy", "-ness" などの構成要素から表現を生成できます

FastTextは、実用的な問題に対する実用的な解決策を提供することで、学術研究だけでなく産業界でも広く採用され、現代のNLPシステムにおける基盤技術の一つとなっています。

