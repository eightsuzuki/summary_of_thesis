# TF-IDF：文書内での単語の重要度を反映する重み付け

**著者**:
* Karen Spärck Jones (University of Cambridge)

**概要**:
TF-IDF（Term Frequency-Inverse Document Frequency）は、文書集合における単語の重要度を定量化する手法です。単語の頻度（TF）と文書頻度の逆数（IDF）を組み合わせることで、文書内で頻繁に出現し、かつ文書集合全体では稀な単語に高い重みを与えます。

**定義**:

**Term Frequency (TF)**:
文書 $d$ 内での単語 $t$ の出現頻度：
$$
\text{TF}(t,d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}
$$

ここで、$f_{t,d}$ は文書 $d$ 内での単語 $t$ の出現回数です。

**Inverse Document Frequency (IDF)**:
単語 $t$ の文書頻度の逆数：
$$
\text{IDF}(t,D) = \log \frac{|D|}{|\{d \in D : t \in d\}|}
$$

ここで、$|D|$ は総文書数、$|\{d \in D : t \in d\}|$ は単語 $t$ を含む文書数です。

**TF-IDF**:
$$
\text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D)
$$

**直感的な解釈**:
- **TF**: 文書内で頻繁に出現する単語は重要
- **IDF**: 多くの文書に出現する単語（"the", "a", "is"など）は重要度が低い
- **TF-IDF**: 文書内では頻繁だが、文書集合全体では稀な単語が最も重要

**例**:
文書集合：
- 文書1: "the cat sat on the mat"
- 文書2: "the dog ran in the park"
- 文書3: "a cat and dog are friends"

"cat"のTF-IDF計算：
- TF(文書1) = 1/6 ≈ 0.167
- TF(文書3) = 1/6 ≈ 0.167
- IDF = log(3/2) ≈ 0.405
- TF-IDF(文書1) = 0.167 × 0.405 ≈ 0.068
- TF-IDF(文書3) = 0.167 × 0.405 ≈ 0.068

**特徴**:
1. **疎な表現**: 多くの単語のTF-IDF値は0
2. **文書特異性**: 各文書に対して異なる重みベクトル
3. **スケーラビリティ**: 大規模文書集合でも効率的に計算可能
4. **解釈可能性**: 各重みの意味が明確

**変種**:
- **Binary TF**: TFを0/1のバイナリ値に
- **Log TF**: $\log(1 + \text{TF})$
- **Max TF**: $\frac{\text{TF}}{\max_{t' \in d} \text{TF}(t',d)}$

**IDFの変種**:
- **Smooth IDF**: $\log(1 + \frac{|D|}{|\{d \in D : t \in d\}|})$
- **Probabilistic IDF**: $\log(\frac{|D| - |\{d \in D : t \in d\}|}{|\{d \in D : t \in d\}|})$

**問題点**:
1. **意味的類似性の欠如**: "cat"と"dog"の類似度が低い
2. **文脈情報の欠如**: 単語の順序や文脈を考慮しない
3. **次元の呪い**: 語彙サイズが大きくなると次元も増大
4. **疎性**: 多くの要素が0で、計算効率が悪い場合がある

**用途**:
- **情報検索**: クエリと文書の類似度計算
- **テキスト分類**: 文書分類の特徴量
- **キーワード抽出**: 文書の重要な単語の特定
- **推薦システム**: ユーザーやアイテムのプロファイル作成

**現代的な位置づけ**:
TF-IDFは現在でも広く使用されていますが、主に以下の場面で：
- 軽量なベースライン手法
- 大規模データでの初期フィルタリング
- 解釈可能性が重要な場面
- リソース制約のある環境

**TF-IDFと埋め込みの関係**:
TF-IDFは埋め込み手法の前駆として重要です：
- **LSA**: TF-IDF行列の特異値分解
- **Word2Vec**: TF-IDFの重みを考慮した共起統計
- **GloVe**: TF-IDFのグローバル統計の活用

**実装の注意点**:
1. **語幹処理**: "running" → "run" のような正規化
2. **ストップワード**: "the", "a", "is" などの除去
3. **正規化**: ベクトルのL2正規化
4. **スパース行列**: メモリ効率のための疎行列実装

TF-IDFは、単語の重要度を定量化する古典的で実用的な手法として、現代のNLPでも重要な役割を果たしています。
