# GloVe：単語表現のためのグローバルベクトル

**著者**:
* Jeffrey Pennington (Stanford University)
* Richard Socher (Stanford University)
* Christopher D. Manning (Stanford University)

**出版**: Empirical Methods in Natural Language Processing (EMNLP 2014)

**URL**: [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162.pdf)

**プロジェクトページ**: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)

---

### 概要

GloVe（Global Vectors for Word Representation）は、単語の分散表現を学習するための教師なし学習アルゴリズムです。GloVeの特徴は、コーパス全体の**グローバルな単語共起統計**を直接利用して単語ベクトルを学習する点にあります。これは、局所的な文脈ウィンドウのみを見るWord2Vecのような予測ベースモデルとは異なるアプローチです。

GloVeは、単語-単語共起行列を作成し、その統計情報を最もよく説明するような単語ベクトルを学習します。このアプローチは、カウントベース手法（LSA、HALなど）の利点と、予測ベース手法（Word2Vec）の利点を組み合わせたものと言えます。実験結果では、GloVeは単語類推タスクや単語類似度タスクにおいて、Word2Vecを含む既存手法を上回る性能を示しています。

### 新規性

GloVeの主な貢献は以下の点です：

* **カウントベースと予測ベースの統合**: 従来、単語埋め込み手法はカウントベース（共起行列の分解）と予測ベース（ニューラルネットワーク）に分かれていましたが、GloVeはこれらの利点を統合する新しいアプローチを提案しました。
* **グローバル統計の明示的利用**: コーパス全体の共起統計を直接目的関数に組み込むことで、より包括的な言語情報を活用します。
* **共起確率の比に基づく学習**: 単なる共起頻度ではなく、共起確率の「比」が単語間の意味的関係を捉えるのに重要であることを示しました。
* **理論的な動機付け**: なぜこの目的関数が適切なのか、という理論的な説明を提供しています。

### 理論/手法の核心

#### 1. 動機：共起確率の比

GloVeの核心的なアイデアは、**共起確率の比（ratio of co-occurrence probabilities）**が単語の意味を捉えるのに有効である、という観察に基づいています。

具体例：
* "ice"（氷）と "steam"（蒸気）という2つの単語を考えます
* これらと他の単語の共起を見ると：
  * "solid"（固体）は "ice" とは頻繁に共起するが、"steam" とはあまり共起しない
  * "gas"（気体）は "steam" とは頻繁に共起するが、"ice" とはあまり共起しない
  * "water"（水）は両方と頻繁に共起する
  * "fashion"（ファッション）は両方ともあまり共起しない

この関係を定量化するために、共起確率の比を考えます：

$$
\frac{P(k \mid i)}{P(k \mid j)}
$$

ここで、$P(k \mid i) = P_{ik} = X_{ik} / X_i$ は単語 $i$ が与えられたときに単語 $k$ が文脈に現れる条件付き確率です。$X_{ik}$ は単語 $i$ と $k$ の共起回数、$X_i = \sum_k X_{ik}$ は単語 $i$ の文脈に現れる全単語の総数です。

上記の例では：
* $P(\text{solid} \mid \text{ice}) / P(\text{solid} \mid \text{steam})$ は大きい（> 1）
* $P(\text{gas} \mid \text{ice}) / P(\text{gas} \mid \text{steam})$ は小さい（< 1）
* $P(\text{water} \mid \text{ice}) / P(\text{water} \mid \text{steam})$ は約1
* $P(\text{fashion} \mid \text{ice}) / P(\text{fashion} \mid \text{steam})$ も約1

この比が、単語間の意味的関係を効果的に捉えることができるのです。

#### 2. モデルの構築

上記の観察から、単語ベクトル間の関係が共起確率の比を反映するようなモデルを構築します。

**ステップ1**: 単語ベクトル $w \in \mathbb{R}^d$ と文脈ベクトル $\tilde{w} \in \mathbb{R}^d$ の間の関係を以下の形式で表現します：

$$
F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
$$

ここで、$F$ はベクトルから実数へのある関数です。

**ステップ2**: ベクトル空間は線形構造を持つため、差分を使って表現します：

$$
F(w_i - w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
$$

**ステップ3**: 引数はベクトル、出力はスカラーなので、内積を使います：

$$
F((w_i - w_j)^T \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
$$

**ステップ4**: $F$ が群準同型（homomorphism）であるという条件から、$F = \exp$ が導かれます：

$$
w_i^T \tilde{w}_k - w_j^T \tilde{w}_k = \log \frac{P_{ik}}{P_{jk}} = \log P_{ik} - \log P_{jk}
$$

これより：

$$
w_i^T \tilde{w}_k = \log P_{ik} = \log X_{ik} - \log X_i
$$

**ステップ5**: $\log X_i$ を吸収するためにバイアス項を導入し、対称性を保つために単語ベクトルと文脈ベクトルの役割を対称化します：

$$
w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log X_{ik}
$$

#### 3. 最終的な目的関数

上記の関係式を最小二乗問題として定式化し、さらに重み関数を導入します：

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

ここで：
* $V$ は語彙サイズ
* $f(X_{ij})$ は重み関数で、以下の性質を持ちます：
  1. $f(0) = 0$（共起しない単語ペアは損失に寄与しない）
  2. $f(x)$ は非減少関数（共起が多いほど重要）
  3. $f(x)$ は大きな $x$ に対して過度に大きくならない（頻出語の過大評価を防ぐ）

**重み関数の具体形**:

$$
f(x) = \begin{cases}
(x/x_{\max})^\alpha & \text{if } x < x_{\max} \\
1 & \text{otherwise}
\end{cases}
$$

論文では $x_{\max} = 100$, $\alpha = 3/4$ が推奨されています。

#### 4. 学習アルゴリズム

1. コーパスから単語-単語共起行列 $X$ を構築（通常、対称的な文脈ウィンドウを使用）
2. 単語ベクトル $w$、文脈ベクトル $\tilde{w}$、バイアス項 $b$, $\tilde{b}$ をランダムに初期化
3. 確率的勾配降下法（SGD）またはAdaGradを用いて目的関数 $J$ を最小化
4. 最終的な単語表現として、$w + \tilde{w}$ を使用（単語ベクトルと文脈ベクトルの和）

**計算の効率化**:
* 共起行列はスパース（疎）なので、$X_{ij} \neq 0$ の要素のみを考慮
* これにより、語彙サイズの二乗に比例する計算を、実際の共起ペア数（通常ははるかに少ない）に比例する計算に削減

### 「キモ」と重要性

GloVeの「キモ」は、**共起確率の比という概念を通じて、グローバルな統計情報を直接的にベクトル空間の幾何学的構造にエンコードする**という点にあります。

**重要性**:

1. **理論的な基盤**: Word2Vecが経験的に成功していたのに対し、GloVeはなぜこのような目的関数が適切なのかという理論的な説明を提供しました。共起確率の比が意味を捉えるという言語学的な洞察が、モデル設計に直接反映されています。

2. **グローバル情報の活用**: 予測ベースモデルが局所的な文脈ウィンドウに依存するのに対し、GloVeはコーパス全体の統計を一度に利用します。これにより、より包括的な言語知識を捉えることができます。

3. **計算効率**: 共起行列の構築は一度だけで済み、その後の最適化は非ゼロ要素のみを対象とするため、大規模コーパスでも効率的に学習できます。また、並列化も容易です。

4. **優れた性能**: 単語類推タスク（"man:woman :: king:?" のような問題）や単語類似度タスクにおいて、当時の最先端手法を上回る性能を達成しました。

5. **解釈可能性**: 目的関数が共起統計を直接モデル化しているため、学習されたベクトルが何を表現しているかが比較的明確です。

6. **後続研究への影響**: GloVeのアイデアは、文書埋め込み、知識グラフ埋め込み、さらには画像-言語の共同埋め込みなど、様々な領域に応用されました。また、カウントベースと予測ベースの手法を統合するという視点は、その後の埋め込み研究に大きな影響を与えました。

GloVeは、単語埋め込みの本質的な性質についての深い洞察を提供し、効率性と性能の両面で優れた手法として、Word2Vecと並んで広く利用されるようになりました。現在でも、多くのNLPアプリケーションで事前学習済みGloVeベクトルが使用されています。

