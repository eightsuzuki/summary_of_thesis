# Word Embedding (単語埋め込み)

このディレクトリでは、単語を密なベクトル表現に変換する各種手法のまとめを収録しています。

## クイック比較表

| 手法 | 表現形式 | 学習方式 | 特徴 | 長所 | 短所 |
| --- | --- | --- | --- | --- | --- |
| One-hot | 疎ベクトル | - | 語彙サイズ次元 | シンプル；直交性 | 意味類似性なし；次元爆発 |
| TF-IDF | 疎ベクトル | 統計的 | 文書内重要度 | 解釈可能；軽量 | 意味類似性なし；疎性 |
| LSA | 密ベクトル | SVD | 潜在意味空間 | 次元削減；意味類似性 | 線形制約；計算コスト |
| HAL | 密ベクトル | 共起統計 | 文脈共起 | 位置関係考慮；実証済み | 次元爆発；疎性 |
| Word2Vec | 点ベクトル | 予測ベース（CBOW/Skip-gram） | 浅いニューラルネット | 高速；意味的類似性 | 多義性に弱い；文脈非依存 |
| GloVe | 点ベクトル | カウントベース（共起行列） | グローバル統計 | 語の類推タスクに強い | メモリ使用量大 |
| FastText | 点ベクトル | 予測ベース（Skip-gram拡張） | サブワード情報 | 未知語対応；形態論的情報 | ベクトルサイズ増大 |
| Word2GM | ガウス分布 | 確率的Skip-gram | 不確実性モデリング | 階層性・非対称性 | パラメータ2倍；最適化難 |
| Word2Box | ボックス（超矩形） | 共起+包含関係 | 領域としての表現 | 包含・階層の明示的表現 | 計算コスト；軸平行制約 |
| ELMo | 文脈化ベクトル | 文脈依存（双方向LSTM） | 多義性対応 | 動的な語義表現 | 計算コスト高 |

## 年代順の流れ（概要）

- **1990**: LSA（潜在意味解析）が登場し、SVDによる分散表現の基礎を確立
- **1996**: HAL（言語の超空間類似体）が提案され、共起統計による埋め込みを実現
- **2013**: Word2Vec（CBOW & Skip-gram）が登場し、効率的な分散表現学習が可能に
- **2014**: GloVe が提案され、グローバル統計情報を活用
- **2015**: ガウス埋め込み（Word2GM）が登場し、確率的表現・階層性の表現が可能に
- **2016**: FastText がサブワード情報を導入し、未知語や形態論的情報に対応
- **2018**: ELMo が文脈依存表現を実現し、多義性問題に対処
- **2018以降**: BERT, GPT などの Transformer ベースモデルが主流に
- **2020**: Word2Box などの幾何学的埋め込みが発展し、包含関係の明示的表現が可能に

## 表現形式による分類

### 疎ベクトル表現（古典的）
単語を高次元の疎なベクトルとして表現：
- **One-hot**：語彙サイズ次元の疎ベクトル（1つの位置のみ1、他は0）
- **TF-IDF**：文書内重要度に基づく疎ベクトル

### 密ベクトル表現（分散表現）
単語を低次元の密なベクトルとして表現：
- **LSA**：SVDによる潜在意味空間への投影
- **HAL**：共起統計による密ベクトル
- **Word2Vec**：CBOW/Skip-gram による予測ベース学習
- **GloVe**：共起行列の統計を直接モデル化
- **FastText**：サブワード（文字n-gram）情報を活用

### 確率分布表現
単語を確率分布として表現し、不確実性や階層性をモデル化：
- **Word2GM（ガウス埋め込み）**：単語をガウス分布 $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ として表現
  - 平均：単語の中心的意味
  - 分散：意味の広がり・不確実性
  - KLダイバージェンスによる非対称な類似度

### 幾何学的領域表現
単語を空間内の領域として表現し、包含関係を明示的にモデル化：
- **Word2Box**：単語をボックス（超矩形）$[\mathbf{m}, \mathbf{M}]$ として表現
  - ボックスの体積：概念の一般性
  - ボックスの包含：is-a 関係
  - ボックスの交差：意味的重なり

### 文脈依存表現
同じ単語でも文脈によって異なる表現を生成：
- **ELMo**：双方向LSTM言語モデルから文脈化表現を抽出
  - 多義性の解決
  - 統語・意味情報の階層的表現

## タスク別の推奨手法

- **基本的な類似度計算**: Word2Vec, GloVe（軽量で高速）
- **未知語・形態論的豊富な言語**: FastText
- **階層的関係・包含関係**: Word2GM, Word2Box
- **多義性・文脈依存タスク**: ELMo（または BERT/GPT など）
- **低リソース環境**: Word2Vec（最小のパラメータとメモリ）
- **解釈可能性重視**: TF-IDF, LSA（統計的基盤が明確）
- **ベースライン・教育目的**: One-hot, HAL（概念理解）

各手法の詳細については、このフォルダ内の個別マークダウンファイルを参照してください。

