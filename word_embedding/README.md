# Word Embedding (単語埋め込み)

このディレクトリでは、単語を密なベクトル表現に変換する各種手法のまとめを収録しています。

## クイック比較表

| 手法 | 表現形式 | 学習方式 | 特徴 | 長所 | 短所 |
| --- | --- | --- | --- | --- | --- |
| Word2Vec | 点ベクトル | 予測ベース（CBOW/Skip-gram） | 浅いニューラルネット | 高速；意味的類似性 | 多義性に弱い；文脈非依存 |
| GloVe | 点ベクトル | カウントベース（共起行列） | グローバル統計 | 語の類推タスクに強い | メモリ使用量大 |
| FastText | 点ベクトル | 予測ベース（Skip-gram拡張） | サブワード情報 | 未知語対応；形態論的情報 | ベクトルサイズ増大 |
| Word2GM | ガウス分布 | 確率的Skip-gram | 不確実性モデリング | 階層性・非対称性 | パラメータ2倍；最適化難 |
| Word2Box | ボックス（超矩形） | 共起+包含関係 | 領域としての表現 | 包含・階層の明示的表現 | 計算コスト；軸平行制約 |
| ELMo | 文脈化ベクトル | 文脈依存（双方向LSTM） | 多義性対応 | 動的な語義表現 | 計算コスト高 |

## 年代順の流れ（概要）

- **2013**: Word2Vec（CBOW & Skip-gram）が登場し、効率的な分散表現学習が可能に
- **2014**: GloVe が提案され、グローバル統計情報を活用
- **2015**: ガウス埋め込み（Word2GM）が登場し、確率的表現・階層性の表現が可能に
- **2016**: FastText がサブワード情報を導入し、未知語や形態論的情報に対応
- **2018**: ELMo が文脈依存表現を実現し、多義性問題に対処
- **2018以降**: BERT, GPT などの Transformer ベースモデルが主流に
- **2020**: Word2Box などの幾何学的埋め込みが発展し、包含関係の明示的表現が可能に

## 表現形式による分類

### 点ベクトル表現（従来型）
単語を $\mathbb{R}^d$ 空間の単一点として表現：
- **Word2Vec**：CBOW/Skip-gram による予測ベース学習
- **GloVe**：共起行列の統計を直接モデル化
- **FastText**：サブワード（文字n-gram）情報を活用

### 確率分布表現
単語を確率分布として表現し、不確実性や階層性をモデル化：
- **Word2GM（ガウス埋め込み）**：単語をガウス分布 $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ として表現
  - 平均：単語の中心的意味
  - 分散：意味の広がり・不確実性
  - KLダイバージェンスによる非対称な類似度

### 幾何学的領域表現
単語を空間内の領域として表現し、包含関係を明示的にモデル化：
- **Word2Box**：単語をボックス（超矩形）$[\mathbf{m}, \mathbf{M}]$ として表現
  - ボックスの体積：概念の一般性
  - ボックスの包含：is-a 関係
  - ボックスの交差：意味的重なり

### 文脈依存表現
同じ単語でも文脈によって異なる表現を生成：
- **ELMo**：双方向LSTM言語モデルから文脈化表現を抽出
  - 多義性の解決
  - 統語・意味情報の階層的表現

## タスク別の推奨手法

- **基本的な類似度計算**: Word2Vec, GloVe（軽量で高速）
- **未知語・形態論的豊富な言語**: FastText
- **階層的関係・包含関係**: Word2GM, Word2Box
- **多義性・文脈依存タスク**: ELMo（または BERT/GPT など）
- **低リソース環境**: Word2Vec（最小のパラメータとメモリ）

各手法の詳細については、このフォルダ内の個別マークダウンファイルを参照してください。

