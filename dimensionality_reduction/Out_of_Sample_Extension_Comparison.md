# 次元削減済みデータへの新要素追加：手法別の適性比較

## 概要

既に次元削減されたデータに新しい要素（データポイント）を追加する際、手法によって難易度が大きく異なります。この問題は「out-of-sample extension」や「incremental learning」と呼ばれます。

## 手法別の適性ランキング

### 🥇 **最強：PCA（Principal Component Analysis）**

**理由**:
- **線形変換**: 変換行列 $V_r$ を保存しておけば、新しいデータポイント $x_{\text{new}}$ に対して単純に適用可能
- **計算式**: $z_{\text{new}} = x_{\text{new}} V_r$（中心化済みデータに対して）
- **計算コスト**: $O(d \times r)$（$d$: 元の次元、$r$: 削減後の次元）
- **再計算不要**: 既存の埋め込みを変更する必要がない

**実装例**:
```python
# 訓練済みPCAモデルがある場合
z_new = pca.transform(x_new)  # 即座に変換可能
```

**利点**:
- ✅ 最も高速（線形変換のみ）
- ✅ 既存データへの影響なし
- ✅ メモリ効率が良い（変換行列のみ保存）
- ✅ 数値的に安定

---

### 🥈 **次点：Autoencoders（オートエンコーダー）**

**理由**:
- **エンコーダーの適用**: 学習済みエンコーダー $f_\theta$ を新しいデータポイントに適用するだけ
- **計算式**: $z_{\text{new}} = f_\theta(x_{\text{new}})$
- **計算コスト**: ニューラルネットワークの順伝播のみ（通常は高速）
- **再計算不要**: 既存の埋め込みを変更する必要がない

**実装例**:
```python
# 訓練済みオートエンコーダーがある場合
z_new = encoder(x_new)  # エンコーダーを適用
```

**利点**:
- ✅ 非線形な構造を捉えられる
- ✅ 既存データへの影響なし
- ✅ 実用的に高速（GPU使用時は特に高速）

**注意点**:
- ⚠️ エンコーダーの学習が必要
- ⚠️ 訓練データの分布から大きく外れると精度が低下

---

### 🥉 **第3位：Kernel PCA**

**理由**:
- **カーネル関数の適用**: 新しいデータポイントと訓練データ間のカーネルを計算
- **計算式**: $z_{\ell}(x_{\text{new}}) = \sum_{i=1}^n \alpha_{\ell i} \, \tilde{k}(x_i, x_{\text{new}})$
- **計算コスト**: $O(n \times d)$（$n$: 訓練データ数、$d$: 元の次元）
- **再計算不要**: 既存の埋め込みを変更する必要がない

**実装例**:
```python
# 訓練済みKernel PCAモデルがある場合
z_new = kernel_pca.transform(x_new)  # カーネル計算が必要
```

**利点**:
- ✅ 非線形な構造を捉えられる
- ✅ 既存データへの影響なし

**注意点**:
- ⚠️ 訓練データ数に比例する計算コスト
- ⚠️ カーネル関数の選択が重要
- ⚠️ 大規模データでは計算コストが高い

---

### ❌ **不適切：t-SNE**

**理由**:
- **全データの再計算が必要**: 新しいデータポイントを追加するには、全データ（既存+新規）に対してt-SNEを再実行する必要がある
- **計算コスト**: $O(n^2)$（全データポイント間の距離計算）
- **既存埋め込みの変更**: 既存の埋め込みが大きく変わる可能性がある

**問題点**:
- ❌ 既存データの埋め込みが変わる
- ❌ 計算コストが高い
- ❌ 再現性の問題（初期化による）

**回避策**:
- 近似手法（Barnes-Hut t-SNE）を使用
- 固定埋め込みに対して最近傍探索で近似

---

### ❌ **不適切：UMAP**

**理由**:
- **全データの再計算が必要**: 新しいデータポイントを追加するには、全データに対してUMAPを再実行する必要がある
- **計算コスト**: $O(n \log n)$（近傍グラフの構築）
- **既存埋め込みの変更**: 既存の埋め込みが変わる可能性がある

**問題点**:
- ❌ 既存データの埋め込みが変わる
- ❌ 計算コストが高い
- ❌ パラメータ（n_neighbors, min_dist）の再調整が必要な場合がある

**回避策**:
- 固定埋め込みに対して最近傍探索で近似
- バッチ処理で定期的に再計算

---

### ❌ **不適切：LLE（Locally Linear Embedding）**

**理由**:
- **近傍グラフの再構築**: 新しいデータポイントを追加するには、全データの近傍グラフを再構築する必要がある
- **計算コスト**: $O(n^2)$（近傍探索と重み計算）
- **既存埋め込みの変更**: 既存の埋め込みが変わる可能性がある

**問題点**:
- ❌ 既存データの埋め込みが変わる
- ❌ 近傍パラメータ $K$ の再調整が必要な場合がある

---

### ❌ **不適切：Isomap**

**理由**:
- **全データの再計算が必要**: 新しいデータポイントを追加するには、全データの測地距離を再計算する必要がある
- **計算コスト**: $O(n^2 \log n)$（最短経路計算）
- **既存埋め込みの変更**: 既存の埋め込みが変わる可能性がある

**問題点**:
- ❌ 既存データの埋め込みが変わる
- ❌ 計算コストが非常に高い

---

### ❌ **不適切：MDS（Multidimensional Scaling）**

**理由**:
- **全データの再計算が必要**: 新しいデータポイントを追加するには、全データの距離行列を再計算する必要がある
- **計算コスト**: $O(n^2)$（距離行列の計算と固有値分解）
- **既存埋め込みの変更**: 既存の埋め込みが変わる可能性がある

**問題点**:
- ❌ 既存データの埋め込みが変わる
- ❌ 計算コストが高い

---

## まとめ表

| 手法 | 新要素追加の難易度 | 計算コスト | 既存埋め込みへの影響 | 推奨度 |
|------|-------------------|-----------|---------------------|--------|
| **PCA** | ⭐ 非常に簡単 | $O(d \times r)$ | なし | ⭐⭐⭐⭐⭐ |
| **Autoencoders** | ⭐⭐ 簡単 | 順伝播のみ | なし | ⭐⭐⭐⭐⭐ |
| **Kernel PCA** | ⭐⭐⭐ やや困難 | $O(n \times d)$ | なし | ⭐⭐⭐ |
| **t-SNE** | ❌ 困難 | $O(n^2)$ | あり | ⭐ |
| **UMAP** | ❌ 困難 | $O(n \log n)$ | あり | ⭐ |
| **LLE** | ❌ 困難 | $O(n^2)$ | あり | ⭐ |
| **Isomap** | ❌ 非常に困難 | $O(n^2 \log n)$ | あり | ⭐ |
| **MDS** | ❌ 困難 | $O(n^2)$ | あり | ⭐ |

## 実用的な推奨事項

### 1. **新要素追加が頻繁な場合**
→ **PCA** を選択
- 最も高速で安定
- 線形構造を捉えるのに適している

### 2. **非線形構造を捉えつつ新要素追加が必要な場合**
→ **Autoencoders** を選択
- 非線形な構造を捉えられる
- エンコーダーの適用のみで高速

### 3. **可視化目的で新要素追加が必要な場合**
→ **PCA** または **Kernel PCA** を選択
- t-SNEやUMAPは再計算が必要だが、固定埋め込みに対して最近傍探索で近似可能

### 4. **バッチ処理が可能な場合**
→ 任意の手法を使用可能
- 定期的に全データを再計算する場合、どの手法でも使用可能

## 結論

**新要素追加に最も適しているのは PCA** です。理由：
1. **線形変換のみ**: 変換行列を保存しておけば即座に適用可能
2. **計算コストが低い**: $O(d \times r)$ の計算のみ
3. **既存データへの影響なし**: 既存の埋め込みが変わらない
4. **数値的に安定**: 線形代数の標準的な操作

非線形構造を捉える必要がある場合は、**Autoencoders** が次点として適しています。エンコーダーの適用のみで新要素を埋め込むことができ、既存データへの影響もありません。




