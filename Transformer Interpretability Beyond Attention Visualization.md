# Transformer Interpretability Beyond Attention Visualization

### 論文の概要

この研究は、Transformerネットワークの判断根拠を可視化するための新しい手法を提案するものです [cite: 3]。既存の手法の多くは、アテンションマップに依存していましたが [cite: 2]、それだけではモデルの挙動を完全に説明できないという問題がありました。提案手法は、**Deep Taylor Decomposition**という原理に基づいて各部分の関連性を計算し、それをネットワークの層を通して伝播させることで、より正確な可視化を実現します [cite: 4]。

この論文の「きも」は、Transformerの予測根拠を説明するために、以下の2つの情報を組み合わせる新しい手法を提案したことです。

1.  **関連性スコア (R):** LRP（関連性伝播法）を使い、ネットワーク全体の計算（アテンションだけでなく、MLP層なども含む）を通して、各部分が最終予測にどれだけ「貢献」したかを示すスコア。 [cite: 4]この論文の**「きも」**（最も重要な点）は、Transformerの予測根拠を説明するために、以下の2つの情報を組み合わせる新しい手法を提案したことです。

1.  **関連性スコア (R):** LRP（関連性伝播法）を使い、ネットワーク全体の計算（アテンションだけでなく、MLP層なども含む）を通して、各部分が最終予測にどれだけ「貢献」したかを示すスコア。 [cite: 4]
2.  **アテンションの勾配 (∇A):** 特定のクラスの予測スコアが、各アテンションの重みに対してどれだけ「敏感」かを示す勾配。これによりクラスごとの判断根拠の違いを捉える。

そして、この論文が**「言いたいこと」**（主張）を簡単にまとめると、以下のようになります。

* Transformerの判断を理解するには、**単にアテンションマップを見るだけでは不十分で、誤解を招くことすらある**。 [cite: 2, 16, 79, 229]
* 提案手法のように、ネットワーク全体の貢献度 (R) とクラス固有の感度 (∇A) を組み合わせることで、**「なぜモデルがそのように予測したのか」を、従来の手法よりもはるかに正確かつ具体的に（クラスを区別して）示すことができる**。 [cite: 3, 6, 36, 7]
* これにより、Transformerの解釈性が向上し、モデルの信頼性検証やデバッグに貢献できる。 [cite: 12]
2.  **アテンションの勾配 (∇A):** 特定のクラスの予測スコアが、各アテンションの重みに対してどれだけ「敏感」かを示す勾配。これによりクラスごとの判断根拠の違いを捉える。

そして、この論文が**「言いたいこと」**（主張）を簡単にまとめると、以下のようになります。

* Transformerの判断を理解するには、**単にアテンションマップを見るだけでは不十分で、誤解を招くことすらある**。 [cite: 2, 16, 79, 229]
* 提案手法のように、ネットワーク全体の貢献度 (R) とクラス固有の感度 (∇A) を組み合わせることで、**「なぜモデルがそのように予測したのか」を、従来の手法よりもはるかに正確かつ具体的に（クラスを区別して）示すことができる**。 [cite: 3, 6, 36, 7]
* これにより、Transformerの解釈性が向上し、モデルの信頼性検証やデバッグに貢献できる。 [cite: 12]
---

### 従来手法の問題点

従来のTransformerの解釈・可視化手法には、いくつかの課題がありました。

* **アテンションへの過度な依存:** 多くの手法がアテンションスコアをそのまま関連性スコアとして利用していましたが、アテンションはTransformerの構成要素の一部に過ぎません [cite: 16, 79]。
* **クラス非依存:** ある画像に複数のクラス（例：犬と猫）が含まれていても、どのクラスに着目しても同じような可視化結果になってしまう手法が多くありました [cite: 33, 53]。
* **Transformer特有の構造への未対応:** Transformerは、スキップコネクションやGELUのような非ReLU活性化関数を多用します [cite: 24, 25]。これらの構造は、既存の関連性伝播手法（LRPなど）において、数値的不安定性や関連性の保存則が成り立たないといった問題を引き起こしていました [cite: 26, 27, 28]。

---

### 提案手法

この論文では、これらの課題を解決するために、LRP (Layer-wise Relevance Propagation) を拡張した新しい手法を提案しています。主なアイデアは以下の通りです。

1.  **勾配と関連性の統合:** 各アテンションブロックにおいて、勾配情報とLRPによって計算された関連性スコアを統合します。
2.  **関連性保存則の維持:** スキップコネクションや行列積（アテンション計算で使われる）においても関連性の総和が保存されるように、正規化項を導入します。
3.  **クラス依存性の実現:** 特定のクラスに対する勾配を用いることで、クラスごとに異なる説明を生成できます [cite: 100]。

#### 数式による解説

提案手法の中核となる数式を解説します。

##### **1. 関連性伝播 (LRP)**

この手法の基礎となっているのが、LRP（Layer-wise Relevance Propagation）です。これは、ニューラルネットワークの出力層から入力層に向かって、予測に対する各ニューロンの「関連性（Relevance）」を分配していく手法です。

一般的なLRPの伝播ルールは、Deep Taylor Decompositionに基づいており、以下の式で表されます [cite: 104]。

$R_{j}^{(n)}=\sum_{i}X_{j}\frac{\partial L_{i}^{(n)}(X,Y)}{\partial X_{j}}\frac{R_{i}^{(n-1)}}{L_{i}^{(n)}(X,Y)}$  [cite: 104]

ここで、
* $R_{j}^{(n)}$ は、第$n$層のニューロン$j$の関連性スコアです。
* $R_{i}^{(n-1)}$ は、第$n-1$層のニューロン$i$の関連性スコアです。
* $L^{(n)}$ は、第$n$層の操作（計算）を表します。
* $X, Y$ は、層の入力や重みです。

この計算の重要な性質として、**関連性保存則** ($\sum_{j}R_{j}^{(n)}=\sum_{i}R_{i}^{(n-1)}$) が成り立ちます [cite: 106]。

##### **2. 非ReLU活性化関数への対応**

Transformerでよく使われるGELUなどの活性化関数は、出力が負の値を取り得ます [cite: 108]。従来のLRPはReLU（出力が非負）を前提としていたため、このままでは適用できません。そこで、論文では正の重み付けがされた関連性のみを考慮するように、伝播ルールを以下のように修正しました [cite: 109, 110]。

$R_{j}^{(n)}=\sum_{\{i|(i,j)\in q\}}\frac{x_{j}w_{ji}}{\sum_{\{j^{\prime}|(j^{\prime},i)\in q\}}x_{j^{\prime}}w_{j^{\prime}i}}R_{i}^{(n-1)}$  [cite: 109]

ここで、$q$ は入力$x_j$と重み$w_{ji}$の積が非負となるインデックスの集合です。

##### **3. スキップコネクションと行列積への対応**

Transformerのスキップコネクション（加算）やアテンションの行列積は、2つのテンソルを混合するため、関連性伝播において特別な扱いが必要です [cite: 112]。

* **加算（スキップコネクション）:** 関連性保存則は成り立ちますが、関連性スコアの絶対値が非常に大きくなり、数値的に不安定になる可能性があります [cite: 118, 124]。
* **行列積:** 関連性保存則が成り立ちません [cite: 119]。

この問題を解決するために、著者らは以下の**正規化**を導入しました。これにより、関連性保存則を維持し、かつ数値を安定させることができます [cite: 127, 129]。

$\overline{R}_{j}^{u^{(n)}}=R_{j}^{u(n)}\frac{|\sum_{j}R_{j}^{u(n)}|}{|\sum_{j}R_{j}^{u(n)}|+|\sum_{k}R_{k}^{n(n)}|}\cdot\frac{\sum_{i}R_{i}^{(n-1)}}{\sum_{j}R_{j}^{u(n)}}$

##### **4. 最終的な関連性マップの計算**

最終的に、各Transformerブロックで計算した関連性マップを統合します。

$\overline{A}^{(b)}=I+\mathbb{E}_{h}(\nabla A^{(b)}\odot R^{(n_{b})})^{+}$  [cite: 139]

$C=\overline{A}^{(1)}\cdot\overline{A}^{(2)}\cdot...\cdot\overline{A}^{(B)}$  [cite: 139]

ここで、
* $\overline{A}^{(b)}$ は、ブロック$b$の重み付けされたアテンション関連性です。
* $\nabla A^{(b)}$ は、アテンションマップ$A^{(b)}$の勾配です。
* $R^{(n_{b})}$ は、$A^{(b)}$に対応する層の関連性スコアです。
* $\odot$ は、要素ごとの積（アダマール積）です。
* $I$ は単位行列で、スキップコネクションを考慮するため加算されます [cite: 141]。
* $C$ が最終的に得られる関連性マップです。

この計算により、勾配情報と関連性スコアが統合され、クラスごとの判断根拠をより正確に反映した可視化が可能になります。

---

### 実験と結果

提案手法は、画像分類(ViT)とテキスト分類(BERT)のタスクで評価されました [cite: 155, 156]。

* **評価ベンチマーク:**
    * **画像:** ImageNetデータセットを用いた摂動テスト（Positive/Negative Perturbation）や、セグメンテーションタスク [cite: 176, 188]。
    * **自然言語処理:** ERASERベンチマークの映画レビューデータセットを用いた根拠抽出タスク [cite: 177, 190]。
* **比較手法:** rollout、raw-attention、Grad-CAM、LRPなど、既存の主要な手法と比較されました [cite: 159]。
* **結果:** 提案手法は、**すべてのタスクと評価指標において、既存の手法を大幅に上回る性能**を示しました [cite: 200, 203, 204]。特に、他の手法では困難だったクラス固有の可視化に成功し、より鮮明で一貫性のある説明を生成できることが実証されています [cite: 196, 198]。

---

### 結論

この論文は、Transformerの解釈可能性における重要な課題（アテンションへの依存、クラス非依存、特有のネットワーク構造）に正面から取り組み、勾配と関連性伝播を統合するという新しい解決策を提示しました [cite: 232, 233]。その結果、画像とテキストの両ドメインにおいて、既存手法を凌駕する最先端の解釈・可視化手法を確立したと言えます [cite: 234]。()