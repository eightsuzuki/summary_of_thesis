# TransE：多関係データモデリングのための埋め込み変換

**著者**:
* Antoine Bordes (Université de Technologie de Compiègne – CNRS Heudiasyc UMR 7253, Compiègne, France)
* Nicolas Usunier (Université de Technologie de Compiègne – CNRS Heudiasyc UMR 7253, Compiègne, France)
* Alberto Garcia-Duran (Université de Technologie de Compiègne – CNRS Heudiasyc UMR 7253, Compiègne, France)
* Jason Weston (Google, New York, NY, USA)
* Oksana Yakhnenko (Google, New York, NY, USA)

**出版**: Advances in Neural Information Processing Systems 26 (NIPS 2013)
**URL**: [Translating Embeddings for Modeling Multi-relational Data](https://proceedings.neurips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)

---

### 概要

この論文では、知識グラフなどの多関係データを低次元のベクトル空間に埋め込むための新しい手法「TransE（Translating Embeddings）」が提案されています。TransEは、関係をエンティティの埋め込みベクトルに対する「並進（translation）」として解釈するという、シンプルかつ直感的なアイデアに基づいています。具体的には、あるトリプル $(h, r, t)$（ヘッドエンティティ $h$、関係 $r$、テールエンティティ $t$）が成り立つ場合、ヘッドエンティティの埋め込みベクトル $h$ に関係の埋め込みベクトル $r$ を加えたものが、テールエンティティの埋め込みベクトル $t$ に近くなるべきであるという仮定を置きます。このシンプルさにもかかわらず、TransEはリンク予測タスクにおいて当時の最先端の手法を大幅に上回り、大規模なデータセットへの適用も可能であることを示しています。

### 新規性

TransEの最も大きな貢献は、知識グラフの関係を低次元ベクトル空間における**シンプルな並進操作**としてモデル化した点にあります。これまでの多関係データモデルの多くは、より複雑な行列やテンソル分解、あるいはより表現力の高いモデルを採用していましたが、それらは多くの場合、複雑な最適化問題や過学習の問題を抱えていました。

TransEは、関係をエンティティの埋め込みの間の翻訳として捉えることで、以下のような新規性をもたらしました。

* **直感的で解釈しやすいモデル**: $(h, r, t)$ が成り立つなら $h + r \approx t$ という仮定は非常に直感的で、特に「is-a」や「part-of」のような階層的な関係を自然に捉えることができます。
* **少ないパラメータ数**: 各エンティティと各関係に対して1つの低次元ベクトルのみを学習するため、パラメータ数が少なく、大規模な知識グラフにもスケーラブルに適用できます。
* **優れた性能とスケーラビリティ**: モデルの単純さにもかかわらず、リンク予測において既存の複雑なモデルを凌駕する性能を示しました。これは、単純なモデルが適切なモデリング仮定を置くことで、複雑なモデルよりも優れた精度とスケーラビリティのトレードオフを達成できることを示唆しています。特に、数百万のエンティティと数千万のトリプルを含む大規模なFreebaseデータセットでの学習に成功しています。

### 理論/手法の核心

TransEモデルは、エンティティと関係のベクトル埋め込みを学習します。これらの埋め込みは $k$ 次元空間 $\mathbb{R}^k$ の値を取ります。

1.  **基本的な考え方**:
    * モデルの基本的なアイデアは、関係 $r$ が誘導する機能的関係が、埋め込み空間における並進に対応するというものです。
    * トリプル $(h, r, t)$ が成り立つ場合、$h + r$ は $t$ に近いべきであり、そうでない場合は遠いべきであると仮定します。
    * ここで、$h, r, t$ はそれぞれヘッドエンティティ、関係、テールエンティティの埋め込みベクトルを表します。

2.  **スコアリング関数（エネルギー関数）**:
    * トリプル $(h, r, t)$ のエネルギー（不適合度）は、エンティティの埋め込み $h, t$ と関係の埋め込み $r$ の間の距離として定義されます。
    * これは、$d(h + r, t)$ と表され、ここで $d$ は何らかの非類似度測度（dissimilarity measure）です。
    * 本論文では、$d$ としてL1ノルムまたはL2ノルムが用いられます。
        * **L1ノルム**: $d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x} - \mathbf{y}||_1 = \sum_{i=1}^k |\mathbf{x}_i - \mathbf{y}_i|$
        * **L2ノルム**: $d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x} - \mathbf{y}||_2 = \sqrt{\sum_{i=1}^k (\mathbf{x}_i - \mathbf{y}_i)^2}$

3.  **損失関数（マージンベースランキング基準）**:
    * 埋め込みを学習するために、モデルは以下のマージンベースのランキング基準を最小化します。

    $$
    \mathcal{L} = \sum_{(h, r, t) \in S} \sum_{(h', r', t') \in S'_{(h,r,t)}} [\gamma + d(h + r, t) - d(h' + r', t')]_+
    $$

    * **各記号の定義**:
        * $S$: 正しいトリプルの訓練セット。
        * $S'_{(h,r,t)}$: 正しいトリプル $(h, r, t)$ に対応する破損した（負例の）トリプルのセット。
        * $[x]_+ = \max(0, x)$: 正の部分を取る関数（ヒンジ損失）。
        * $\gamma > 0$: マージンハイパーパラメータ。正例のスコアと負例のスコアの間に少なくともこのマージンだけ差があることを促します。
        * $h, r, t$: それぞれヘッドエンティティ、関係、テールエンティティの埋め込みベクトル。
        * $h', r', t'$: 破損したトリプルの要素。

    * **破損したトリプルの生成方法**: 破損したトリプル $S'_{(h,r,t)}$ は、正しいトリプル $(h, r, t)$ のヘッドエンティティ $h$ またはテールエンティティ $t$ のいずれかを、ランダムなエンティティで置き換えることによって生成されます（両方を同時に置き換えることはありません）。
        $$
        S'_{(h,r,t)} = \{(h', r, t) \mid h' \in \mathcal{E}\} \cup \{(h, r, t') \mid t' \in \mathcal{E}\}
        $$
        ここで $\mathcal{E}$ は全エンティティのセットです。

    * **制約**: エンティティの埋め込みベクトル $e \in \mathcal{E}$ には、$L_2$ ノルムが1であるという制約 $||e||_2 = 1$ が課せられます。これは、訓練プロセスがエンティティ埋め込みのノルムを人工的に増大させることによって損失を自明に最小化するのを防ぐために重要です。関係の埋め込み $r \in \mathcal{L}$ には、特にノルム制約は課せられません。

4.  **最適化**: 損失関数の最小化は、ミニバッチモードでの確率的勾配降下法（Stochastic Gradient Descent, SGD）によって行われます。

### 「キモ」と重要性

TransEの「キモ」は、**知識グラフ内の関係を、低次元の埋め込み空間におけるシンプルなベクトル並進（translation）として捉える**という、その極めて直感的で洗練されたアイデアにあります。これにより、複雑な多関係データを、幾何学的な変換として理解し、操作することが可能になりました。

このアイデアの重要性は以下の点に集約されます。

* **モデルの簡潔さと効率性**: これまでの埋め込みモデルが複雑な変換行列やテンソルを必要としていたのに対し、TransEは各関係を単一のベクトルとして表現することで、パラメータ数を劇的に削減しました。これにより、学習が容易になり、計算コストが低減し、非常に大規模な知識グラフにもスケーラブルに適用できるようになりました。
* **階層的関係の自然なモデリング**: 知識グラフには「is-a」や「part-of」のような階層的な関係が頻繁に登場します。並進の概念は、これらの階層をベクトル空間上で自然に表現するのに適しており、例えば親エンティティに特定の関係ベクトルを加えることで子エンティティの位置に到達するといった解釈が可能です。
* **性能の飛躍的向上**: そのシンプルさにもかかわらず、TransEはリンク予測タスクにおいて既存の多くの複雑なモデルを大幅に上回る性能を示しました。これは、必ずしも複雑なモデルがより良いとは限らず、データの本質的な特性に合わせた適切なモデリング仮定が、より良い結果をもたらすことを実証しました。
* **知識グラフ研究への影響**: TransEの成功は、その後の知識グラフ埋め込み研究の方向性に大きな影響を与えました。TransEを基盤とした多くの派生モデル（TransH, TransR, TransDなど）が提案され、関係の多様性や複雑性をより詳細にモデリングする研究が進みました。TransEは、知識グラフの補完、質問応答、推薦システムなど、様々な下流タスクにおける知識の活用を促進する基盤技術となりました。

総じて、TransEは、そのシンプルながらも強力な「並進」のアイデアによって、知識グラフ埋め込み研究の新たな扉を開き、この分野のその後の発展の礎を築いた、極めて重要な論文であると言えます。
