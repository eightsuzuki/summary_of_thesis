# DistMult: 知識ベースの学習と推論のためのエンティティと関係の埋め込み

**著者**:
* Bishan Yang (Microsoft Research, Redmond, WA, USA)
* Scott Wen-tau Yih (Microsoft Research, Redmond, WA, USA)
* Xiaodong He (Microsoft Research, Redmond, WA, USA)
* Jianfeng Gao (Microsoft Research, Redmond, WA, USA)
* Li Deng (Microsoft Research, Redmond, WA, USA)

**出版**: International Conference on Learning Representations (ICLR 2015)
**arXiv**: [arXiv:1412.6575](https://arxiv.org/pdf/1412.6575) (論文名："Embedding Entities and Relations for Learning and Inference in Knowledge Bases")

---

### 概要

この論文では、知識ベース（Knowledge Base, KB）のエンティティと関係を低次元のベクトル空間に埋め込むための、新しいシンプルかつ効果的なモデルである「DistMult（Diagonal Matrix Factorization）」が提案されています。DistMultは、知識グラフのトリプル $(h, r, t)$（ヘッドエンティティ $h$、関係 $r$、テールエンティティ $t$）の妥当性を、エンティティと関係の埋め込みベクトルの要素ごとの積和として評価します。特に、関係を対角行列で表現することで、モデルのパラメータ数を抑えつつ、効率的な学習と推論を可能にしています。TransEのような加算ベースのモデルとは異なり、DistMultは乗算ベースのモデルであり、既存のテンソル分解モデル（例：RESCAL）の特殊なケースと見なすことができます。

### 新規性

DistMultの主な新規性とその貢献は以下の点にあります。

1.  **シンプルで効率的な乗算ベースのモデル**:
    * TransEのような並進ベースのモデルが関係を「加算」として捉えるのに対し、DistMultは関係を「乗算」として捉えます。具体的には、関係ごとに**対角行列**を学習し、この行列を用いてエンティティ間の相互作用をモデル化します。
    * この対角行列の仮定により、モデルのパラメータ数がRESCALのような一般的なテンソル分解モデルに比べて大幅に削減されます。これにより、大規模な知識グラフに対しても効率的な学習と推論が可能になります。

2.  **既存モデルとの関係の明確化**:
    * DistMultは、テンソル分解モデルであるRESCALの特殊なケースとして位置づけられます。RESCALが関係をフル行列で表現するのに対し、DistMultは対角行列に限定することで、対称的な関係を効率的にモデル化できます。
    * また、DistMultは、そのシンプルさにもかかわらず、多くのリンク予測タスクにおいてTransEなどの先行モデルと同等かそれ以上の性能を達成することを示し、乗算ベースのモデルの有効性を再認識させました。

3.  **推論能力の向上**:
    * モデルの埋め込みが、元の知識ベースから直接学習された推論ルールや、論理的推論の枠組みとどのように統合できるかについても議論されており、埋め込み学習と論理的推論の架け橋となる可能性が示されています。

### 理論/手法の核心

DistMultモデルは、各エンティティ $e \in \mathcal{E}$ と各関係 $r \in \mathcal{L}$ を、同じ次元 $k$ の低次元ベクトル空間 $\mathbb{R}^k$ に埋め込みます。

1.  **スコアリング関数**:
    * トリプル $(h, r, t)$ の妥当性（スコア）は、ヘッドエンティティの埋め込み $\mathbf{h} \in \mathbb{R}^k$、関係の埋め込み $\mathbf{r} \in \mathbb{R}^k$、テールエンティティの埋め込み $\mathbf{t} \in \mathbb{R}^k$ の三線形積（tri-linear dot product）として定義されます。

    $$
    f(h, r, t) = \mathbf{h}^T \text{diag}(\mathbf{r}) \mathbf{t}
    $$
    ここで、$\text{diag}(\mathbf{r})$ は関係ベクトル $\mathbf{r}$ を対角成分とする $k \times k$ の対角行列です。
    この式は、より具体的には以下のように要素ごとの積の和として書くことができます。

    $$
    f(h, r, t) = \sum_{i=1}^k h_i \cdot r_i \cdot t_i
    $$
    * $h_i, r_i, t_i$ はそれぞれベクトル $\mathbf{h}, \mathbf{r}, \mathbf{t}$ の $i$ 番目の要素です。
    * スコアが高いほど、トリプルが正しい確率が高いとみなされます。

2.  **損失関数**:
    * 学習は、正例のトリプルのスコアを高く、負例のトリプルのスコアを低くすることを目的とした、ロジスティック損失（またはマージンベースのランキング損失）を用いて行われます。
    * 一般的には、正例を1、負例を0として二項分類問題として捉え、各トリプルに対して以下のようなロジスティック損失を最小化します。

    $$
    \mathcal{L} = - \sum_{(h,r,t) \in S \cup S'} \left[ y_{(h,r,t)} \log(\sigma(f(h,r,t))) + (1-y_{(h,r,t)}) \log(1 - \sigma(f(h,r,t))) \right]
    $$
    ここで、
    * $S$: 正しいトリプルのセット。
    * $S'$: 破損した（負例の）トリプルのセット（例: 既存のトリプルのヘッドまたはテールをランダムなエンティティで置き換える）。
    * $y_{(h,r,t)}$: トリプル $(h,r,t)$ が正例であれば1、負例であれば0となるラベル。
    * $\sigma(x) = 1 / (1 + \exp(-x))$: シグモイド関数。

3.  **制約**:
    * エンティティの埋め込みベクトル $\mathbf{h}, \mathbf{t}$ には、L2ノルムが1以下であるという制約 $||v||_2 \le 1$ が課せられることがあります。関係ベクトル $\mathbf{r}$ にも同様の制約が課せられることがあります。これは、学習プロセスが自明な解に収束するのを防ぎ、埋め込み空間のスケールを安定させるために重要です。

### 「キモ」と重要性

DistMultの「キモ」は、**関係を対角行列で表現し、エンティティ間の相互作用を要素ごとの積の和として評価する**という、極めてシンプルかつ効率的な乗算ベースのモデル化にあります。

このアイデアの重要性は以下の点に集約されます。

* **対称関係の効率的なモデリング**: 対角行列は本質的に対称性を持つため、"is_sibling_of" のような対称的な関係をモデル化するのに特に適しています。関係 $r$ が対称である場合、$\text{diag}(\mathbf{r})$ は $\text{diag}(\mathbf{r})^T$ と等しくなり、$\mathbf{h}^T \text{diag}(\mathbf{r}) \mathbf{t} = \mathbf{t}^T \text{diag}(\mathbf{r}) \mathbf{h}$ が成り立ちます。
* **パラメータ数の削減とスケーラビリティ**: RESCALのような一般的なテンソル分解モデルと比較して、関係ごとのパラメータ数を $k^2$ から $k$ に削減することで、モデルの複雑性を大幅に低減し、大規模な知識グラフにも効率的に適用できる高いスケーラビリティを実現しました。
* **シンプルさにもかかわらず高精度**: モデルの単純さにもかかわらず、リンク予測などのタスクにおいて、複雑なモデル（TransE、TransHなど）と同等かそれ以上の競争力のある性能を示しました。これは、必ずしも複雑なモデルが常に最良の選択肢ではないことを改めて示唆しています。
* **その後の研究への影響**: DistMultは、乗算ベースの埋め込みモデルの代表例となり、その後のComplEx（複素数空間への拡張により非対称関係もモデル化）やHolE（円形畳み込みを用いた表現）など、多くの派生モデルの基礎となりました。これにより、知識グラフ埋め込みモデルの設計空間が広がり、異なる種類の関係やデータの特性に合わせたモデル開発が進みました。

DistMultは、知識グラフ埋め込みの分野において、シンプルさ、効率性、そして優れた性能を両立させた重要なモデルであり、特に大規模な知識グラフの補完や推論において広く採用されています。
