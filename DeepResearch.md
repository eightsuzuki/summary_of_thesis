LLMs and Internal Syntactic Representations: A Literature Review
Large language models (LLMs) like BERT, RoBERTa, GPT, T5, and LLaMA have been widely analyzed to understand how they internally represent grammatical and syntactic information. Researchers have investigated whether these models learn concepts such as dependency trees, phrase structure, and other linguistic relations in their attention patterns, hidden layer representations, and other internal components (embeddings, MLP layers, etc.). A broad range of studies – from probing tasks and attention head analysis to structural probes and cross-lingual comparisons – consistently show that pre-trained Transformers encode substantial syntactic structure despite no explicit grammar supervision[1][2]. Below, we organize the key findings by category, highlighting representative research in each area (focusing on peer-reviewed conference and journal papers).
Analyzing Attention Heads and Syntactic Structure
One line of work compares Transformer attention maps to linguistic syntax. By examining the self-attention weights, researchers ask whether certain attention heads correspond to grammatical relations (e.g. head–dependent links in a parse tree). Overall, studies find that some attention heads do attend in ways that reflect syntax, although attention alone is not a full parse model. Notably, no single head encodes all syntax, but individual heads often specialize in particular relations, and combining information from all heads yields a decent approximation of parse structures.
•	Clark et al. (2019) – In one of the earliest BERT analysis papers, they showed that specific attention heads correspond remarkably well to certain dependency relations (for example, one head reliably attends from verbs to their direct objects, another links determiners to nouns)[1][3]. They even found heads that track coreference. While the best single head achieved only moderate parsing accuracy (~34% UAS), a simple attention-based parser using all heads reached 77% unlabeled attachment score (UAS) on dependency parsing[4], indicating BERT’s attention maps encode a “fairly thorough representation” of English syntax[4].
•	Mareček & Rosa (2018); Raganato & Tiedemann (2018) – These earlier studies (on translation models) also converted Transformer attention weights into dependency trees. They observed that some heads align with linguistic word alignments or dependencies, though overall parse quality from raw attention was limited. These works motivated later methods to probe attention-syntax correspondence more systematically.
•	Chinese BERT Analysis (2023) – A recent study on Chinese BERT (character-based) confirmed similar patterns in a non-English context[5]. They found that some individual heads capture specific dependency types in Chinese (including unique constructions like “把/被” sentences), but no single head handled all syntax. When combining all heads’ information, the model could “parse a sentence well,” suggesting that BERT’s attention heads collectively encode a large amount of syntactic knowledge[6]. This study also noted attention heads are less effective for long-distance relations (performance dropped as the gap between dependent and head grew)[7], echoing findings that attention tends to focus locally or on certain positions.
Probing Hidden Representations for Syntax
Another major approach uses probing tasks to evaluate the hidden states and embeddings of LLMs for encoded grammatical information. In probing, a simple classifier is trained to predict a linguistic property (POS tag, constituency label, dependency relation, etc.) from the model’s internal representations. High probe accuracy indicates the model’s embeddings make that information accessible. These studies have revealed which layers of the model capture syntax and how the model’s understanding of syntax compares to classical NLP pipelines.
•	Tenney et al. (2019) – “BERT Rediscovers the NLP Pipeline.” Using an edge probing suite of tasks, this work showed that BERT’s layers correspond to the traditional NLP pipeline[8]. The lower layers encode part-of-speech and basic syntactic chunking; middle layers best capture syntactic parsing (dependencies and constituents); higher layers focus on semantic roles and coreference[8]. Impressively, the internal representations were localizable: each linguistic task peaked at a particular layer, in the expected order (POS → parsing → NER → SRL → coreference), though some adjustments happen dynamically (later layers can revise earlier decisions)[8].
•	Jawahar et al. (2019) – Through a series of probes on BERT, they found a clear hierarchical organization of linguistic information across layers[9]. In their summary: “The intermediate layers of BERT compose a rich hierarchy: surface features at the bottom, syntactic features in the middle, and semantic features at the top.”[9] This means BERT’s middle layers were the most syntactically informative, outperforming lower or higher layers on tasks like constituent labeling. They also noted that BERT’s representations for phrases resemble classic tree structures, implying the model’s composition mechanism mimics tree-like syntax[10]. For example, BERT could implicitly group words into phrase-level representations in lower layers, and needed deeper layers to handle longer-range agreements (see next section).
•	Liu et al. (2019) – In a comprehensive study of ELMo, GPT, and BERT, Liu and colleagues reported that BERT’s middle layers give the highest probe scores for syntactic tasks, consistent with the above. This aligns with the observation that contextual embeddings encode syntactic info strongest at intermediate layers[11]. Such results across models indicate an emergent pattern: Transformers first capture local lexical patterns, then build up syntactic structure, and finally abstract semantic information.
•	Htut et al. (2019) – This probing study specifically asked “Do attention heads in BERT track syntactic dependencies?” (published as an extended abstract). They found some correspondence (certain heads specialized for specific dependencies) but also that many heads did not align with clear syntax, reinforcing the need for probe classifiers or combining heads to fully evaluate syntactic knowledge[12]. This nuanced finding mirrors Clark et al. (above), and it underscores that attention weights alone are not a perfect indicator of knowledge – sometimes the information is present in the model but not directly readable from one attention pattern[13][14].
Structural Probes and Reconstructing Parse Trees
Instead of simple classification probes, some researchers designed structural probes to test if entire parse trees are encoded in the geometry of LLM hidden states. A structural probe typically tries to find a transformation of the model’s embeddings such that distances or directions correspond to syntactic relations (e.g., tree distance or parent-child links). These probes directly evaluate whether hierarchical tree structure is embedded in the model’s representation space.
•	Hewitt & Manning (2019) – This seminal work introduced the Structural Probe[15][16]. They discovered a linear transformation of BERT’s hidden states under which squared L2 distance between word vectors corresponds to the tree distance in the sentence’s parse tree[17]. In other words, they found a subspace of BERT’s embedding space that explicitly mirrors the structure of dependency parse trees. Using this method, they showed BERT encodes entire unlabeled parse trees implicitly, with performance far above chance – achieving about 80% undirected UAS (unlabeled attachment score) in reconstructing English dependency trees[18]. This is striking because BERT was never trained on tree structures, yet the probe could recover tree distances with high correlation[19]. The authors conclude that “entire syntax trees are embedded implicitly in deep models’ vector geometry”[2], providing strong evidence that LLMs learn a rich hierarchical representation of syntax during self-supervised training.
•	Probing for Labeled Trees – Subsequent work extended structural probes to predict not just tree shape but also dependency labels. For example, one study added a simple model on top of BERT to predict dependency labels between pairs of words, finding that certain relational information (subjects, objects, etc.) can also be decoded. While pure unlabeled structure is most robustly encoded, these efforts showed some degree of labeled grammatical relations can be extracted from embeddings (though with lower accuracy than unlabeled structure).
•	Attention-Based Parsers vs. Probes – It’s worth noting the contrast between using attention weights for parsing and using learned probes. Clark et al. (2019) achieved 77% UAS with an attention-based method[4], slightly below Hewitt & Manning’s 80% UUAS (undirected UAS) with a learned probe[18]. This gap suggests that while raw attention maps carry a lot of syntactic signal, a probe that can optimally mix information from all layers (and directions) uncovers a bit more. In practice, both approaches showed that BERT’s unsupervised latent knowledge of syntax rivals that of a supervised parser, an astonishing finding in 2019.
Hierarchical Structure & Long-Distance Dependencies in LLMs
A core question is whether LLMs truly learn hierarchical syntactic generalizations (as opposed to shallow patterns). Linguistic hierarchy is reflected in phenomena like nested clauses, long-distance agreement, and recursive structures. Researchers have tested LLMs on such cases and analyzed internal activations for evidence of hierarchical processing.
•	Subject–Verb Agreement and Long Dependencies: A notable example is subject-verb number agreement across intervening phrases (e.g., “The dogs under the tree are barking” vs “The dog under the trees is barking”). Goldberg (2019) and others found that BERT can handle these agreements with high accuracy, suggesting it captures the hierarchical notion of grammatical subjects ignoring distracting nouns. Jawahar et al. (2019) observed that BERT needs its deeper layers to track long-range dependencies – the model successfully handles long-term subject–verb agreement, but only when the higher layers are included, indicating those layers carry the needed long-distance grammatical information[10]. This finding implies an internal mechanism that accumulates and preserves syntax over multiple layers to resolve dependencies that span clauses.
•	Hierarchical vs. Linear Generalization: Several targeted evaluations (Marvin & Linzen 2018; McCoy et al. 2019) tested whether models prefer hierarchical syntax rules or linear patterns. Transformers like GPT-2 and BERT tended to perform much better than RNNs on these tests, correctly favoring grammatical, hierarchical interpretations in most cases. For instance, models learned that agreement is determined by parse structure, not linear proximity – a sign of hierarchical understanding. There are still exceptions (certain center-embedded structures or garden-path sentences can trip up models), but overall modern LLMs demonstrate a strong grasp of syntactic hierarchy, likely learned from the statistics of language.
•	Phrase Structure Knowledge: Beyond dependencies, LLMs also seem to encode constituent structure (phrase boundaries and types). Probing tasks for constituency (e.g., is this word the start of a VP?) show above-chance recoverability from embeddings. BERT was even shown to implicitly distinguish syntactic parse trees versus linear n-gram structure – one probe by Wu et al. (2020) found BERT’s representation of sentences is more sensitive to syntactic reorderings than to random word order, meaning the model’s internal states change in a way consistent with parse-tree changes. Additionally, as noted earlier, BERT’s composition mechanism appears “tree-like”[10], and one can sometimes see neuron activations or attention heads that correspond to forming complete phrases before combining them at higher layers (mirroring a bottom-up parse).
•	Mechanistic Interpretability (Circuits): Very recent research in mechanistic interpretability has gone further by identifying specific circuits in models that handle hierarchical patterns. For example, Olsson et al. (2022) found in a 2-layer Transformer a pair of attention heads (dubbed an “induction head” circuit) that learns to repeat patterns from earlier in context – effectively implementing a kind of bracketing or matching that could be seen as a simple form of hierarchy handling[20]. Such work, though in its early stages, demonstrates at the circuit level how Transformers might implement recursive or hierarchical operations through combinations of attention and feed-forward computations.
In summary, across these analyses, LLMs show a robust ability to encode and utilize hierarchical grammatical structure, even for long-range relations, although the degree of understanding can vary by construction and model size.
Beyond Attention: MLP Layers and Other Components’ Contributions
While attention heads often steal the spotlight in interpretability, researchers have also examined other internal components (value vectors, feed-forward layers, neurons) for their role in encoding syntax. The Transformer architecture includes feed-forward networks (MLPs) in each layer, and these constitute a large portion of the model’s parameters. Do they also carry linguistic information? Recent studies suggest yes – important linguistic patterns are distributed across the model, not just in attention weights.
•	Geva et al. (2021) – Feed-Forward Layers as Key-Value Memories: This work showed that the position-wise feed-forward sublayers in Transformers act like key–value memory systems[21]. Each feed-forward layer “keys” on certain textual patterns and maps them to “values” that bias the next-word output distribution[22][23]. Crucially, lower-layer MLPs tend to capture shallow patterns (e.g. common n-grams or simple syntactic templates), while upper layers capture more abstract or semantic patterns[23][24]. This means information like a particular functional phrase or partial construction could be picked up by a feed-forward neuron in an early layer. For example, a neuron might fire for the pattern “either _ or _” indicating a coordination structure. The existence of such pattern-matching suggests that syntactic constructs are encoded in the network’s weights (especially MLP layers) in a distributed way, complementing what attention heads do. Geva et al. found these patterns are often human-interpretable and that the MLP “memory” contributes significantly to the model’s sequence prediction[25][26]. In sum, feed-forward layers store a lot of knowledge (including linguistic knowledge), not just attention.
•	Redundancy and Pruning Studies: Another hint that syntax isn’t only in attention comes from pruning experiments. He et al. (2023) analyzed the redundancy of Transformer modules and found that one can drop a large fraction of attention heads (or even entire attention layers) with minimal loss in performance[27]. For instance, they report that a LLaMA-2 model can lose up to 50% of its self-attention layers and still perform almost on par with the full model[28]. This implies that much of the essential information (likely including syntactic cues for language understanding) was preserved in other parts of the network – presumably in the residual connections and MLP layers. In contrast, pruning MLP layers had a more damaging effect on performance[29], suggesting that the feed-forward sublayers are carrying crucial information. This finding supports the notion that syntactic (and other linguistic) information is distributed across the model’s components, and not exclusively stored in attention weight patterns. It aligns with earlier observations that attention weights by themselves are not faithful explanations of model decisions[13]; other hidden elements (like the values and MLP neurons) encode the bulk of knowledge that the model uses.
•	Neuron-Level Probing: Some research has drilled down to individual neurons (dimensions in hidden state vectors) to see if any correspond to grammatical features. For example, Dalvi et al. (2019) identified neurons in an MT model whose activations correlated with specific linguistic properties (voice, tense, etc.). In BERT and GPT, similar neuron-level analyses have found units that respond to punctuation, to specific syntactic constructions, or even act as Boolean flags for being “in a quoted clause,” for instance. While syntax is generally too complex to be captured by single neurons, these studies demonstrate that high-level grammatical signals emerge in combinations of neurons, and a few neurons can sometimes be identified with a particular aspect of syntax (like whether a verb is in a passive construction).
In summary, looking beyond attention reveals that LLMs’ understanding of syntax is a whole-network phenomenon. Attention heads, MLP layers, and embedding subspaces collectively contribute, each capturing different facets – heads often align tokens in syntactic relations, MLPs store syntactic patterns and long-term dependencies, and the overall layer-wise stacking builds an increasingly abstract syntactic representation.
Cross-Lingual and Multilingual Syntactic Representations
A fascinating aspect of multilingual Transformers (like mBERT or XLM-R) is how they handle syntax across different languages. Do they learn a unified syntactic representation that generalizes across tongues, or separate per-language syntax? Research suggests that multilingual models do develop language-general syntactic abstractions, enabling surprising cross-lingual transfer in parsing and related tasks.
•	Chi et al. (2020) – Universal Dependencies in mBERT: Using the structural probe on Multilingual BERT (mBERT), Chi and colleagues found that the model contains a shared multilingual syntactic subspace[30]. When they trained a probe to extract parse trees from mBERT’s embeddings for English, that same probe (without retraining) could parse sentences in other languages with notable accuracy – meaning mBERT’s geometry for syntax is largely language-independent[31]. They visualized dependency pair embeddings from English and French and observed that relations cluster by type (e.g. subjects, objects, modifiers) rather than by language[32][33]. In other words, mBERT seems to encode a kind of “universal grammar”: grammatical relationships like “subject of a verb” or “adjective modifying noun” occupy similar directions in the representation space across many languages[33]. This explains why mBERT can zero-shot transfer a parser or POS tagger from one language to another. The study, published at ACL 2020, provides evidence that simply by training on raw text in multiple languages, the Transformer finds common structural patterns (subjects, adverbs, objects, etc.) that align with traditional linguistic categories across languages[34][35].
•	Pires et al. (2019) – Titled “How multilingual is Multilingual BERT?”, this work observed that mBERT can indeed project languages into a common space. They showed mBERT could accurately do POS tagging and simple dependency parsing in languages it was not explicitly trained for (zero-shot), especially for language pairs using the same script. This implies a shared syntax representation is at play. They noted, however, that script differences and low-resource languages posed challenges, suggesting the limits of mBERT’s universality.
•	Language-Specific BERTs (Japanese, Chinese, etc.): Studies on BERT models trained in languages like Chinese and Japanese (which have very different grammar from English) reveal both similarities and differences. As mentioned, the Chinese BERT analysis in 2023 found analogous syntactic encodings: attention heads encoding dependencies and hidden layers supporting POS, constituency, and dependency probes[5][6]. One interesting finding was that after fine-tuning Chinese BERT on a POS-tagging task, the model’s syntactic probing performance improved, whereas fine-tuning on an NLI (natural language inference) task degraded the syntactic knowledge in the representations[36]. This suggests that task training can shift what grammatical info is emphasized: tasks like POS or parsing reinforce low-level syntax in the model, while purely semantic tasks might cause the model to “forget” some syntax in favor of abstract features. For Japanese BERT, research is comparatively sparse, but given Japanese is head-final and has different dependency structures, it would be an interesting test case. Multilingual models (mBERT, XLM-R) have shown they can handle Japanese syntax too, likely by finding universal dependency patterns (e.g., subject/object roles) and adjusting to language-specific word order through positional cues.
•	Cross-Lingual Probing: Efforts like Morphosyntactic probing across 42 languages (Papadimitriou et al. 2021) have created multilingual probe datasets (tasks like plural detection, case marking, etc.). These have demonstrated that models like XLM-R learn a significant amount of morphosyntactic features for many languages and that there is often a correlation between languages – i.e., the model might use a similar neuron or subspace for encoding a grammatical concept in Spanish and in Italian, reflecting the relatedness of their syntax. However, for languages with very different grammar (say, Chinese vs English word order), the model still copes by isolating word-order-specific information while sharing deeper relational information.
In summary, multilingual LLMs appear to learn a form of language-neutral syntactic representation, enabling cross-language generalizations. At the same time, they adjust to each language’s surface form (e.g., word order or case marking) as needed. This mix of universal syntax and language-specific signals in one model is an exciting outcome of training on multilingual data, and ongoing research continues to probe how the model’s internals organize this knowledge.
Conclusion
Across many studies and evaluation methods, a clear picture emerges: Transformer-based LLMs learn a substantial amount of grammatical structure internally, even without explicit syntactic supervision. Attention head analyses show partial alignment with linguistic dependencies[3], probing studies demonstrate that syntactic features are encoded especially in middle layers[9], and structural probes reveal entire parse trees latent in the models’ vector geometry[2]. Models not only capture surface-level cues, but also abstract hierarchical relations and long-distance dependencies, approaching the capabilities of traditional parsers in some cases. Moreover, this syntactic knowledge is robust across languages and is distributed across various components of the network (not just attention mechanisms). All of these findings reinforce the notion that LLMs induce an implicit grammar of language: they rediscover many of the same principles that linguists formalized, but in the form of learned weight patterns and activation spaces. This growing body of literature (sometimes dubbed “BERTology”[37]) provides insight into why LLMs are so effective – part of the reason is that by pre-training on massive text, they infer the underlying syntactic structures that govern language, and leverage these structures for understanding and generation. Researchers continue to refine these analyses, extending them to newer models (e.g., GPT-3/4 or LLaMA), to other languages, and to more nuanced aspects of “grammar” (such as semantics, pragmatics, and beyond), bridging the gap between black-box neural nets and human-interpretable linguistic knowledge.
Sources:
•	Clark et al., 2019 – “What Does BERT Look At? An Analysis of BERT’s Attention.” (EMNLP 2019) [1][3]
•	Tenney et al., 2019 – “BERT Rediscovers the Classical NLP Pipeline.” (ACL 2019) [8]
•	Jawahar et al., 2019 – “What Does BERT Learn about the Structure of Language?” (ACL 2019) [9]
•	Hewitt & Manning, 2019 – “A Structural Probe for Finding Syntax in Word Representations.” (NAACL 2019) [17][18]
•	Chi et al., 2020 – “Finding Universal Grammatical Relations in Multilingual BERT.” (ACL 2020) [33][32]
•	Geva et al., 2021 – “Transformer Feed-Forward Layers Are Key-Value Memories.” (EMNLP 2021) [21][23]
•	He et al., 2024 – “What Matters in Transformers? Not All Attention is Needed.” (preprint 2024, UMD) [27]
•	Additional references: Raganato & Tiedemann 2018; Mareček & Rosa 2018; Htut et al. 2019[12]; Goldberg 2019; Liu et al. 2019; Papadimitriou et al. 2021; Wang et al. 2023 (Chinese BERT)[5][6]; Rogers et al. 2020 (BERTology survey)[38]; Vig & Belinkov 2019; Olsson et al. 2022 (mechanistic analysis)[20].
 
[1] [3] [4] [18] www-nlp.stanford.edu
https://www-nlp.stanford.edu/pubs/clark2019what.pdf
[2] [15] [16] [17] [19] A Structural Probe for Finding Syntax in Word Representations
https://aclanthology.org/N19-1419.pdf
[5] [6] [7] [36]  What does Chinese BERT learn about syntactic knowledge? - PMC 
https://pmc.ncbi.nlm.nih.gov/articles/PMC10403162/
[8] BERT Rediscovers the Classical NLP Pipeline - ACL Anthology
https://aclanthology.org/P19-1452/
[9] [10] What Does BERT Learn about the Structure of Language? - ACL Anthology
https://aclanthology.org/P19-1356/
[11] [PDF] What's so special about BERT's layers? A closer look at the NLP ...
https://research.rug.nl/files/154948143/2020.findings_emnlp.389.pdf
[12] [13] [14] Roles and Utilization of Attention Heads in Transformer-based Neural Language Models
https://aclanthology.org/2020.acl-main.311.pdf
[20] A circuit for predicting hierarchical structure in-context in Large ...
https://arxiv.org/html/2509.21534v1
[21] [22] [23] [24] [25] [26] Transformer Feed-Forward Layers Are Key-Value Memories
https://aclanthology.org/2021.emnlp-main.446.pdf
[27] [28] [29] What Matters in Transformers? Not All Attention is Needed
https://arxiv.org/html/2406.15786v1
[30] [31] [32] [33] [34] [35] Finding Cross-Lingual Syntax in Multilingual BERT | SAIL Blog
http://ai.stanford.edu/blog/finding-crosslingual-syntax/
[37] BERT (language model) - Wikipedia
https://en.wikipedia.org/wiki/BERT_(language_model)
[38] A Primer in BERTology: What We Know About How BERT Works
https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00349/96482/A-Primer-in-BERTology-What-We-Know-About-How-BERT
