# TransR: 関係固有の空間での並進による知識グラフ埋め込み学習

**著者**:
* Yankai Lin (Tsinghua University)
* Zhiyuan Liu (Tsinghua University)
* Maosong Sun (Tsinghua University)
* Yang Liu (Samsung Research and Development Institute of China)
* Xuan Zhu (Samsung Research and Development Institute of China)

**出版**: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015)
**URL**: [Learning Entity and Relation Embeddings for Knowledge Graph Completion](https://cdn.aaai.org/ojs/9491/9491-13-13019-1-2-20201228.pdf)

---

### 概要

この論文では、知識グラフの補完を目的として、エンティティと関係の埋め込みを学習する新しいモデル「TransR（Translating on Relation-specific planes/spaces）」が提案されています。TransEやTransHといった先行モデルは、エンティティと関係を同じ意味空間に配置していましたが、実際にはエンティティは複数の側面を持ち、様々な関係がエンティティの異なる側面に焦点を当てることがあります。TransRは、この問題を解決するために、エンティティ空間と関係空間を別々に構築し、エンティティを関係固有の空間に投影してから並進操作を行うことで、より洗練されたモデリングを実現します。これにより、TransEやTransHと比較してリンク予測、トリプル分類、関係事実抽出などのタスクで顕著な性能向上を達成しています。

### 新規性

TransRの最も重要な新規性は、**エンティティと関係を異なる（分離された）埋め込み空間でモデル化する**という点にあります。これまでのTransEやTransHは、エンティティと関係が同じベクトル空間に存在すると仮定していました。しかし、この仮定は以下のような問題を引き起こしていました。

* **エンティティの多面性と関係の焦点**: エンティティは多くの属性や側面を持つ一方で、関係はそれらのうち特定の側面のみに焦点を当てることがあります。例えば、「発明者」という関係は「人」というエンティティの「職業」や「創造性」の側面に焦点を当てるかもしれませんが、「居住地」という関係は「人」の「地理的情報」の側面に焦点を当てます。同じ空間では、これらの異なる側面を単一のエンティティ埋め込みで適切に表現することは困難でした。
* **次元の不一致**: エンティティと関係は概念的に異なる存在であるため、同じ次元の空間に埋め込むことが必ずしも最適ではないという指摘がありました。

TransRは、これらの課題に対し、**各関係 $r$ に対応する関係空間を導入し、エンティティの埋め込みをその関係空間に投影する**という新しいメカニズムを提案しました。これにより、エンティティが関係に応じて異なる側面を強調した表現を持つことが可能になり、関係の多様なプロパティや複雑なマッピングパターン（例：1対多、多対1、多対多）をより正確に捉えることができるようになりました。これは、TransEやTransHでは解決しきれなかった表現能力の限界を克服する画期的なアプローチでした。

### 理論/手法の核心

TransRモデルは、エンティティを $k$ 次元の**エンティティ空間** $\mathbb{R}^k$ に埋め込み、関係を $d$ 次元の**関係空間** $\mathbb{R}^d$ に埋め込みます。通常、$k$ と $d$ は異なる値をとることが許されます。

1.  **エンティティの投影**:
    * 各関係 $r \in \mathcal{L}$ に対して、関係固有の**投影行列** $\mathbf{M}_r \in \mathbb{R}^{k \times d}$ が導入されます。
    * トリプル $(h, r, t)$ が与えられたとき、ヘッドエンティティの埋め込み $\mathbf{h} \in \mathbb{R}^k$ とテールエンティティの埋め込み $\mathbf{t} \in \mathbb{R}^k$ は、この投影行列 $\mathbf{M}_r$ を用いて関係空間 $\mathbb{R}^d$ へ投影されます。

    $$
    \mathbf{h}_r = \mathbf{h} \mathbf{M}_r \\
    \mathbf{t}_r = \mathbf{t} \mathbf{M}_r
    $$
    ここで、$\mathbf{h}_r$ と $\mathbf{t}_r$ はそれぞれ、関係 $r$ に対応する関係空間に投影されたヘッドエンティティとテールエンティティのベクトル表現です。この投影により、エンティティは関係の文脈に合わせた表現を持つことができます。

2.  **スコアリング関数**:
    * 投影されたエンティティ $\mathbf{h}_r, \mathbf{t}_r$ と関係ベクトル $\mathbf{r} \in \mathbb{R}^d$ は、TransEと同様に並進の原理に従います。つまり、$h_r + r \approx t_r$ が成り立つべきであると仮定します。
    * トリプル $(h, r, t)$ の適合度（スコア）は、以下のL2ノルムの二乗（またはL1ノルム）を用いて定義されます。

    $$
    f_r(\mathbf{h}, \mathbf{t}) = ||\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r||_2^2
    $$
    または
    $$
    f_r(\mathbf{h}, \mathbf{t}) = ||\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r||_1
    $$
    スコアが小さいほど、トリプルは正しいとみなされます。

3.  **損失関数（マージンベースランキング基準）**:
    * モデルは、正例のトリプルと負例のトリプルの間にマージン $\gamma$ を設けるマージンベースのランキング基準を最小化することで学習されます。

    $$
    \mathcal{L} = \sum_{(h, r, t) \in S} \sum_{(h', r', t') \in S'_{(h,r,t)}} [\gamma + f_r(\mathbf{h}, \mathbf{t}) - f_{r'}(\mathbf{h'}, \mathbf{t'})]_+
    $$
    * **各記号の定義**:
        * $S$: 正しいトリプルの訓練セット。
        * $S'_{(h,r,t)}$: 破損した（負例の）トリプルのセット。通常、正しいトリプルのヘッドまたはテールをランダムなエンティティで置き換えることで生成されます。
        * $[x]_+ = \max(0, x)$: ヒンジ損失。
        * $\gamma > 0$: マージンハイパーパラメータ。
        * $\mathbf{h}, \mathbf{r}, \mathbf{t}$: それぞれヘッドエンティティ、関係、テールエンティティの埋め込みベクトル。
        * $\mathbf{h}', \mathbf{r}', \mathbf{t}'$: 破損したトリプルの要素。

4.  **制約**:
    * エンティティの埋め込み $\mathbf{e}$ と関係の埋め込み $\mathbf{r}$ には、それぞれ $||e||_2 \le 1$ および $||r||_2 \le 1$ のL2ノルム制約が課せられます。
    * 投影行列 $\mathbf{M}_r$ のノルムにも制約が課せられる場合があります（例：$||\mathbf{M}_r||_{Frob} \le 1$、フロベニウスノルム）。

5.  **CTransR (Cluster-based TransR)**:
    * TransRの拡張として、CTransRも提案されています。これは、特に多対多の関係において、関係が非常に多様なエンティティペアに適用される場合に、単一の関係ベクトルでは不十分であるという考えに基づいています。
    * CTransRでは、ある関係 $r$ に関連するヘッド-テールエンティティペアを複数のグループ（クラスター）に分け、各クラスターに対して異なる関係ベクトル $\mathbf{r}_c$ を学習します。
    * CTransRのスコアリング関数は以下のようになります。
        $$
        f_r(\mathbf{h}, \mathbf{t}) = ||\mathbf{h}_{r,c} + \mathbf{r}_c - \mathbf{t}_{r,c}||_2^2 + \alpha ||\mathbf{r}_c - \mathbf{r}||_2^2
        $$
        ここで、$\mathbf{r}_c$ はクラスター $c$ の関係ベクトル、$\mathbf{h}_{r,c}, \mathbf{t}_{r,c}$ はクラスター $c$ に関連する投影されたエンティティベクトルです。$\alpha$ は、クラスター関係ベクトル $\mathbf{r}_c$ が元の関係ベクトル $\mathbf{r}$ からあまり離れないように制御するハイパーパラメータです。

### 「キモ」と重要性

TransRの「キモ」は、**エンティティと関係を別々の空間で表現し、関係ごとにエンティティをその関係空間に投影する**という、より現実的で柔軟なモデリングアプローチにあります。これにより、TransEやTransHが抱えていた、エンティティの多面性や関係の焦点の問題を効果的に解決しました。

このアイデアの重要性は以下の点に集約されます。

* **表現能力の飛躍的向上**: エンティティが関係ごとに異なる「側面」を表現できるようになったことで、モデルはより複雑な関係パターン（例：1対多、多対1、多対多）や、関係がエンティティの特定の属性に焦点を当てるようなセマンティクスを、はるかに正確に捉えることが可能になりました。これは、知識グラフのセマンティックな豊かさをモデルに反映させる上で極めて重要な進歩でした。
* **実用タスクでの優れた性能**: TransRは、リンク予測、トリプル分類、関係事実抽出といった主要な知識グラフ関連タスクにおいて、TransEやTransHといった先行モデルを大幅に上回る性能を示しました。これは、理論的な洗練が実用的な成果に直結することを示しています。
* **知識グラフ埋め込み研究の深化**: TransRは、エンティティと関係を分離した空間でモデル化するという新しいパラダイムを確立しました。このアイデアは、その後の知識グラフ埋め込み研究に大きな影響を与え、関係の複雑さやエンティティの多義性をより深く探求する多くの派生モデル（例：TransD, TransGなど）の基礎となりました。これにより、より高性能で実用的な知識グラフ埋め込みモデルの開発が加速されました。

TransRは、知識グラフの埋め込みにおいて、表現能力と計算効率のバランスを取りながら、モデルのセマンティックな豊かさを大幅に向上させた、非常に影響力の大きい論文であると言えます。
