# Quantifying Attention Flow in Transformers

---

### **1. 生の注意（Raw Attention）**

### **概念**

生の注意（Raw Attention）は、トランスフォーマーモデル内で直接計算される注意重みそのものです。各層の自己注意メカニズムで得られる注意行列（Attention Matrix）から得られます。この行列のエントリは、特定のトークンが他のトークンにどれだけ「注意」を向けているか（重要性）を示しています。

### **特徴**

- **情報源**: 各トークン間の直接的な依存関係を示します。
- **高層での問題**: モデルの層が深くなると、埋め込みが文脈化され、注意重みがほぼ均一になる傾向があります。これにより、トークン間の識別性が失われ、生の注意重みが入力トークンの重要性を反映しにくくなります。
- **解釈の限界**: 注意重みが必ずしもモデルの意思決定におけるトークンの重要性を表していないため、解釈に誤りが生じる可能性があります（Serrano and Smith, 2019）。

### **利用ケース**

生の注意は簡単に計算できるため、トランスフォーマーモデルの可視化や初期分析によく用いられます。しかし、その単純さゆえに、深い層では信頼性が低下します。

---

### **2. 注意ロールアウト（Attention Rollout）**

### **概念**

注意ロールアウトは、モデル内の各層を通じて、入力トークンから埋め込みへの情報伝播を追跡する手法です。この方法では、入力トークンの「識別子」が注意重みに基づいて線形的に結合されると仮定します。

### **計算方法**

- 各層の注意重みを前層の注意重みと再帰的に掛け合わせていきます（行列積を用いる）。
- 残差接続を考慮するため、注意行列に単位行列を加え、重みを再正規化します。
- 式としては以下のように表されます：

ここで、は注意ロールアウト、は生の注意を表します。
    
    A~(li)={A(li)A~(li−1)if i>jA(li)if i=j\tilde{A}(l_i) =
    \begin{cases}
    A(l_i) \tilde{A}(l_{i-1}) & \text{if } i > j \\
    A(l_i) & \text{if } i = j
    \end{cases}
    
    A~\tilde{A}
    
    AA
    

### **特徴**

- **線形結合**: 注意重みを単純な割合として扱い、情報を伝播させます。
- **焦点化**: 注意ロールアウトの結果は、より集中的な注意パターンを生成します。特定の入力トークンに高い重みが集中しやすく、解釈しやすい点があります。
- **欠点**: 仮定が単純すぎるため、結果が必ずしも正確でない場合があります。

### **利用ケース**

注意ロールアウトは、入力トークンから高層埋め込みまでの情報の流れを解析するのに適しています。また、入力トークンの重要性を評価する場合にも役立ちます。

---

### **3. 注意フロー（Attention Flow）**

### **概念**

注意フローは、注意グラフをフローネットワーク（流れのあるグラフ）と見なし、情報伝播を最大フローアルゴリズムを用いて計算する手法です。これにより、各入力トークンから高層の埋め込みへの最大注意フローを評価します。

### **計算方法**

- フローネットワークのエッジ容量を注意重みとして扱います。
- 最大フローアルゴリズムを適用し、ソースノード（入力トークン）からシンクノード（高層埋め込み）への最大フロー値を計算します。
- フロー値は各経路のエッジ重みの**最小値**として計算されます（ロールアウトのような行列積ではなく）。

### **特徴**

- **容量ベースの計算**: 各エッジが持つ最大の容量を活用して情報の流れを計算します。
- **分散的な重み**: 注意フローは最も重要なトークンの集合全体を示し、注意ロールアウトのような鋭い区別を生成しません。そのため、複数のトークンが重要な場合に役立ちます。
- **計算コスト**: 計算の複雑性が高く、注意ロールアウト（）よりも注意フロー（）の方が計算コストが高いです。
    
    O(d⋅n2)O(d \cdot n^2)
    
    O(d2⋅n4)O(d^2 \cdot n^4)
    

### **利用ケース**

注意フローは、特定のトークンがどの程度の情報をモデルの高層まで運んでいるかを理解するのに有用です。特に、複数のトークンが結果に影響を与える場合、注意フローはその重要な集合を特定できます。

---

### **比較まとめ**

| 特徴 | 生の注意 | 注意ロールアウト | 注意フロー |
| --- | --- | --- | --- |
| **基本原理** | 生の注意重みをそのまま使用 | 注意重みを再帰的に計算 | 注意グラフをフローネットワークとして扱う |
| **情報の伝播方法** | 層ごとに計算 | 線形的に伝播 | 最大フローとして伝播 |
| **解釈可能性** | 限定的（層が深いと均一化） | 集中的で明確な注意パターン | 分散的でトークン集合を示す |
| **計算コスト** | 低い | 中程度 | 高い |
| **適用例** | 可視化や初期分析 | 入力トークンの重要性評価 | 複数トークンが重要な場合の解析 |

---

### **総合的な考察**

- **生の注意**は計算が簡単で直感的ですが、深い層では信頼性が低くなります。
- **注意ロールアウト**は、入力トークンの情報伝播を集中的に追跡するのに適していますが、単純化された仮定があるため、結果に限界があります。
- **注意フロー**は計算コストが高いものの、複数のトークンが重要な場合や、全体的な情報流れを解析したい場合に非常に有用です。

他に特定の手法や応用例について詳細を知りたい場合は教えてください！