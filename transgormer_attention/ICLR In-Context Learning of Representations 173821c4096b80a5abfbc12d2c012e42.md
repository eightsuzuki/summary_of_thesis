# ICLR: In-Context Learning of Representations

---

### **研究の概要**

本研究では、大規模言語モデル（LLM）が事前学習データに基づいて概念の表現をどのように構造化するかを探ります。特に、モデルが「インコンテキスト学習」（ICL）を通じて、事前学習されたセマンティクスを、提供された文脈に基づいて新たに定義されたセマンティクスに再編成する能力を持つかを検討しました。この問いを検証するために、「グラフ追跡タスク」という単純なタスクを提案し、モデルの中間表現を分析しました。

---

### **実験のセットアップ**

### **グラフ追跡タスク**

- グラフのノードには「apple」「bird」などの一般的な概念が割り当てられます。
- グラフは事前定義された幾何構造（例えば、正方格子やリング構造）を持ちます。
- モデルはランダムウォークのトレース（例: ノード間の遷移データ）を入力として与えられ、その構造を学習することが期待されます。

---

### **主な結果**

![image.png](<image/ICLR In-Context Learning of Representations 173821c4096b80a5abfbc12d2c012e42/image.png>)

![image.png](<image/ICLR In-Context Learning of Representations 173821c4096b80a5abfbc12d2c012e42/image 1.png>)

![image.png](<image/ICLR In-Context Learning of Representations 173821c4096b80a5abfbc12d2c012e42/image 2.png>)

### **内部表現の可視化**

主成分分析（PCA）を用いて、モデルの中間表現がコンテキストによってどのように変化するかを観察しました。

1. **正方格子やリング構造の表現**:
    - 十分なコンテキストが与えられると、モデルの表現がグラフ構造を反映する形で再編成されます（図1, 図2参照）。
    - 初期層では事前学習されたセマンティクスの影響が見られますが、層が深くなるにつれてグラフ構造が支配的になります。
2. **セマンティクスの競合**:
    - 事前学習されたセマンティクス（例: 曜日の循環構造）と、コンテキストによる新しいセマンティクスが競合する場合、新しい構造は高次の主成分に現れることが確認されました（図3参照）。

### **ディリクレエネルギーの最小化**

モデル表現がグラフ構造をどの程度正確に反映しているかを評価するために、ディリクレエネルギーを計算しました。
$$
EG(x)=∑i,jAi,j(xi−xj)2E_G(x) = \sum_{i,j} A_{i,j} (x_i - x_j)^2
$$
エネルギーの減少は、モデルが正しいグラフ構造を学習していることを示唆します。

---

### **エネルギー最小化仮説**

モデルは暗黙的なエネルギー最小化プロセスを通じて、データの構造的表現を推論している可能性があります。

### **スペクトル埋め込みと主成分分析**

ディリクレエネルギーの最小化によって得られる表現は、PCAの主要な成分$（z(2), z(3)）$と一致することが証明されました。

---

### **議論**

1. **文脈拡大による能力の向上**:
    - コンテキストのスケール拡大により、新たな能力が出現する可能性があります。
2. **神経科学との関連**:
    - 本研究の結果は、脳が抽象的な構造知識を形成するプロセス（例: 海馬-嗅内皮質）に類似している可能性があります。

---

この研究は、LLMがコンテキストに基づいて柔軟に表現を再構築できる能力を示し、ディリクレエネルギー最小化を仮説として提案しています。興味があれば詳細な数式展開や証明についても解説できますので、お知らせください。