# Mapping 1,000+ Language Models via the Log-Likelihood Vector

**「2 言語モデルをテキスト確率分布空間にマッピングする」** の節の全文翻訳です（省略せずに全文を訳しています citeturn1file2）。

---

# 2 言語モデルをテキスト確率分布空間にマッピングする

本節では、我々の提案手法を提示する。セクション2.2および2.3では、テキスト生成確率から導出されるモデルの特徴ベクトルを紹介する。セクション2.4では、これらの特徴を用いて構築された座標系における二乗ユークリッド距離が、モデル間のカルバック・ライブラー（KL）ダイバージェンスに近似することを示す。さらに、セクション2.5では、得られたモデル座標の解釈について述べる。なお、あるトークン系列を生成するための条件付き確率の系列を用いてモデル座標を定義する手法の拡張は、付録Eに示している。

---

## 2.1 自己回帰型言語モデル

ここで、XX をすべての可能なテキストの集合、VV をトークンの語彙とする。テキスト x∈Xx \in X はトークンの列として表され、すなわち

x=(y1,…,yn),yt∈V.x = (y_1, \dots, y_n), \quad y_t \in V.

テキストの最大長を nmax⁡n_{\max} とすれば、

X=⋃n=0nmax⁡Vn.X = \bigcup_{n=0}^{n_{\max}} V^n.

また、KK 個の言語モデル {pi}i=1K\{p_i\}_{i=1}^K を考える。ここで、y0y_0 はシーケンス開始（BOS）トークンを表し、各言語モデル pip_i は直前のトークン列

yt−1=(y0,…,yt−1)y_{t-1} = (y_0, \dots, y_{t-1})

を条件として次のトークン yty_t を予測する。すなわち、pip_i による条件付き確率は

yt∼pi(yt∣yt−1),t=1,…,n.y_t \sim p_i(y_t \mid y_{t-1}), \quad t = 1, \dots, n.

このとき、モデル pip_i の下でのテキスト xx の確率（x∼pix \sim p_i と書く）は

pi(x)=∏t=1npi(yt∣yt−1)p_i(x) = \prod_{t=1}^{n} p_i(y_t \mid y_{t-1})

と与えられる。

さらに、理論的な議論のために、基底分布を表す言語モデル p0p_0 も導入する。ここでは、各テキストが互いに独立に p0p_0 から抽出された NN 個のテキストからなるデータセット（コーパス）

D=(x1,x2,…,xN)∈XND = (x_1, x_2, \dots, x_N) \in X^N

を仮定する。

---

## 2.2 対数尤度ベクトル

モデル pip_i に対して、テキスト xx を生成する確率を pi(x)p_i(x) と表す。統計モデル選択の慣例に従い、pi(x)p_i(x) を「テキスト xx が与えられたときのモデル pip_i の尤度」と呼ぶ。これに対し、対数尤度は次のように定義される：

ℓi(x)=∑t=1nlog⁡pi(yt∣yt−1).\ell_i(x) = \sum_{t=1}^{n} \log p_i(y_t \mid y_{t-1}).

実際の言語モデルの実装では、−ℓi(x)-\ell_i(x) がテキスト xx のクロスエントロピー損失に対応し、exp⁡(−ℓi(x)/n)\exp(-\ell_i(x)/n) はパープレキシティとして知られている。

我々の手法は非常にシンプルである。データセット DD が NN 個のテキストからなることから、各モデル pip_i の特徴ベクトルとして対数尤度ベクトル

ℓi=(ℓi(x1),…,ℓi(xN))⊤∈RN\ell_i = \bigl( \ell_i(x_1), \dots, \ell_i(x_N) \bigr)^\top \in \mathbb{R}^N

を用いる。まず、これら KK 個のモデルの対数尤度ベクトル ℓi\ell_i を縦に積み上げて、対数尤度行列

L=(ℓ1⋮ℓK)∈RK×NL = \begin{pmatrix} \ell_1 \\ \vdots \\ \ell_K \end{pmatrix} \in \mathbb{R}^{K \times N}

を構築する。

---

## 2.3 ダブルセンタリング

モデル解析の前処理として、我々は行列 LL に対してダブルセンタリング（Borg and Groenen, 2005）という手法を適用する。まず、各行（各モデルごと）のセンタリングを行う。各行の平均、すなわち平均対数尤度は次のように定義される：

ℓˉi=1N∑s=1Nℓi(xs).\bar{\ell}_i = \frac{1}{N} \sum_{s=1}^{N} \ell_i(x_s).

この値を各成分から引くことで、中心化された対数尤度ベクトル ξi=(ξi1,…,ξiN)⊤∈RN\xi_i = (\xi_{i1}, \dots, \xi_{iN})^\top \in \mathbb{R}^N を定義する。ここで

ξis:=ℓi(xs)−ℓˉi,s=1,…,N.\xi_{is} := \ell_i(x_s) - \bar{\ell}_i, \quad s = 1, \dots, N.

次に、この中心化されたベクトル群 (ξ1,…,ξK)⊤(\xi_1, \dots, \xi_K)^\top に対して、列ごと（各テキストごと）のセンタリングを行う。全モデルの平均ベクトルは

ξˉ=1K∑i=1Kξi\bar{\xi} = \frac{1}{K} \sum_{i=1}^{K} \xi_i

と定義され、この平均ベクトルを各 ξi\xi_i から引くことで、ダブルセンタリングされた対数尤度ベクトル

qi=ξi−ξˉq_i = \xi_i - \bar{\xi}

を得る。詳細は付録BおよびCを参照せよ。

---

## 2.4 カルバック・ライブラー（KL）ダイバージェンス

カルバック・ライブラー（KL）ダイバージェンスは、確率分布空間において2つのモデル pip_i と pjp_j の差異を測るために頻繁に用いられる指標である。これは以下のように定義される：

KL(pi,pj)=∑x∈Xpi(x)log⁡pi(x)pj(x)=Ex∼pi(ℓi(x)−ℓj(x)).(1)KL(p_i, p_j) = \sum_{x \in X} p_i(x) \log \frac{p_i(x)}{p_j(x)} = \mathbb{E}_{x \sim p_i} \bigl( \ell_i(x) - \ell_j(x) \bigr). \quad (1)

ここで、データセット DD は未知の基底モデル p0p_0 から生成され、かつモデル pip_i と pjp_j は p0p_0 を十分に近似していると仮定する。この仮定の下、KLダイバージェンスは次のように近似できる：

2 KL(pi,pj)≈Var⁡x∼p0(ℓi(x)−ℓj(x)).(2)2\,KL(p_i, p_j) \approx \operatorname{Var}_{x \sim p_0} \bigl( \ell_i(x) - \ell_j(x) \bigr). \quad (2)

(1) の定義が ℓi(x)−ℓj(x)\ell_i(x) - \ell_j(x) の期待値を含むのに対し、(2) の近似は分散の形をとる。この結果はやや驚くべきであるが、非常に示唆に富む。特に、KLダイバージェンス自体は非対称であるが、(2) の近似は対称となる。我々は、データセット DD に基づいて以下のように (2) を推定する：

2 KL(pi,pj)≈1N∥ξi−ξj∥2.(3)2\,KL(p_i, p_j) \approx \frac{1}{N} \| \xi_i - \xi_j \|^2. \quad (3)

なお、∥ξi−ξj∥2=∥qi−qj∥2\| \xi_i - \xi_j \|^2 = \| q_i - q_j \|^2 であるため、(3) が成立する。さらに、もしモデル pip_i の座標を qi/Nq_i/\sqrt{N} とみなすならば、2点間の二乗ユークリッド距離は 2 KL(pi,pj)2\,KL(p_i, p_j) を近似することになる。

---

## 2.5 モデル座標

我々は主に、モデル pip_i の特徴ベクトルとして qiq_i を用い、これをモデル座標と呼ぶ。(3) で示したように、qq 座標系における二乗ユークリッド距離は、言語モデル間のKLダイバージェンスを近似する。これは、qiq_i が確率分布空間内における pip_i の位置を表していることを意味する。なお、ξi\xi_i は原点からのオフセットのみが qiq_i と異なるため、ξi\xi_i もまたモデル座標として機能し、∥qi−qj∥2=∥ξi−ξj∥2\| q_i - q_j \|^2 = \| \xi_i - \xi_j \|^2 となる。しかし、より解釈しやすい成分を持つ qiq_i を採用するため、本論文では以降 qiq_i を用いる。

可視化の目的では、モデルマップの座標として主に ℓi\ell_i を使用する。これは、ℓi\ell_i が直感的に「高さ」方向に N ℓˉi\sqrt{N}\,\bar{\ell}_i を、また「水平方向」には qiq_i をエンコードしていると解釈できるからである。付録D.6に示すように、

∥ℓi−ℓj∥2=∥qi−qj∥2+N(ℓˉi−ℓˉj)2,(4)\| \ell_i - \ell_j \|^2 = \| q_i - q_j \|^2 + N(\bar{\ell}_i - \bar{\ell}_j)^2, \quad (4)

すなわち、ℓ\ell 座標系における二乗ユークリッド距離は 2 KL(pi,pj)2\,KL(p_i, p_j) と N(ℓˉi−ℓˉj)2N(\bar{\ell}_i - \bar{\ell}_j)^2 の和に分解できる。

---

以上が、セクション2「言語モデルをテキスト確率分布空間にマッピングする」の全文の日本語訳となります。各数式および説明は原文の内容を省略せずに忠実に訳しています citeturn1file2.

以下は、セクション2.4で示されている計算の詳細な説明です。

---

### 1. KLダイバージェンスの定義

まず、言語モデル pip_i と pjp_j について、KLダイバージェンスは次のように定義されます：

KL(pi,pj)=∑x∈Xpi(x)log⁡pi(x)pj(x)=Ex∼pi(ℓi(x)−ℓj(x)).(1)KL(p_i, p_j) = \sum_{x \in X} p_i(x) \log \frac{p_i(x)}{p_j(x)} = \mathbb{E}_{x\sim p_i}\bigl(\ell_i(x)-\ell_j(x)\bigr). \tag{1}

ここで、ℓi(x)=∑t=1nlog⁡pi(yt∣yt−1)\ell_i(x) = \sum_{t=1}^{n} \log p_i(y_t\mid y_{t-1}) は、テキスト xx に対するモデル pip_i の対数尤度です。

---

### 2. モデルが真の分布を十分に近似しているという仮定

データセット DD のテキストは未知の基底モデル p0p_0 から生成され、さらに pip_i と pjp_j は p0p_0 を十分に近似していると仮定します。この場合、両モデルとも p0p_0 に対してよく近似しているため、

Ex∼p0(ℓi(x)−ℓj(x))\mathbb{E}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr)

はほぼゼロになると考えられます。つまり、期待値部分は小さい（またはゼロ）とみなせるため、KLダイバージェンスの大きさは

「対数尤度差のばらつき（分散）」によって支配されると考えられ、次のように近似されます：

2 KL(pi,pj)≈Var⁡x∼p0(ℓi(x)−ℓj(x)).(2)2\,KL(p_i, p_j) \approx \operatorname{Var}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr). \tag{2}

※ここで、分散 Var⁡(X)\operatorname{Var}(X) は XX の平均からの偏差の二乗の期待値として定義されます。

---

### 3. サンプルデータからの近似

実際の解析では、データセット D={x1,x2,…,xN}D = \{x_1, x_2, \dots, x_N\} を用いて上記の分散を推定します。

サンプル分散は、

1N∑s=1N{(ℓi(xs)−ℓj(xs))−Ex∼p0(ℓi(x)−ℓj(x))}2\frac{1}{N} \sum_{s=1}^{N} \Bigl\{ \Bigl(\ell_i(x_s)-\ell_j(x_s)\Bigr) - \mathbb{E}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr) \Bigr\}^2

と表されますが、上記の仮定より期待値の項はほぼゼロと考えられるため、実質的には

1N∑s=1N(ℓi(xs)−ℓj(xs))2\frac{1}{N} \sum_{s=1}^{N} \Bigl(\ell_i(x_s)-\ell_j(x_s)\Bigr)^2

となります。

---

### 4. 対数尤度ベクトルの中心化と分散の計算

各モデル pip_i の対数尤度ベクトルを

ℓi=(ℓi(x1),ℓi(x2),…,ℓi(xN))⊤\ell_i = \bigl(\ell_i(x_1), \ell_i(x_2), \dots, \ell_i(x_N)\bigr)^\top

と定義し、その平均を

ℓˉi=1N∑s=1Nℓi(xs)\bar{\ell}_i = \frac{1}{N} \sum_{s=1}^{N} \ell_i(x_s)

とします。ここで、各要素を平均値で引いて中心化したベクトルを

ξi=ℓi−ℓˉi1N,\xi_i = \ell_i - \bar{\ell}_i\mathbf{1}_N,

すなわち ξis=ℓi(xs)−ℓˉi,\text{すなわち } \xi_{is} = \ell_i(x_s) - \bar{\ell}_i,

と定義します。同様に、モデル pjp_j に対しても ξj\xi_j を定義します。

すると、各テキスト xsx_s における対数尤度差から平均差を除いた値は

ℓi(xs)−ℓj(xs)−(ℓˉi−ℓˉj)=ξis−ξjs\ell_i(x_s)-\ell_j(x_s) - (\bar{\ell}_i - \bar{\ell}_j) = \xi_{is} - \xi_{js}

となります。このとき、サンプル分散は

1N∑s=1N(ξis−ξjs)2=1N∥ξi−ξj∥2\frac{1}{N} \sum_{s=1}^{N} (\xi_{is} - \xi_{js})^2 = \frac{1}{N}\|\xi_i - \xi_j\|^2

と表せます。これにより、式 (2) の推定として

2 KL(pi,pj)≈1N∥ξi−ξj∥2(3)2\,KL(p_i, p_j) \approx \frac{1}{N}\|\xi_i - \xi_j\|^2 \tag{3}

が得られます。

また、付録では qiq_i を「ダブルセンタリング」したベクトルとして定義しており、

∥ξi−ξj∥2=∥qi−qj∥2\| \xi_i - \xi_j \|^2 = \| q_i - q_j \|^2

が成立するため、同様に (3) は qq 座標系でも成り立ちます。

---

### 5. 座標のスケーリングによる解釈

さらに、もし各モデルの座標を

qiN\frac{q_i}{\sqrt{N}}

とスケールすると、2点間の二乗ユークリッド距離は

∥qiN−qjN∥2=1N∥qi−qj∥2,\left\| \frac{q_i}{\sqrt{N}} - \frac{q_j}{\sqrt{N}} \right\|^2 = \frac{1}{N}\|q_i - q_j\|^2,

となり、これがちょうど 2 KL(pi,pj)2\,KL(p_i, p_j) に近似されることになります。つまり、スケール済みの座標間の距離が、KLダイバージェンスを表す指標として機能するという解釈が可能です。

---

以上のように、

1. KLダイバージェンスの定義から始まり、
2. 真の分布  からの生成という仮定を用いて期待値がほぼゼロとなると考え、
    
    p0p_0
    
3. 対数尤度の差の分散として KL ダイバージェンスを近似し、
4. サンプルデータに基づいて中心化した対数尤度ベクトルのユークリッド距離（正確にはその二乗）から分散を推定する、
という流れで式 (3) が導出されています citeturn1file2。

この計算により、モデル間の距離を効率的かつ大規模に計算することが可能となり、従来のペアワイズテキスト生成を行う手法に比べ、計算コストが大幅に削減されるという利点があります。

以下は、式 (2) への変換過程の中間式（導出過程）の例です。なお、ここでは以下の仮定をおきます。

- テキスト  は未知の基底モデル  に従って生成される。
    
    xx
    
    p0p_0
    
- 各モデル  と  は  を十分に近似しているため、
    
    pip_i
    
    pjp_j
    
    p0p_0
    
    Ex∼p0(ℓi(x)−ℓj(x))≈0.\mathbb{E}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr) \approx 0.
    

この仮定の下で、以下のような変換過程を考えます。

---

1. まず、対数尤度差を
    
    Δ(x):=ℓi(x)−ℓj(x)\Delta(x) := \ell_i(x)-\ell_j(x)
    
    とおくと、定義 (1) より
    
    KL(pi,pj)=Ex∼pi(Δ(x)).KL(p_i, p_j) = \mathbb{E}_{x\sim p_i}\bigl(\Delta(x)\bigr).
    
    しかし、pip_i が p0p_0 に近いので、期待値の評価は p0p_0 で行っても大きな誤差は生じないとみなせ、
    
    Ex∼p0(Δ(x))≈0.\mathbb{E}_{x\sim p_0}\bigl(\Delta(x)\bigr) \approx 0.
    
2. 次に、もし Δ(x)\Delta(x) の平均がゼロ（または非常に小さい）ならば、その「ばらつき」は
    
    Var⁡x∼p0(Δ(x))=Ex∼p0[Δ(x)2]−(Ex∼p0Δ(x))2≈Ex∼p0[Δ(x)2].\operatorname{Var}_{x\sim p_0}\bigl(\Delta(x)\bigr) = \mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr] - \Bigl(\mathbb{E}_{x\sim p_0}\Delta(x)\Bigr)^2 \approx \mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr].
    
3. 一方で、Δ(x)\Delta(x) が p0p_0 周りの小さな摂動とみなせる場合、対数の性質から第二次の項が支配的になるという性質（2次近似）が成立します。具体的には、対数関数のテイラー展開を用いると、
    
    log⁡pi(x)pj(x)≈Δ(x)−12Δ(x)2,\log \frac{p_i(x)}{p_j(x)} \approx \Delta(x) - \frac{1}{2}\Delta(x)^2,
    
    （※厳密にはモデル間の差が小さい場合の近似）となり、pip_i および pjp_j が p0p_0 に近いと仮定すると、1次の項の寄与は互いに打ち消しあい、主要な寄与は二乗項に現れると考えられます。
    
4. その結果、KLダイバージェンスは
    
    KL(pi,pj)≈12 Ex∼p0[Δ(x)2],KL(p_i, p_j) \approx \frac{1}{2}\,\mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr],
    
    すなわち
    
    2 KL(pi,pj)≈Ex∼p0[Δ(x)2]≈Var⁡x∼p0(ℓi(x)−ℓj(x)).2\,KL(p_i, p_j) \approx \mathbb{E}_{x\sim p_0}\bigl[\Delta(x)^2\bigr] \approx \operatorname{Var}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr).
    

これにより、式 (2)

2 KL(pi,pj)≈Var⁡x∼p0(ℓi(x)−ℓj(x))2\,KL(p_i, p_j) \approx \operatorname{Var}_{x\sim p_0}\bigl(\ell_i(x)-\ell_j(x)\bigr)

が得られるという変換となります。

---

この変換過程では、モデル間の差が小さい（＝ pip_i や pjp_j が p0p_0 に非常に近い）という仮定のもと、期待値がほぼゼロであるために二乗項（すなわち分散）が主要な寄与となるという考え方を用いています .