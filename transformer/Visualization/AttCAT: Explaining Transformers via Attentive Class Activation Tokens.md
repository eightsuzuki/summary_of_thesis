# AttCAT: Explaining Transformers via Attentive Class Activation Tokens

**著者**: Yao Qiang (Wayne State University), Deng Pan (Wayne State University), Chengyin Li (Wayne State University), Xin Li (Wayne State University), Rhongho Jang (Wayne State University), Dongxiao Zhu (Wayne State University)  
**会議**: NeurIPS 2022  
**論文リンク**: [NeurIPS 2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/20e45668fefa793bd9f2edf19be12c4b-Abstract-Conference.html)  
**コード**: [GitHub](https://github.com/qiangyao1988/AttCAT)

---

### 概要

この論文は、Transformerモデルの内部動作を説明するための新しい手法**AttCAT (Attentive Class Activation Tokens)**を提案した研究です。Transformerは様々な自然言語処理やコンピュータビジョンタスクで優れた性能を示していますが、その成功の理由は十分に説明されていません。既存の説明手法は、自己注意機構の解剖や勾配ベースのアトリビューションに焦点を当てていますが、以下の理由によりTransformerの内部動作を忠実に説明できていません：第一に、特徴値の大きさを考慮せずに注意重みだけを見ても自己注意機構を十分に明らかにできないこと、第二に、Transformerの情報フローの重要な部分を占めるスキップ接続モジュールが説明において十分に活用されていないこと、第三に、個別特徴の勾配ベースアトリビューションが特徴間の相互作用を考慮していないことです。本論文では、エンコードされた特徴、その勾配、および注意重みを活用して、Transformerの出力に対する忠実で信頼性の高い説明を生成する手法を提案しています。

---

### 論文の核心：既存手法の問題点とAttCATの解決策

#### 1. 既存説明手法の問題点

Transformerの説明手法には、以下の3つの主要な問題があります：

##### 問題1：注意重みだけでは不十分

- **注意重みの限界**: 注意重みだけでは、特徴値の大きさを考慮していないため、自己注意機構の実際の動作を十分に反映できない
- **情報の欠落**: 注意重みが高くても、対応する特徴値が小さい場合、実際の寄与は小さい可能性がある
- **誤解を招く解釈**: 注意重みだけを見ると、実際の寄与を過大評価または過小評価する可能性がある

##### 問題2：スキップ接続の活用不足

- **スキップ接続の重要性**: Transformerでは、スキップ接続（残差接続）が情報フローの重要な部分を占めている
- **既存手法の限界**: 既存の説明手法は主に自己注意モジュールに焦点を当てており、スキップ接続が十分に活用されていない
- **情報の見落とし**: スキップ接続を通じた情報の流れが説明に反映されていない

##### 問題3：特徴間の相互作用の欠如

- **個別特徴の限界**: 勾配ベースのアトリビューションは、個別の特徴の寄与を計算するが、特徴間の相互作用を考慮していない
- **非線形性の無視**: Transformerの非線形な性質により、特徴間の相互作用が重要であるにもかかわらず、それが説明に反映されていない
- **不完全な説明**: 特徴間の相互作用を無視した説明は、モデルの実際の動作を完全に説明できない

#### 2. AttCATの解決策

AttCATは、以下の3つの要素を組み合わせることで、これらの問題を解決します：

- **エンコードされた特徴**: 特徴値の大きさを考慮
- **勾配**: 特徴の感度を測定
- **注意重み**: 特徴間の関係を捉える

これらの要素を統合することで、Transformerの出力に対する忠実で信頼性の高い説明を生成します。

---

### AttCATのアプローチ

#### 1. Attentive Class Activation Tokens (AttCAT) の概念

AttCATは、**注意を考慮したクラス活性化トークン**という概念に基づいています。

##### Class Activation Tokens (CAT)

- **クラス活性化**: 特定のクラス（カテゴリ）に対して、どのトークンが活性化しているかを特定
- **トークンレベルの説明**: 各トークンが分類決定にどの程度寄与しているかを定量化

##### Attentive Component

- **注意の統合**: 注意重みを考慮することで、トークン間の関係を捉える
- **特徴値との組み合わせ**: 注意重みと特徴値の大きさを組み合わせて、実際の寄与を計算

#### 2. 説明の生成プロセス

AttCATの説明生成プロセスは、以下のステップで構成されます：

##### ステップ1：特徴の抽出

- **エンコードされた特徴**: Transformerの各層からエンコードされた特徴を抽出
- **特徴値の大きさ**: 各特徴の値の大きさを記録

##### ステップ2：勾配の計算

- **勾配の計算**: 目標クラスに対する各特徴の勾配を計算
- **感度の測定**: 勾配の大きさから、特徴が出力にどの程度敏感かを測定

##### ステップ3：注意重みの取得

- **注意重みの抽出**: 自己注意機構から注意重みを抽出
- **トークン間の関係**: 注意重みから、トークン間の関係を理解

##### ステップ4：統合と説明の生成

- **3要素の統合**: エンコードされた特徴、勾配、注意重みを統合
- **AttCATスコアの計算**: 統合された情報から、各トークンのAttCATスコアを計算
- **説明の可視化**: 計算されたスコアを可視化して説明を生成

#### 3. スキップ接続の考慮

AttCATは、スキップ接続を明示的に考慮します：

- **残差接続の追跡**: スキップ接続を通じた情報の流れを追跡
- **情報の統合**: スキップ接続と自己注意機構の両方からの情報を統合
- **完全な情報フロー**: Transformerの完全な情報フローを説明に反映

---

### 技術的な詳細

#### 1. AttCATスコアの計算

AttCATスコアは、以下の要素を組み合わせて計算されます：

##### エンコードされた特徴の寄与

各トークン $i$ のエンコードされた特徴 $f_i$ の寄与を考慮します。

##### 勾配ベースの感度

目標クラス $c$ に対する特徴 $f_i$ の勾配 $\frac{\partial y_c}{\partial f_i}$ を計算し、感度を測定します。

##### 注意重みの統合

トークン $i$ からトークン $j$ への注意重み $A_{ij}$ を考慮し、トークン間の関係を捉えます。

##### 統合されたAttCATスコア

これらの要素を組み合わせて、各トークン $i$ のAttCATスコア $S_i$ を計算します：

$$S_i = \sum_j \alpha_{ij} \cdot f_j \cdot \frac{\partial y_c}{\partial f_j}$$

ここで、$\alpha_{ij}$ は注意重みと特徴値の大きさを考慮した重み付け係数です。

#### 2. 特徴間の相互作用の考慮

AttCATは、特徴間の相互作用を以下のように考慮します：

- **注意による相互作用**: 注意重みを通じて、トークン間の相互作用を捉える
- **非線形な相互作用**: 勾配と特徴値の組み合わせにより、非線形な相互作用を考慮
- **統合的な説明**: 個別の特徴だけでなく、特徴間の相互作用も説明に反映

#### 3. スキップ接続の統合

スキップ接続を考慮するため、以下のアプローチを使用します：

- **残差接続の追跡**: 各層での残差接続を通じた情報の流れを追跡
- **情報の統合**: 自己注意機構とスキップ接続の両方からの情報を統合
- **完全な説明**: Transformerのすべての情報フローを説明に反映

---

### 実験と評価

#### 1. 実験設定

論文では、以下の設定で実験が行われました：

##### データセット

- **自然言語処理タスク**: 感情分析、自然言語推論など
- **コンピュータビジョンタスク**: 画像分類など
- **様々なドメイン**: テキスト、画像など、様々なドメインで評価

##### モデル

- **BERT**: Bidirectional Encoder Representations from Transformers
- **ViT**: Vision Transformer
- **その他のTransformerアーキテクチャ**: 様々なTransformerベースモデルで評価

##### ベースライン手法

- **注意重みのみ**: 注意重みだけを使用した説明
- **勾配ベース手法**: Integrated Gradients、Gradient × Inputなど
- **既存の説明手法**: その他の既存のTransformer説明手法

#### 2. 評価指標

AttCATの性能を評価するために、以下の指標が使用されました：

##### 忠実性（Faithfulness）

- **予測への寄与**: 説明がモデルの予測にどれだけ忠実かを測定
- **特徴の重要度**: 説明された重要度が実際の寄与と一致するかを検証

##### 信頼性（Confidence）

- **説明の一貫性**: 同じ入力に対する説明の一貫性を測定
- **ノイズへの頑健性**: ノイズに対する説明の頑健性を評価

##### 汎用性（Generalization）

- **アーキテクチャ間の汎用性**: 異なるTransformerアーキテクチャでの性能
- **タスク間の汎用性**: 異なるタスクでの性能
- **データセット間の汎用性**: 異なるデータセットでの性能

#### 3. 主要な結果

実験結果から、以下のことが明らかになりました：

##### 忠実性の向上

- **既存手法との比較**: AttCATは、既存の説明手法と比較して、より忠実な説明を生成
- **特徴値の考慮**: 特徴値の大きさを考慮することで、より正確な説明が可能
- **スキップ接続の効果**: スキップ接続を考慮することで、説明の忠実性が向上

##### 信頼性の向上

- **一貫性**: AttCATは、同じ入力に対して一貫した説明を生成
- **頑健性**: ノイズに対して頑健な説明を生成
- **信頼性の高い説明**: より信頼性の高い説明を提供

##### 汎用性の実証

- **アーキテクチャ間**: 様々なTransformerアーキテクチャで優れた性能を示す
- **タスク間**: 様々なタスクで優れた性能を示す
- **データセット間**: 様々なデータセットで優れた性能を示す

---

### 論文の意義と貢献

#### 1. 説明手法の改善

AttCATは、Transformerの説明手法を以下の点で改善しました：

- **特徴値の考慮**: 注意重みだけでなく、特徴値の大きさも考慮することで、より正確な説明を生成
- **スキップ接続の活用**: スキップ接続を明示的に考慮することで、完全な情報フローを説明に反映
- **特徴間の相互作用**: 特徴間の相互作用を考慮することで、より包括的な説明を生成

#### 2. 理論的な貢献

AttCATは、以下の理論的な貢献を提供します：

- **統合的なアプローチ**: エンコードされた特徴、勾配、注意重みを統合する新しいアプローチ
- **数学的な定式化**: AttCATスコアの数学的な定式化
- **忠実性の保証**: 説明の忠実性を保証する理論的な基盤

#### 3. 実用的な応用

AttCATは、以下の実用的な応用が可能です：

- **モデルの理解**: Transformerモデルの動作を深く理解
- **デバッグ**: モデルの予測が期待通りでない場合の原因を特定
- **バイアスの発見**: モデルが学習したバイアスや不適切なパターンを発見
- **モデルの改善**: 説明から得られた洞察を基にモデルを改善

---

### 技術的な革新点

#### 1. 3要素の統合

AttCATの最大の革新点は、以下の3つの要素を統合したことです：

- **エンコードされた特徴**: 特徴値の大きさを考慮
- **勾配**: 特徴の感度を測定
- **注意重み**: 特徴間の関係を捉える

これらの要素を組み合わせることで、より包括的で正確な説明を生成します。

#### 2. スキップ接続の明示的な考慮

AttCATは、スキップ接続を明示的に考慮する初めての説明手法の一つです：

- **残差接続の追跡**: スキップ接続を通じた情報の流れを追跡
- **情報の統合**: 自己注意機構とスキップ接続の両方からの情報を統合
- **完全な説明**: Transformerのすべての情報フローを説明に反映

#### 3. 特徴間の相互作用の考慮

AttCATは、特徴間の相互作用を以下のように考慮します：

- **注意による相互作用**: 注意重みを通じて、トークン間の相互作用を捉える
- **非線形な相互作用**: 勾配と特徴値の組み合わせにより、非線形な相互作用を考慮
- **統合的な説明**: 個別の特徴だけでなく、特徴間の相互作用も説明に反映

---

### 限界と今後の課題

#### 1. 計算コスト

AttCATは、以下の理由で計算コストが高くなる可能性があります：

- **勾配の計算**: 全特徴に対する勾配を計算する必要がある
- **注意重みの取得**: 全層、全ヘッドの注意重みを取得する必要がある
- **統合処理**: 3つの要素を統合する処理に時間がかかる場合がある

#### 2. 解釈の難しさ

AttCATスコアの解釈は、以下の理由で困難な場合があります：

- **複雑性**: 3つの要素を統合したスコアは複雑
- **主観性**: 説明の解釈には主観的な要素が含まれる
- **文脈依存性**: 説明は入力テキストに強く依存

#### 3. 評価の難しさ

説明手法の評価は、以下の理由で困難です：

- **定量的な評価**: 説明の質を定量的に評価する方法が限られている
- **人間による評価**: 主観的な評価に依存する部分がある
- **汎用性**: 様々なタスクやモデルに適用可能かどうかの検証が必要

---

### 実践的な示唆

#### 1. モデルの理解と検証

AttCATを使用することで、以下のことが可能になります：

- **動作の理解**: Transformerモデルがどのように動作しているかを理解
- **予測の検証**: モデルの予測が期待通りかどうかを検証
- **異常の検出**: 予期しないパターンやバイアスを発見

#### 2. モデルの改善

AttCATの説明から、以下のような改善方法を発見できます：

- **データの不足**: 重要なパターンがデータに不足している場合の特定
- **アーキテクチャの改善**: より良い説明が可能なアーキテクチャの設計
- **正則化**: 不適切なパターンを抑制する正則化手法の設計

#### 3. 教育と研究

AttCATは、以下の教育・研究目的に活用できます：

- **教育ツール**: Transformerの動作を理解するための教育ツール
- **研究の支援**: Transformerの説明可能性に関する研究を支援
- **コミュニケーション**: モデルの動作を非専門家に説明するためのツール

---

### まとめ

AttCATは、Transformerモデルの内部動作を説明するための画期的な手法です。エンコードされた特徴、勾配、注意重みを統合することで、既存手法の問題点（注意重みだけでは不十分、スキップ接続の活用不足、特徴間の相互作用の欠如）を解決し、より忠実で信頼性の高い説明を生成します。この手法は、様々なTransformerアーキテクチャ、評価指標、データセット、タスクに対して優れた性能を示し、Transformerの説明可能性を大幅に向上させます。

AttCATは、Transformerのブラックボックス性を解消し、モデルの動作を理解・検証・改善するための強力なツールを提供します。今後の研究では、計算効率の向上、より直感的な説明の生成、評価方法の改善などが重要な課題となるでしょう。
