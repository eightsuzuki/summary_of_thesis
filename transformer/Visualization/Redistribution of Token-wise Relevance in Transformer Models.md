# Redistribution of Token-wise Relevance in Transformer Models

**著者**: 澤木陽人 (早稲田大学), 村田昇 (早稲田大学)
**会議**: 2025年度 人工知能学会全国大会（第39回）一般セッション「AIと社会」
**発表日時**: 2025年5月30日 9:40～10:00
**セッション**: 4I1-GS-11-03
**URL**: https://confit.atlas.jp/guide/event/jsai2025/subject/4I1-GS-11-03/classlist

この論文は、Transformerモデルにおけるトークン間の関連度（relevance）を再配分する手法を提案しています。Layer-wise Relevance Propagation (LRP)の概念を拡張し、注意機構内のパターン変換の関連度をトークン間で再配分し、入力トークンに遡及させます。

---

### 1. 新規性：この論文の最も大きな貢献と、既存研究と比べて何が新しいのか

この論文の最大の貢献は、Transformerモデルの注意機構において、**層間でのパターン変換の影響を適切に考慮**しながら、入力トークンの寄与を正確に評価する手法を提案した点です。

既存研究では、最終層の注意重みを直接観察することでモデルの挙動を説明しようとしていました。しかし、この方法では、**層間でのパターン変換の影響を適切に考慮できず**、入力トークンの寄与を正確に評価することができませんでした。

本論文では、Layer-wise Relevance Propagation (LRP)の概念を拡張し、注意機構内のパターン変換の関連度をトークン間で再配分し、入力トークンに遡及させる手法を提案しています。これにより、入力由来の情報の影響を正確に把握し、モデルの内部動作を直感的に理解することが可能となります。

### 2. 理論/手法の核心：提案されている理論や手法の要点

#### 問題設定

Transformerモデルの注意機構は、トークン間の関連度を重みとして中間特徴を統合します。しかし、最終層の注意重みだけでは、以下の問題があります：

1. **層間でのパターン変換の影響**: 各層でパターン変換が行われるため、最終層の注意重みだけでは入力トークンの寄与を適切に評価できない
2. **情報の流れの追跡**: 入力から出力までの情報の流れを正確に追跡することが困難

#### 提案手法：トークン間関連度の再配分

本論文では、以下のような手法を提案しています：

1. **LRPの拡張**: Layer-wise Relevance Propagation (LRP)の概念をTransformerモデルに適用
2. **注意機構内のパターン変換の考慮**: 注意機構におけるパターン変換の関連度を計算
3. **トークン間での再配分**: 計算された関連度をトークン間で再配分し、入力トークンに遡及

この手法により、各入力トークンが最終的な予測にどれだけ寄与したかを、層間でのパターン変換を考慮しながら正確に定量化できます。

### 3. 「キモ」と重要性：この論文の核となるアイデアと分野への影響

* **論文の「キモ」 (核となるアイデア)**
    1. **層間でのパターン変換の考慮**: 最終層の注意重みだけでなく、層間でのパターン変換の影響を適切に考慮することで、より正確な説明を提供
    2. **LRPの拡張**: LRPの概念をTransformerモデルに適用し、注意機構の特性を考慮した新しい伝播ルールを提案

* **分野への重要性と影響**
    1. **Transformerモデルの解釈可能性の向上**: 入力トークンの寄与を正確に定量化することで、Transformerモデルの解釈可能性と透明性が向上
    2. **実用的な応用**: モデルの内部動作を直感的に理解できるため、実用的な応用における信頼性の向上が期待される
    3. **説明可能性手法の拡張**: LRPをTransformerモデルに適用する新しいアプローチを提供

---

**参考文献**:
- 澤木陽人, 村田昇. (2025). Transformerモデルにおけるトークン間関連度の再配分. 2025年度 人工知能学会全国大会（第39回）一般セッション「AIと社会」. https://confit.atlas.jp/guide/event/jsai2025/subject/4I1-GS-11-03/classlist

