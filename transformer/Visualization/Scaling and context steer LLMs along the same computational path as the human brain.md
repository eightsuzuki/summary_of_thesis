# Scaling and context steer LLMs along the same computational path as the human brain

**著者**: Joséphine Raugel, Stéphane d'Ascoli, Jérémy Rapin, Valentin Wyart, Jean-Rémi King  
**arXiv**: [arXiv:2512.01591](https://arxiv.org/abs/2512.01591)  
**出版**: NeurIPS 2025 - Spotlight

---

## 概要

本研究は、大規模言語モデル（LLM）と人間の脳が、言語処理において類似した計算パス（computational path）を辿るかどうかを検証した研究です。従来の研究では、LLMと脳の表現が部分的に整列していることが示されていましたが、この整列が**計算の順序**（temporal sequence of computations）の類似性から生じているかどうかは不明でした。

本研究では、10時間のオーディオブックを聞いている参加者の**時間分解された脳信号**（temporally-resolved brain signals）を分析し、22種類のLLM（サイズとアーキテクチャタイプが異なる）の各層の活性化と比較しました。その結果、LLMと脳は**類似した順序で表現を生成**することが明らかになりました。具体的には、LLMの初期層の活性化は早期の脳反応と最も整列し、深い層の活性化は後期の脳反応と最も整列することが示されました。この整列パターンは、TransformerアーキテクチャとRNNアーキテクチャの両方で一貫して観察されました。さらに、この整列の出現は**モデルサイズ**と**コンテキスト長**に依存することが判明しました。

---

## 新規性

本研究の主な貢献は以下の点にあります：

1. **時間的計算パスの検証**: LLMと脳の表現の整列が、単なる表現の類似性だけでなく、**計算の時間的順序**の類似性から生じていることを初めて実証しました。

2. **層深度と脳反応時間の対応関係**: LLMの層深度（浅い層→深い層）と脳の反応時間（早期→後期）の間に系統的な対応関係があることを発見しました。

3. **アーキテクチャ非依存の一貫性**: TransformerとRNNという異なるアーキテクチャタイプの両方で、同様の整列パターンが観察されることを示しました。

4. **スケーリングとコンテキスト長の影響**: モデルサイズとコンテキスト長が、この整列パターンの出現に重要な役割を果たすことを明らかにしました。

---

## 理論/手法の核心

### 1. 実験設定

#### データ
- **脳信号データ**: 参加者が10時間のオーディオブックを聞いている間の時間分解された脳信号（fMRI、MEG、EEGなど）
- **言語モデル**: 22種類のLLMをベンチマーク
  - サイズ: 小規模から大規模まで様々
  - アーキテクチャ: Transformer系とRNN系の両方を含む

#### アライメント手法
LLMの各層の活性化と、対応する時間窓での脳信号との整列度を測定します。整列度の計算には、以下のような手法が用いられます：

- **表現の類似性**: 各層の活性化ベクトルと脳信号の相関やコサイン類似度
- **時間的対応**: LLMの層深度（浅い→深い）と脳の反応時間（早期→後期）の対応関係

### 2. 主要な仮説

本研究は以下の仮説を検証しています：

**仮説**: LLMと脳は、言語処理において**類似した計算の順序**を辿る。すなわち：
- LLMの初期層は、早期の脳反応（低レベルの言語特徴の処理）と整列する
- LLMの深い層は、後期の脳反応（高レベルの意味処理）と整列する

### 3. 整列パターンの測定

#### 線形マッピング（Linear Mapping）

本研究では、脳信号とLLM活性化の間の整列を測定する前に、**線形マッピング**（linear mapping）を適用します。

##### 線形マッピングの必要性

脳信号（MEGセンサーなど）とLLM活性化の間には、**一対一の対応関係が存在しません**。例えば：
- **MEGセンサー**: 通常100-300個程度のセンサー（$s$ 個）
- **LLM活性化**: 各層で768-4096次元などの高次元ベクトル（$d$ 次元）

これらの異なる次元空間を比較するため、線形マッピングが必要です。

##### 表現の定義

Kriegeskorte、DiCarlo、King & Dehaeneらの研究に従い、本研究では神経「表現」（representation）を**「線形に読み取り可能な情報」**として操作的に定義します。これは、脳信号から線形変換によってLLM活性化を予測できる情報を意味します。

##### Ridge回帰による線形マッピング

具体的には、**Ridge回帰**（正則化線形回帰）を用いて、脳活動からLLM活性化を予測する線形写像を学習します。

**データの定義**:
- **脳活動**: $X_t \in \mathbb{R}^{w \times s}$
  - $w$: 時間窓（window）の数、またはサンプル数
  - $s$: MEGセンサー数（または脳信号の次元数）
  - $t$: 時間インデックス
- **LLM活性化**: $Y \in \mathbb{R}^{w \times d}$
  - $w$: トークン数（またはサンプル数）
  - $d$: LLM活性化の次元数（例：768、1024、4096）

**Ridge回帰の目的関数**:

$$
\hat{W} = \underset{W}{\operatorname{argmin}} \|Y - X_t W\|_F^2 + \lambda \|W\|_F^2
$$

ここで：
- $W \in \mathbb{R}^{s \times d}$: 線形変換行列（学習パラメータ）
- $\lambda$: 正則化パラメータ（過学習を防ぐ）
- $\| \cdot \|_F$: フロベニウスノルム（行列の全要素の二乗和の平方根）

**解析解**:

Ridge回帰の解析解は以下のように与えられます：

$$
\hat{W} = (X_t^\top X_t + \lambda I)^{-1} X_t^\top Y
$$

ここで、$I$ は単位行列です。

##### 線形マッピング後の整列計算

線形マッピング $W$ を学習した後、脳活動をLLM活性化空間に射影します：

$$
\hat{Y} = X_t \hat{W}
$$

この射影された表現 $\hat{Y}$ と実際のLLM活性化 $Y$ の間で、ピアソン相関係数などの整列指標を計算します。

##### 線形マッピングの利点

1. **次元の統一**: 異なる次元空間のデータを比較可能にする
2. **解釈可能性**: 線形変換は解釈しやすく、各センサーがLLM活性化のどの次元に寄与するかを理解できる
3. **計算効率**: 線形変換は計算が高速
4. **正則化による汎化**: Ridge回帰の正則化により、過学習を防ぎ、より汎用的な対応関係を学習できる

#### 層深度と時間の対応関係

各LLM層 $l$ について、最も整列する脳反応時間 $t$ を特定します：

$$
\text{Alignment}(l, t) = \text{similarity}(\text{LLM\_activation}_l, \text{brain\_signal}_t)
$$

期待される結果：
- 浅い層（$l$ が小さい）→ 早期の脳反応（$t$ が小さい）と整列
- 深い層（$l$ が大きい）→ 後期の脳反応（$t$ が大きい）と整列

##### 対応関係の詳細

本研究では、**各単語ごとに独立して整列**を計算し、その後全単語で統合します。

**各単語ごとの独立した整列**:
- 各単語 $i$ の提示時点を **Time 0** として、その単語に対する反応を測定
- 例：Token 10とToken 100は別々に分析される
- 各単語について、LLMの各層の活性化と、その単語に対するMEG反応を比較

**層深度と時間の対応関係**:
- **浅い層（Layer 1）** ↔ **早期のMEG反応（0-200ms）**
- **中間層（Layer 5）** ↔ **中期のMEG反応（200-500ms）**
- **深い層（Layer 12）** ↔ **後期のMEG反応（500ms以上）**

**全単語での統合**:
- 全500単語について同様の対応を計算
- 全単語の整列スコアを平均して、層ごとの整列パターンを評価

##### 実際の計算プロセス

1. **全500単語について**:
   - 各単語 $i$ の提示時点を Time 0 とする
   - LLM Layer $l$, Token $i$ の活性化: $a_{l,i}$
   - 単語 $i$ の提示後、時点 $t$ でのMEG信号（線形マッピング後）: $b_{i,t}$

2. **各層 $l$ と各時点 $t$ について**:
   - 全500単語にわたる相関係数を計算:
     $$
     r([a_{l,1}, a_{l,2}, ..., a_{l,500}], [b_{1,t}, b_{2,t}, ..., b_{500,t}])
     $$

3. **各層について、最も高い相関係数を持つ時点を選択**:
   - Layer 1 → $\max_t$ が 0-200ms の範囲
   - Layer 5 → $\max_t$ が 200-500ms の範囲
   - Layer 12 → $\max_t$ が 500ms以上

##### この対応関係の意味

- **段階的処理の類似性**: LLMと脳が、浅い層/早期反応から深い層/後期反応へと段階的に情報を処理していることを示す
- **計算パスの類似性**: 単なる表現の類似性だけでなく、処理の時間的順序も類似していることを示す

#### 整列スコアの計算
全層にわたる整列パターンの強度を定量化します：

$$
\text{Alignment\_Score} = \frac{1}{L} \sum_{l=1}^{L} \max_t \text{Alignment}(l, t)
$$

ここで、$L$ はLLMの層数です。

#### ピアソン相関指標としてのアライメントスコア

本研究では、**ピアソン相関係数**（Pearson correlation coefficient）をアライメント指標として使用しています。

##### ピアソン相関係数の定義

ピアソン相関係数は、2つの変数間の線形関係の強さを測定する指標で、以下の式で定義されます：

$$
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$

ここで：
- $x_i, y_i$: 2つの変数の観測値
- $\bar{x}, \bar{y}$: それぞれの平均値
- $n$: 観測数
- $r$: -1から1の値を取り、1に近いほど正の相関が強い

##### 脳-LLMアライメントでの使用

本研究では、各LLM層 $l$ の活性化ベクトルと、時間 $t$ での脳信号ベクトルの間のピアソン相関係数を計算します：

$$
\text{Alignment}(l, t) = r(\text{LLM\_activation}_l, \text{brain\_signal}_t)
$$

具体的な計算手順：

1. **データ準備**:
   - **LLM活性化**: 層 $l$ の各トークン位置での埋め込みベクトル（例：768次元や1024次元）
   - **脳信号**: 時間 $t$ での複数のボクセル/チャネルの信号値（fMRI、MEG、EEGなど）

2. **時系列データとしての扱い**:
   - 各トークン位置 $i$ について：
     - LLM層 $l$ のトークン $i$ での活性化: $a_{l,i} \in \mathbb{R}^d$
     - 対応する時間 $t_i$ での脳信号: $b_{t_i} \in \mathbb{R}^m$
   - 時系列全体でピアソン相関を計算（各次元ごと、または主成分分析後の次元で）

3. **次元の対応付け**:
   - 必要に応じて、両方を同じ次元に射影（PCA、CCA（正準相関分析）など）
   - 複数次元がある場合は、各次元の相関係数の平均を取るか、CCAを使用

##### ピアソン相関を使用する理由

1. **スケール不変性**: 異なるスケールのデータ間でも比較可能
2. **線形関係の測定**: LLMと脳の表現間の線形な対応関係を直接測定
3. **標準化された指標**: -1から1の範囲で解釈しやすい
4. **統計的検定**: 有意性検定が可能で、統計的に信頼性の高い結果が得られる

##### 最終的なアライメントスコア

全層にわたる整列パターンの強度を定量化する際、各層について最も高いピアソン相関係数を持つ時間窓を選択します：

$$
\text{Alignment\_Score} = \frac{1}{L} \sum_{l=1}^{L} \max_t r(\text{LLM\_activation}_l, \text{brain\_signal}_t)
$$

この指標により、LLMの各層が脳のどの時点の処理と最も対応しているかを定量的に評価できます。

---

## 実験結果

### 1. 層深度と脳反応時間の対応関係

実験結果から、以下のパターンが確認されました：

- **初期層（1-3層）**: 早期の脳反応（0-200ms）と最も高い整列を示す
- **中間層（4-12層）**: 中期の脳反応（200-500ms）と整列
- **深い層（13層以上）**: 後期の脳反応（500ms以上）と最も高い整列を示す

この結果は、LLMと脳が**段階的に情報を処理**していることを示唆しています。

### 2. アーキテクチャタイプの影響

- **Transformerアーキテクチャ**: 明確な層深度と時間の対応関係が観察された
- **RNNアーキテクチャ**: Transformerと同様のパターンが確認されたが、時間的分解能がやや低い傾向

この結果は、**アーキテクチャ非依存**の計算パターンが存在することを示唆しています。

### 3. モデルサイズの影響

- **小規模モデル**: 整列パターンが弱い、または不明瞭
- **中規模モデル**: 整列パターンが出現し始める
- **大規模モデル**: 明確で一貫した整列パターンが観察される

この結果は、**スケーリング**が整列パターンの出現に重要であることを示しています。

### 4. コンテキスト長の影響

- **短いコンテキスト**: 整列パターンが弱い
- **長いコンテキスト**: より明確な整列パターンが観察される

この結果は、**コンテキスト長**が整列パターンの強度に影響を与えることを示しています。

---

## 「キモ」と重要性

### 本研究の「キモ」

本研究の最も重要な発見は、**LLMと脳が類似した計算の順序を辿る**という点です。これは単なる表現の類似性を超えて、**計算プロセスそのものの類似性**を示唆しています。

具体的には：
1. **段階的処理の類似性**: LLMの層深度と脳の反応時間の間に系統的な対応関係がある
2. **アーキテクチャ非依存性**: 異なるアーキテクチャ（Transformer、RNN）でも同様のパターンが観察される
3. **スケーリングの重要性**: モデルサイズとコンテキスト長が整列パターンの出現に必要

### 重要性

1. **認知科学への貢献**: 人工知能システムと生物学的システムの間の計算的類似性を実証することで、言語処理の計算原理の理解を深めます。

2. **LLMの解釈可能性**: LLMの各層がどのような言語処理段階に対応しているかを理解することで、モデルの内部動作をより深く理解できます。

3. **脳科学への示唆**: LLMの計算パスが脳の処理パスと類似していることから、LLMを脳の処理メカニズムを理解するためのモデルとして活用できる可能性があります。

4. **スケーリングの理解**: モデルサイズとコンテキスト長が整列パターンに影響を与えることから、より脳に近い計算パスを実現するための設計指針が得られます。

5. **AIと認知の橋渡し**: 人工知能と認知科学の間の橋渡しを提供し、両分野の相互理解を促進します。

---

## 結論

本研究は、LLMと人間の脳が言語処理において**類似した計算パスを辿る**ことを実証しました。特に、LLMの層深度と脳の反応時間の間に系統的な対応関係があることが明らかになりました。この整列パターンは、TransformerとRNNの両方で一貫して観察され、モデルサイズとコンテキスト長に依存することが判明しました。

これらの知見は、人工知能システムと生物学的システムの間の計算的類似性を理解する上で重要な貢献であり、LLMの解釈可能性や認知科学への応用において重要な示唆を提供します。

