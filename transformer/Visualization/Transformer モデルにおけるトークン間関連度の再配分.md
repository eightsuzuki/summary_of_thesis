# Transformer モデルにおけるトークン間関連度の再配分

**著者**: 澤木陽人 (早稲田大学 先進理工学研究科), 村田昇 (早稲田大学)  
**会議**: 2025年度 人工知能学会全国大会（第39回）一般セッション「AIと社会」  
**発表日時**: 2025年5月30日 9:40～10:00  
**セッション**: 4I1-GS-11-03  
**URL**: https://confit.atlas.jp/guide/event/jsai2025/subject/4I1-GS-11-03/classlist

---

### 概要

この論文は、**Vision Transformer (ViT)におけるトークン間の関連度（relevance）を再配分する手法**を提案した研究です。Transformerモデルの注意機構は、トークン間の関連度を重みとして中間特徴を統合しますが、最終層の注意重みだけでは層間でのパターン変換の影響を適切に考慮できず、入力トークンの寄与を正確に評価することができません。本論文では、**Layer-wise Relevance Propagation (LRP)の概念を拡張**し、注意機構内のパターン変換の関連度をトークン間で再配分し、入力トークンに遡及させる手法を提案しています。これにより、パターン変換が行われる前の入力トークンの影響を定量化し、モデルの内部動作を直感的に理解することが可能となります。

---

### 1. 新規性：この論文の最も大きな貢献と、既存研究と比べて何が新しいのか

この論文の最大の貢献は、Transformerモデルの注意機構において、**層間でのパターン変換の影響を適切に考慮**しながら、入力トークンの寄与を正確に評価する手法を提案した点です。

既存研究では、最終層の注意重みを直接観察することでモデルの挙動を説明しようとしていました。しかし、この方法では、**層間でのパターン変換の影響を適切に考慮できず**、入力トークンの寄与を正確に評価することができませんでした。

本論文では、Layer-wise Relevance Propagation (LRP)の概念を拡張し、注意機構内のパターン変換の関連度をトークン間で再配分し、入力トークンに遡及させる手法を提案しています。これにより、入力由来の情報の影響を正確に把握し、モデルの内部動作を直感的に理解することが可能となります。

### 2. 理論/手法の核心：提案されている理論や手法の要点

#### Vision Transformer (ViT) の概要

ViTは、画像を$n$枚のパッチに分割し、CLSトークンを加えた合計$N = n+1$個のトークンをモデルに入力します。各トークンは$d$次元の表現に変換され、$L+1$層にわたるTransformer Blockで処理されます。

各トークンは**Key ($\mathbf{k}$), Query ($\mathbf{q}$), Value ($\mathbf{v}$)**の3つの特徴量を持ち、実質的に各トークンが持つ画像の特徴量は**Value: $\mathbf{v}$**と考えられます。

#### Self-Attention機構

Self-Attentionは、QueryとKeyを用いてValueを変換する処理機構です：

$$\hat{\mathbf{v}}_i = w_{i0} \mathbf{v}_0 + w_{i1} \mathbf{v}_1 + \cdots + w_{ii} \mathbf{v}_i + \cdots + w_{in} \mathbf{v}_n$$

$$w_{ij} \propto \langle \mathbf{k}_i, \mathbf{q}_j \rangle$$

ここで、$w_{ij}$はsoftmax関数で正規化され、$\sum_{j=0}^{n} w_{ij} = 1$を満たします。

#### 問題設定

Transformerモデルの注意機構は、トークン間の関連度を重みとして中間特徴を統合します。しかし、最終層の注意重みだけでは、以下の問題があります：

1. **層間でのパターン変換の影響**: 各層でパターン変換が行われるため、最終層の注意重みだけでは入力トークンの寄与を適切に評価できない
2. **情報の流れの追跡**: 入力から出力までの情報の流れを正確に追跡することが困難

#### 提案手法：Layer-wise Relevance Propagation (LRP) の拡張

本論文では、LRPの概念をTransformerモデルに拡張し、以下の手順で関連度を再配分します：

1. **出力層での初期化**: 出力層での関連度ベクトルを初期化（例：CLSトークンに1、その他に0）
2. **層ごとの関連度伝播**: 各層で関連度を下位層に伝播
3. **注意機構での再配分**: Self-Attention機構内で、パターン変換の関連度をトークン間で再配分
4. **入力トークンへの遡及**: 最終的に入力トークンへの関連度を計算

#### LRPルールの提案

本論文では、以下の4つのLRPルールを提案しています：

| ルール | Activation | Weight |
|-------|-----------|--------|
| (i) | 1 | $w_{ij} + \delta_{ij}$ |
| (ii) | $\|\mathbf{v}_i - \bar{\mathbf{v}}\|_2$ | $w_{ij} + \delta_{ij}$ |
| (iii) | 1 | $\|\mathbf{v}_j - \mathbf{v}_i\|_2 w_{ij} + \delta_{ij}$ |
| (iv) | $\|\mathbf{v}_i - \bar{\mathbf{v}}\|_2$ | $\|\mathbf{v}_j - \mathbf{v}_i\|_2 w_{ij} + \delta_{ij}$ |

ここで：
- $\bar{\mathbf{v}}$は当該層・Head内の全トークンのValueの平均
- $\delta_{ij}$はクロネッカーのデルタ（$i=j$のとき1、それ以外0）

**LRPルール (ii)**: 平均からの距離をactivationとして使用し、平均から逸脱したトークンほど特定の特徴を捉えていると仮定

**LRPルール (iii)**: Valueの更新則を以下のように書き換えることで、トークン間の距離を考慮：

$$\hat{\mathbf{v}}_i = \sum_j w_{ij} \mathbf{v}_j = \mathbf{v}_i + \sum_j w_{ij} (\mathbf{v}_j - \mathbf{v}_i)$$

距離の離れたトークン同士の方が互いの情報が混合される際の影響が大きいことを反映しています。

### 3. 「キモ」と重要性：この論文の核となるアイデアと分野への影響

* **論文の「キモ」 (核となるアイデア)**
    1. **層間でのパターン変換の考慮**: 最終層の注意重みだけでなく、層間でのパターン変換の影響を適切に考慮することで、より正確な説明を提供
    2. **LRPの拡張**: LRPの概念をTransformerモデルに適用し、注意機構の特性を考慮した新しい伝播ルールを提案

* **分野への重要性と影響**
    1. **Transformerモデルの解釈可能性の向上**: 入力トークンの寄与を正確に定量化することで、Transformerモデルの解釈可能性と透明性が向上
    2. **実用的な応用**: モデルの内部動作を直感的に理解できるため、実用的な応用における信頼性の向上が期待される
    3. **説明可能性手法の拡張**: LRPをTransformerモデルに適用する新しいアプローチを提供

### 4. 実験と評価

#### 実験設定

- **モデル**: DINO [Caron 21]で事前学習されたViT
- **アーキテクチャ**: 6 Head、12層
- **入力画像サイズ**: 224×224
- **パッチサイズ**: 16×16（CLSトークンを含めて197トークン）
- **出力関連度の初期値**: $\mathbf{R}^{(L)} = [1,0,\dots,0] \in \mathbb{R}^{197}$（CLSトークンに1、その他に0）

#### 可視化結果

提案手法を適用した結果、以下のような知見が得られました：

- **Head 6**: 画像に写る猫全体を、Attention Rollout (i) に比べてよく捉えることができている
- **局所的特徴**: 猫の鼻や耳など、特徴的な局所領域にも注目がかかるHeadがある
- **背景への分散**: 一部のHeadでは、背景領域に注目が分散してしまう様子も見られた

#### 考察

$||\mathbf{v}_j - \mathbf{v}_i||_2$をweightに導入することは、Valueの更新量の観点からは自然であるものの、似た特徴を持つ画像トークンのValueは出力層付近では似通った値に近づくことから、却って関連度の高さを打ち消してしまった可能性が考えられます。

### 5. 技術的な詳細

#### 関連度の保存則

LRPでは、各層で関連度の総和が保存されるという重要な性質があります：

$$\sum_i R_i^{(l-1)} = \sum_j R_j^{(l)}$$

この保存則により、出力層での関連度が入力層まで正確に伝播されます。

#### Multi-Head Attentionへの対応

ViTではMulti-Head Attentionが使用されるため、各HeadごとにLRPを適用し、最終的にHead間で関連度を集約します。

#### パターン変換の考慮

各層でのパターン変換（線形変換、非線形変換、正規化など）の影響を考慮しながら、関連度を下位層に伝播させます。

---

### まとめ

本研究では、Transformerにおける出力の説明可能性に着目し、Layer-wise Relevance Propagationに基づいた関連度の再配分を、トークンが持つValueの値を活用して定義しました。通常、ノードの活性化値とエッジの重みを元に計算されるLRPを、ViTにおけるトークンのValueベースの活性化値とAttentionの重みを用いることで、特徴量生成に寄与した画像領域を、入力時点にまで遡って可視化することが可能です。

TransformerモデルにおけるLRPの仕組みの応用は、様々な画像処理タスクにおける推論過程の透明性を向上させうる。提案手法として扱った定義のほかにも、パターン変換の特性に合った配分ルールの構成について、引き続き改良を進めていく必要があります。

---

**参考文献**:
- 澤木陽人, 村田昇. (2025). Transformerモデルにおけるトークン間関連度の再配分. 2025年度 人工知能学会全国大会（第39回）一般セッション「AIと社会」. https://confit.atlas.jp/guide/event/jsai2025/subject/4I1-GS-11-03/classlist
- Abnar, S. and Zuidema, W. (2020). Quantifying Attention Flow in Transformers. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
- Caron, M., et al. (2021). Emerging Properties in Self-Supervised Vision Transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision.
- Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929.

