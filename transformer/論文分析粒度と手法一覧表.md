# Transformer論文の分析粒度と手法一覧表

この表は、transformerディレクトリ内の各論文が、LLMのどの粒度レベルを分析し、どのような手法を使用しているかをまとめたものです。

## 粒度レベルの定義

1. **Embeddings**: 初期層で離散トークンを連続ベクトル表現に変換する層
2. **Transformer Blocks**: マルチヘッド自己注意機構とフィードフォワードネットワークで構成されるブロック全体
3. **Attention Mechanisms**: 各層の注意機構（注意重み、クエリ・キー・バリュー）
4. **Layer Outputs (Hidden States)**: 各Transformer層の出力（中間表現）
5. **Final Output Predictions**: 最終的な予測出力（次トークン確率分布、分類スコアなど）

---

## 分析粒度と手法の対応表

| 論文 | ディレクトリ | 1. Embeddings | 2. Transformer Blocks | 3. Attention Mechanisms | 4. Layer Outputs | 5. Final Output | 主な手法 |
|------|------------|---------------|----------------------|----------------------|-----------------|-----------------|---------|
| **Model Analysis** |
| Mapping 1,000+ Language Models via Log-Likelihood | Model Analysis | - | - | - | - | ✅ 対数尤度ベクトル | 対数尤度ベクトル、KLダイバージェンス、モデル座標マッピング |
| **Political Ideology** |
| Whose Opinions Do Language Models Reflect? | Political Ideology | - | - | - | - | ✅ 次トークン確率分布 | 次トークン対数確率、1-Wasserstein距離、意見分布比較 |
| From Pretraining Data to LMs to Downstream Tasks | Political Ideology | - | - | - | - | ✅ マスク充填/生成テキスト | 政治的傾向プロービング、追加事前学習、下流タスク評価 |
| The political ideology of conversational AI | Political Ideology | - | - | - | - | ✅ 会話応答 | サーベイ形式のバッテリー、スケーリング分析 |
| **Positional Encoding** |
| Absolute Positional Encoding (sin-cos) | Positional Encoding | ✅ 位置埋め込み | - | ✅ 位置情報の注入 | - | - | 固定三角関数基底 |
| Learned Positional Embedding | Positional Encoding | ✅ 学習可能な位置埋め込み | - | ✅ 位置情報の注入 | - | - | 学習可能パラメータ |
| Relative Positional Encoding (Shaw 2018) | Positional Encoding | ✅ 相対位置埋め込み | - | ✅ 相対距離の表現 | - | - | 相対距離行列 |
| Transformer-XL Relative Encoding | Positional Encoding | ✅ セグメント間の位置 | - | ✅ 再帰的相対位置 | ✅ セグメント間の情報保持 | - | セグメント再帰、相対位置 |
| ALiBi | Positional Encoding | - | - | ✅ 線形バイアス | - | - | 距離比例線形バイアス |
| RoPE | Positional Encoding | ✅ 回転埋め込み | - | ✅ 回転による位置表現 | - | - | 複素平面回転、位相差 |
| xPos | Positional Encoding | ✅ スケーリング安定化 | - | ✅ 減衰制御 | - | - | RoPEのスケーリング拡張 |
| YaRN | Positional Encoding | ✅ 周波数拡張 | - | ✅ 補間による長文対応 | - | - | 周波数補間、RoPE拡張 |
| PoSE | Positional Encoding | ✅ 位置スキップ学習 | - | ✅ 長文汎化 | - | - | 位置スキップ学習スキーム |
| Massive Values in Self-Attention | Positional Encoding | - | - | ✅ 注意値のノルム | ✅ 層間の値ベクトル | - | 値ベクトルノルム分析 |
| **Visualization** |
| Attention is Not Only a Weight | Visualization | - | - | ✅ 注意重み+ベクトルノルム | - | - | ノルムベース分析、値ベクトル解析 |
| Redistribution of Token-wise Relevance | Visualization | ✅ 入力トークン | ✅ 全層を追跡 | ✅ 注意機構内のパターン変換 | ✅ 層間の関連度伝播 | ✅ 最終予測への寄与 | LRP拡張、トークン間関連度再配分 |
| Discretized Integrated Gradients (DIG) | Visualization | ✅ 入力埋め込み | ✅ 全層を経由 | - | ✅ 中間層の勾配 | ✅ 最終出力への寄与 | 離散化IG、勾配積分 |
| Uniform Discretized Integrated Gradients (UDIG) | Visualization | ✅ 入力埋め込み | ✅ 全層を経由 | - | ✅ 中間層の勾配 | ✅ 最終出力への寄与 | 非線形経路IG、離散空間適応 |
| Sequential Integrated Gradients (SIG) | Visualization | ✅ 入力埋め込み | ✅ 全層を経由 | - | ✅ 逐次的な層出力 | ✅ 最終出力への寄与 | 逐次IG、順序依存性考慮 |
| AttnLRP | Visualization | ✅ 入力トークン | ✅ 全構成要素（注意/MLP/正規化） | ✅ 注意層のLRP伝播 | ✅ 各層の関連性スコア | ✅ 最終予測への寄与 | LRP、Deep Taylor Decomposition |
| Quantifying Attention Flow | Visualization | - | - | ✅ 注意ロールアウト | ✅ 層間の情報伝播 | - | 注意ロールアウト、残差接続考慮 |
| Circuit Tracing | Visualization | ✅ 入力トークン | ✅ 計算グラフ全体 | ✅ 注意ヘッド | ✅ 各層のfeature | ✅ 最終出力 | Cross-Layer Transcoder (CLT)、Attribution Graph |
| Scaling and context steer LLMs | Visualization | - | ✅ 全層の活性化 | - | ✅ 各層の活性化ベクトル | - | 層活性化と脳信号の整列、線形マッピング |
| What Does BERT Look at? | Visualization | - | - | ✅ 144個の注意ヘッド（12層×12ヘッド） | - | - | 注意パターン分析、言語現象の検証 |
| Transformer Interpretability Beyond Attention | Visualization | ✅ 入力トークン | ✅ 全層（注意+MLP） | ✅ 注意勾配 | ✅ 各層の関連性スコア | ✅ 最終予測への寄与 | LRP、注意勾配の組み合わせ |
| Attention Flows | Visualization | - | - | ✅ 注意機構の変化 | ✅ 層間の注意追跡 | - | 注意可視化、ファインチューニング前後比較 |
| bertviz | Visualization | - | - | ✅ マルチスケール注意可視化 | - | - | 注意可視化ツール |
| ecco | Visualization | - | - | - | ✅ 隠れ状態の探索 | ✅ 生成確率分布 | 隠れ状態可視化ライブラリ |
| TransformerLens | Visualization | ✅ 埋め込み | ✅ 全層 | ✅ 注意機構 | ✅ 各層の活性化 | ✅ 最終出力 | 機械論的解釈性ライブラリ |
| ExBERT | Visualization | ✅ 埋め込み表現 | - | - | ✅ 中間表現 | - | 表現可視化ツール |
| HEMERA | Visualization | ✅ 入力特徴 | ✅ 全層 | - | ✅ 各層の活性化 | ✅ 予測スコア | 説明可能なTransformer、アトリビューション |

---

## 粒度レベル別の手法分類

### 1. Embeddings を分析する論文

| 論文 | 手法 |
|------|------|
| Positional Encoding系 | 位置情報の符号化方法（絶対/相対/回転など） |
| Redistribution of Token-wise Relevance | LRPで入力トークンに遡及 |
| DIG/UDIG/SIG | 入力埋め込みから勾配を計算 |
| AttnLRP | 入力トークンへの関連性伝播 |
| Circuit Tracing | 入力トークンから計算グラフを構築 |
| TransformerLens | 埋め込み層の分析 |
| ExBERT | 埋め込み表現の可視化 |

### 2. Transformer Blocks を分析する論文

| 論文 | 手法 |
|------|------|
| Redistribution of Token-wise Relevance | LRPで全層を追跡 |
| AttnLRP | 全構成要素（注意/MLP/正規化）へのLRP適用 |
| Circuit Tracing | 計算グラフ全体の再構成 |
| Scaling and context steer LLMs | 全層の活性化を脳信号と比較 |
| Transformer Interpretability Beyond Attention | 全層（注意+MLP）へのLRP適用 |
| TransformerLens | 全層の機械論的分析 |
| HEMERA | 全層を通じた説明可能な予測 |

### 3. Attention Mechanisms を分析する論文

| 論文 | 手法 |
|------|------|
| Positional Encoding系 | 位置情報の注入方法 |
| Attention is Not Only a Weight | 注意重み+ベクトルノルムの分析 |
| Redistribution of Token-wise Relevance | 注意機構内のパターン変換 |
| Quantifying Attention Flow | 注意ロールアウト、層間伝播 |
| What Does BERT Look at? | 144個の注意ヘッドのパターン分析 |
| Transformer Interpretability Beyond Attention | 注意勾配の計算 |
| Attention Flows | ファインチューニング前後の注意変化 |
| bertviz | マルチスケール注意可視化 |

### 4. Layer Outputs (Hidden States) を分析する論文

| 論文 | 手法 |
|------|------|
| Transformer-XL Relative Encoding | セグメント間の情報保持 |
| Redistribution of Token-wise Relevance | 層間の関連度伝播 |
| DIG/UDIG/SIG | 中間層の勾配計算 |
| AttnLRP | 各層の関連性スコア |
| Quantifying Attention Flow | 層間の情報伝播追跡 |
| Circuit Tracing | 各層のfeature抽出 |
| Scaling and context steer LLMs | 各層の活性化ベクトルと脳信号の整列 |
| Transformer Interpretability Beyond Attention | 各層の関連性スコア |
| ecco | 隠れ状態の探索 |
| TransformerLens | 各層の活性化分析 |
| ExBERT | 中間表現の可視化 |
| HEMERA | 各層の活性化分析 |

### 5. Final Output Predictions を分析する論文

| 論文 | 手法 |
|------|------|
| Mapping 1,000+ Language Models | 対数尤度ベクトル、モデル間比較 |
| Whose Opinions Do Language Models Reflect? | 次トークン確率分布、意見分布比較 |
| From Pretraining Data to... | マスク充填/生成テキストの政治的傾向 |
| The political ideology of conversational AI | 会話応答の政治的傾向 |
| Redistribution of Token-wise Relevance | 最終予測への入力トークンの寄与 |
| DIG/UDIG/SIG | 最終出力への特徴量の寄与 |
| AttnLRP | 最終予測への各部分の寄与 |
| Transformer Interpretability Beyond Attention | 最終予測への関連性スコア |
| ecco | 生成確率分布の可視化 |
| TransformerLens | 最終出力の分析 |
| HEMERA | 予測スコアの説明 |

---

## 手法の分類

### 勾配ベース手法
- **DIG/UDIG/SIG**: Integrated Gradientsの変種
- **Transformer Interpretability Beyond Attention**: 注意勾配

### 伝播ベース手法
- **AttnLRP**: Layer-wise Relevance Propagation
- **Redistribution of Token-wise Relevance**: LRP拡張
- **Transformer Interpretability Beyond Attention**: LRP + 注意勾配

### 注意分析手法
- **Attention is Not Only a Weight**: ノルムベース分析
- **Quantifying Attention Flow**: 注意ロールアウト
- **What Does BERT Look at?**: 注意ヘッドパターン分析
- **Attention Flows**: ファインチューニング前後比較

### 計算グラフ手法
- **Circuit Tracing**: Cross-Layer Transcoder (CLT)

### 統計・比較手法
- **Mapping 1,000+ Language Models**: 対数尤度ベクトル、KLダイバージェンス
- **Whose Opinions Do Language Models Reflect?**: 1-Wasserstein距離
- **Scaling and context steer LLMs**: 層活性化と脳信号の整列

### 位置符号化手法
- **Positional Encoding系**: 各種位置符号化方法

---

## まとめ

- **Embeddings**: Positional Encoding系の論文が主に扱う。可視化論文では入力として扱われることが多い。
- **Transformer Blocks**: LRP系の手法（AttnLRP、Redistribution of Token-wise Relevance）やCircuit Tracingが全層を分析。
- **Attention Mechanisms**: 最も多くの論文が扱う粒度。注意重み、注意ロールアウト、注意ヘッド分析など様々な手法がある。
- **Layer Outputs**: 中間表現の分析。勾配ベース手法や伝播ベース手法が各層の出力を追跡。
- **Final Output Predictions**: Model AnalysisとPolitical Ideologyの論文が主に扱う。可視化論文では最終的な寄与度を計算する際に使用。

